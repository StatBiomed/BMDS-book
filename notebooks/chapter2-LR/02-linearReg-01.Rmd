

## Linear Regression Basics

To understand how linear regression works and how effective the least squares 
method is, we will start with simulated data, so that we can compare the 
estimation to the ground truth for data generation.

Here, we use terminology by using capital letters to denote random variables, 
e.g., $Y$ and $X$, and use lower letters to denote the value of individual 
samples, e.g., $y$ and $x$ or $y_i$ and $x_i$ for a specific sample $i$. 

<!-- Also, we will use $y$ as the actual observed output target, $\hat{y}$ as 
the predicted (or estimated) value, and $\bar{y}$ of the average value of all 
observed samples.
-->

### Simulating data
- For *X*, we generate 100 normally distributed random numbers with mean 1.5 
  and standard deviation 2.5. 

- For predicted value $Y_e$, we assume an intercept ($\alpha$) of 2 and a 
  slope ($\beta$) of 0.3 and we write $Y_e = 2 + 0.3 x$

  Later, we will estimate the values of $\alpha$ and $\beta$ using the least 
  squares method and see how that changes the efficacy of the model. 

- Though we estimate $Y_e = \alpha + \beta X$, in reality Y is rarely perfectly 
  linear. It usually has an error component or 
  **residual**: $Y = \alpha + \beta X + R$, where *R* is a random variable and 
  is assumed to be normally distributed.
    
  Therefore for the actual value *Y*, we add a residual term (`res`), a random 
  variable distributed normally with mean 0 and a standard deviation of 0.5.

The following cell shows the code snippet to generate these numbers and convert 
these three columns in a data frame. Read through the code carefully and run 
the cell to output a sample of our simulated data.

```{r}
## Fix seed: each run gives the same random numbers so the same outputs. 
# Commenting out this line would read similar but different outputs at each run. 
# Try it out!
set.seed(0)

## Generate data
X = 2.5 * rnorm(100) + 1.5   # Array of 100 values with mean = 1.5, stddev = 2.5
ypred = 2 + 0.3 * X          # Prediction of Y, assuming a = 2, b = 0.3

res = 0.5 * rnorm(100)       # Generate 100 residual terms
yact = 2 + 0.3 * X + res     # Actual values of Y

## Create dataframe to store our X, ypred, and yact values
df = data.frame('X' = X, 'ypred' = ypred, 'yact' = yact)

## Show the first six rows of our dataframe
head(df)
```


Now let’s plot both the actual output (`y`) and predicted output (`ypred`) 
against the input variable (`X`) to see what the difference between `yact` and 
`ypred` is, and therefore, to see how accurately the proposed equation 
(`ypred = 2 + 0.3 * X`) has been able to predict the value of the output:

```{r fig2-simu-scatter, out.width = '75%'}
# You can use basic plotting functions
# plot(x=df$X, y=df$yact, col="red")
# lines(x=df$X, y=df$ypred, col="darkgreen")


# But let's use ggplot2 for higher flexibility
library(ggplot2)

ggplot(df, aes(X)) +                              # basic graphical object
  geom_point(aes(y=yact), colour="black") +       # first layer
  geom_line(aes(y=ypred), colour="darkgreen") +   # second layer
  ggtitle('Actual vs Predicted values from the dummy dataset')
```


### Least squares method

Now, using our simulated data from the previous step, let’s estimate the 
optimum values of our variable coefficients, $\alpha$ and $\beta$. Using the 
predictor variable, `X`, and the output variable, `yact`, we will calculate the 
values of $\alpha$ and $\beta$ using the Least Squares method described in the 
lecture.

$$\hat{\alpha}, \hat{\beta} = \text{argmin}_{\alpha, \beta} \sum_{i=1}^n(y_i - (\alpha + \beta * x_i))^2$$

The cell below creates the same dataframe as previously. Run the cell to get 
started!

```{r}
set.seed(0)

# Generate data
X = 2.5 * rnorm(100) + 1.5   # Array of 100 values with mean = 1.5, stddev = 2.5
ypred = 2 + 0.3 * X          # Prediction of Y, assuming a = 2, b = 0.3

res = 0.5 * rnorm(100)       # Generate 100 residual terms
yact = 2 + 0.3 * X + res     # Actual values of Y

# Create dataframe to store our X, ypred, and yact values
df = data.frame('X' = X, 'ypred' = ypred, 'yact' = yact)
```

Just to reiterate, here are the formulas for $\alpha$ and $\beta$ again:

 $$\hat\beta=\frac{\sum_{i=1}^n(X_i-\bar X)(Y_i-\bar Y)}{\sum_{i=1}^n(X_i-\bar X)^2}=\frac{\text{cov}(X,Y)}{\text{var}(X)}$$
 
 $$\hat\alpha=\bar Y-\hat\beta * \bar X$$

To calculate these coefficients, we will create a few more columns in our `df` 
data frame. We need to calculate `xmean` and `ymean` to calculate the covariance
of X and Y (`xycov`) and the variance of X (`xvar`) before we can work out the 
values for `alpha` and `beta`.

```{r}
# Calculate the mean of X and Y
xmean = mean(X)
ymean = mean(yact)

# Calculate the terms needed for the numator and denominator of beta
df['xycov'] = (df['X'] - xmean) * (df['yact'] - ymean)
df['xvar'] = (df['X'] - xmean)**2

# Calculate beta and alpha
beta = sum(df['xycov']) / sum(df['xvar'])
alpha = ymean - (beta * xmean)
print(paste('alpha =', alpha, ';', 'beta =', beta))
```

As we can see, the values are only a little different from what we had assumed 
earlier. 

Let’s see how the value of $R^2$  changes if we use the new values of $\alpha$ 
and $\beta$. 

The equation for the new model can be written as:
$$ y=1.934 + 0.328 * x $$ 

Let’s create a new column in `df` to accommodate the values generated by this 
equation and call this `ypred2`, and calculate the new $R^2$. 

```{r}
# Create new column to store new predictions
df['ypred2'] = alpha + beta * df['X']

# Calculate new SSR with new predictions of Y.
# Note that SST remains the same since yact and ymean do not change.
df['SSR2'] = (df['ypred2'] - ymean)**2
df['SST'] = (df['yact'] - ymean)**2
SSR2 = sum(df['SSR2'])
SST = sum(df['SST'])

# Calculate new R2
R2_2 = SSR2 / SST
print(paste('New R2 =', R2_2))
```

The new value of $R^2= 0.695$ shows a slight improvement from the previous 
value of $R^2=0.583$ (obtained with $\alpha=2,~\beta=0.3$).

Let’s also plot our new prediction model against the actual values and our 
earlier assumed model, just to get a better visual understanding. 

```{r fig2-simu-OLS, out.width = '75%'}
library(ggplot2)

# Put color into aes
ggplot(df, aes(X)) +                              # basic graphical object
  geom_point(aes(y=yact), colour="black") +       # first layer
  geom_line(aes(y=ypred, colour="Guess")) +       # second layer
  geom_line(aes(y=ypred2, colour="OLS")) +        # third layer
  scale_colour_manual(name="Models", values = c("Guess"="darkgreen", "OLS"="red")) +
  ggtitle('Actual vs Predicted with guessed parameters vs Predicted with calculated parameters')
```

As we can see, the `ypred2` and `ypred` are more or less overlapping since the 
respective values of ɑ and β are not very different.

Next, we will explore other methods of determining model efficacy by using the 
notebook called `02-linearReg-03.Rmd`.


### Model efficacy

How do we know the values we calculate for α and β are giving us a good model? 
We can explain the total variability in our model with the 
**Total Sum of Squares** or SST:

$$SST = \sum_{i=1}^n\Bigl(\text{yact}_i - \text{yavg}\Bigr)^2, \qquad\qquad \text{yavg}=\frac1n \sum_{i=1}^n \text{yact}_i$$
            
Mathematically, we have

$$ \sum_{i=1}^n\Bigl(\text{yact}_i - \text{yavg}\Bigr)^2
= \sum_{i=1}^n\Bigl(\text{ypred}_i -\text{yavg} \Bigr)^2
+ \sum_{i=1}^n\Bigl(\text{yact}_i - \text{ypred}_i\Bigr)^2$$

**Note,** this relationship only holds if using least squares method.

The identity reads as 

**Sum of Squares Total**  = **Sum of Squares Regression** + **Sum of Squares Error**,

or simply ,  

**SST** = **SSR** + **SSE**.

The Regression Sum of Squares or SSR measures the variation of the 
regression/predicted values, and the Sum of Squares Error SSE the 
variation between the actual and the predicted values.  
An alternative saying is that SSR is the difference explained by the model, SSE 
is the difference not explained by the model and is random, and SST is the 
total error.
**Note**, we often use SSE (Sum of Squares Error) and SSD (Sum of Squares 
Difference) interchangeably.

### *R-Squared*

The higher the ratio of SSR to SST, the better the model is. This ratio is 
quantified by the **coefficient of determination** (also known as 
***R<sup>2</sup>*** or ***R*-squared**):

$$ R^2= \frac{SSR}{SST}$$

Since $SST= SSR+SSE$, $\qquad 0\le R^2\le 1$.  

The closer it is to 1, the better the model. Note that there are many other 
factors that we need to analyse before we can conclude a linear regression 
model is effective, but a high $R^2$ is a pretty good indicator.

Let’s see what the value of $R^2$ is for our simulated dataset.

```{r}
# Calculate the mean of Y
ymean = mean(df$yact)
print(paste('Mean of Y =', ymean)) # paste brings a white space by default

# Calculate SSR and SST
df['SSR'] = (df['ypred'] - ymean)**2
df['SST'] = (df['yact'] - ymean)**2
SSR = sum(df['SSR'])
SST = sum(df['SST'])

# Calculate R-squared
R2 = SSR / SST
print(paste('R2 =', R2))
```



The value of $R^2=0.583$ suggests that `ypred` provides a decent prediction of 
the `yact`. 

We have randomly assumed some values for $\alpha$  and $\beta$, but these may 
or may not be the best values. In the next step, we will use the least sum of 
square method to calculate the optimum value for $\alpha$ and $\beta$ to see if 
there is an improvement in $R^2$.

To get started on the next step, open the notebook called `02-linearReg-02.Rmd`.

