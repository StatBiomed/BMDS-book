
## Least Squares Using Simulated Data

Now, using our simulated data from the previous step, let’s estimate the optimum values of our variable coefficients, $\alpha$ and $\beta$. Using the predictor variable, `X`, and the output variable, `yact`, we will calculate the values of $\alpha$ and $\beta$ using the Least Squares method described in the lecture.

The cell below creates the same dataframe as previously. Run the cell to get started!

```{r}
set.seed(0)

# Generate data
X = 2.5 * rnorm(100) + 1.5   # Array of 100 values with mean = 1.5, stddev = 2.5
ypred = 2 + 0.3 * X          # Prediction of Y, assuming a = 2, b = 0.3

res = 0.5 * rnorm(100)       # Generate 100 residual terms
yact = 2 + 0.3 * X + res     # Actual values of Y

# Create dataframe to store our X, ypred, and yact values
df = data.frame('X' = X, 'ypred' = ypred, 'yact' = yact)
```

Just to reiterate, here are the formulas for $\alpha$ and $\beta$ again:

 $$\hat\beta=\frac{\sum_{i=1}^n(X_i-\bar X)(Y_i-\bar Y)}{\sum_{i=1}^n(X_i-\bar X)^2}=\frac{\text{cov}(X,Y)}{\text{var}(X)}$$
 
 $$\hat\alpha=\bar Y-\hat\beta * \bar X$$

To calculate these coefficients, we will create a few more columns in our `df` 
data frame. We need to calculate `xmean` and `ymean` to calculate the covariance
of X and Y (`xycov`) and the variance of X (`xvar`) before we can work out the 
values for `alpha` and `beta`.

```{r}
# Calculate the mean of X and Y
xmean = mean(X)
ymean = mean(yact)

# Calculate the terms needed for the numator and denominator of beta
df['xycov'] = (df['X'] - xmean) * (df['yact'] - ymean)
df['xvar'] = (df['X'] - xmean)**2

# Calculate beta and alpha
beta = sum(df['xycov']) / sum(df['xvar'])
alpha = ymean - (beta * xmean)
print(paste('alpha =', alpha, ';', 'beta =', beta))
```

As we can see, the values are only a little different from what we had assumed 
earlier. 

Let’s see how the value of $R^2$  changes if we use the new values of $\alpha$ 
and $\beta$. 

The equation for the new model can be written as:
$$ y=1.934 + 0.328 * x $$ 

Let’s create a new column in `df` to accommodate the values generated by this 
equation and call this `ypred2`, and calculate the new $R^2$. 

```{r}
# Create new column to store new predictions
df['ypred2'] = alpha + beta * df['X']

# Calculate new SSR with new predictions of Y.
# Note that SST remains the same since yact and ymean do not change.
df['SSR2'] = (df['ypred2'] - ymean)**2
df['SST'] = (df['yact'] - ymean)**2
SSR2 = sum(df['SSR2'])
SST = sum(df['SST'])

# Calculate new R2
R2_2 = SSR2 / SST
print(paste('New R2 =', R2_2))
```

The new value of $R^2= 0.695$ shows a slight improvement from the previous 
value of $R^2=0.583$ (obtained with $\alpha=2,~\beta=0.3$).

Let’s also plot our new prediction model against the actual values and our 
earlier assumed model, just to get a better visual understanding. 

```{r}
library(ggplot2)

# Put color into aes
ggplot(df, aes(X)) +                              # basic graphical object
  geom_point(aes(y=yact), colour="black") +       # first layer
  geom_line(aes(y=ypred, colour="Guess")) +       # second layer
  geom_line(aes(y=ypred2, colour="OLS")) +        # third layer
  scale_colour_manual(name="Models", values = c("Guess"="darkgreen", "OLS"="red")) +
  ggtitle('Actual vs Predicted with guessed parameters vs Predicted with calculated parameters')
```

As we can see, the `ypred2` and `ypred` are more or less overlapping since the 
respective values of ɑ and β are not very different.

Next, we will explore other methods of determining model efficacy by using the 
notebook called `02-linearReg-03.Rmd`.
