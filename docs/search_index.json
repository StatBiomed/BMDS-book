[["index.html", "Biomedical Data Science - introduction with case studies Welcome", " Biomedical Data Science - introduction with case studies BIOF1001 teaching team 2025-09-08 Welcome Welcome to the book Biomedical Data Science - an introduction with case studies. Most contents are demonstrated with R programming language. This book is designed as a collection of R Markdown notebooks, as supplementary to the lecture notes for the course BIOF1001: Introduction to Biomedical Data Science, an undergraduate course (Year 1) at the University of Hong Kong. Note: Most contents may be only updated before or right after the lectures, so please refer to the updated version. GitHub Repository: you can find the source files on StatBiomed/BMDS-book and the way to re-build this book. "],["preface.html", "Preface Introduction for readers Other reference books Acknowledgements", " Preface This book is designed as the supporting textbook for BIOF1001: Introduction to Biomedical Data Science, an undergraduate course (Year 1) at the University of Hong Kong. This book is not aimed to be a comprehensive textbook, but rather more Rmarkdown notebooks as supplementary to lecture notes so that students can reproduce the teaching contents more easily. Introduction for readers What you will learn from this course/book In part I, you will find a general introduction to data science (by Dr Jason Wong, Dr Ray Hsu, and Dr Yuanhua Huang): Basic programming and visualisation skills: R scripts for the quantitative methods and data visualisation. Quantitative methods: t-test, correlation analysis, clustering, linear regression, linear classification. Gain familiarity with common databases in the biomedical domain. Introduce ethical, legal, social and technological issues related to biomedical data sciences. Introduce good practice in managing a data science project and communicate results to key stakeholders. In part II, you will experience data types in four different biomedical topics, which will be illustrated with both introduction and cases that are suitable for problem-based learning format: Medical imaging and digital health, by Dr Joshua Ho and Dr Eden Ti Cancer genomics, by Dr David Shih Epidemiology, by Dr Kathy Leung Population genetics, by Dr Clara Tang What we recommend you do while reading this book To enhance the knowledge and skills learned from this book, we recommend that readers Read the materials/slides provided in each module Practice quantitative skills by solving problems using R Other reference books Besides this online book as a collection of R materials for the teaching contents, we also recommend the following online books as reference: Introduction to Data Science: Data Wrangling and Visualization with R by Rafeal A. Irizarry Advanced Data Science: Statistics and Prediction Algorithms Through Case Studies by Rafeal A. Irizarry Acknowledgements We thank all teachers and student helpers contributing to this course across all years, including 2022: Dr Lequan Yu and Dr Carlos Wong 2022 &amp; 2023: Dr Asif Javed, Dr Tommy Lam, and Dr Kathy Leung Student helpers: Mr Mingze Gao, Ms Fangxin Cai, and Mr Hoi Man Chung. "],["introR.html", "Chapter 1 Introduction to R 1.1 Intro to R programming (Session 1) 1.2 Intro to R programming (Session 2)", " Chapter 1 Introduction to R 1.1 Intro to R programming (Session 1) This notebook collects the scripts used for teaching in BBMS1021/BIOF1001 for Introduction to R. You can get this Rmd file on Moodle or here (right-click and “save link as” to download). R is a programming language, particularly popular for its power in statistical computing, elegant graphics, and also genomic data analysis. It is a free and open-source software, with active support and development from the community. Additionally, R is relatively easy to get started for scientific computing. Note To learn and practice R programming, you need to install R and RStudio on your computer. You can follow the instructions for installation in the Appendix A chapter. 1.1.1 R as a basic calculator R can be used like a calculator for arithmetic operations: 2 + 2 #&gt; [1] 4 5 * 3 #&gt; [1] 15 10 / 2 #&gt; [1] 5 2 ^ 3 #&gt; [1] 8 Can you write the command to calculate your BMI (i.e. \\(mass_{kg} \\div {height_m}^2\\))? # Anything after # means they are comments, and R won&#39;t execute it # type your command below What is the area of a circle with a radius of 3? # type your command below Like any good scientific calculator, R also has bulit-in mathematical functions such as log(), exp(), sqrt(), abs(), sin(), cos(), tan() , etc. # take the Sine of 90 degrees (i.e. 1.571 in radians) sin(1.571) #&gt; [1] 1 1.1.2 Variables In computer programming, variables are used to store information. It is called a variable because you can also change the information that is stored. # example of assigning variable called age age &lt;- 18 age #&gt; [1] 18 name &lt;- &quot;John&quot; name #&gt; [1] &quot;John&quot; Variables can be called just about anything, but there are some rules in R: They can contain letters (a-z, A-Z), digits (0-9), dots (.), and underscores (_). Special characters like $, #, %, or spaces are not allowed. A variable name must start with a letter or a dot (.), but if it starts with a dot, the next character cannot be a digit. Variable names are case-sensitive (e.g., age, Age, and AGE are different variables). You cannot use particular words such as TRUE, FALSE, if, else, or function as variable names as they are reserved for other purposes in R. Have a go at create a few variables using different variable names. # create a few variables below As mentioned, variables can reassigned with new values. It can also be assigned the value of another variable. Let’s try this: # create a variable called height and set to 170 height &lt;- 170 height #&gt; [1] 170 # now the assign a new value to height height &lt;- 175 height #&gt; [1] 175 # now create another variable called new_height and set to 165 new_height &lt;- 165 height &lt;- new_height height #&gt; [1] 165 Using R’s calculator capabilities. # add 2cm to the height of 1.70m height &lt;- 1.70 height &lt;- height + 0.02 height #&gt; [1] 1.72 Now using variables, can you calculated your BMI? # type your code here In R, it is also possible to assign variables using the equals sign = . However, it is generally recommend to use the left arrow &lt;- for variable assignment in R. 1.1.3 Data types All variables in R need to belong to a data type. In R (similar to most other programming languages), there are a few commonly used data types that are predefined in the built-in environment. You can use the class() and typeof() functions to check the class and data type of any variable. In total, R has five data types: Numeric (e.g. 3.1416, 20) Integers (e.g. 5L, -10L) Complex (e.g. 3 +2i) Logical (e.g. TRUE or FALSE) Characters (e.g. “Yuanhua”, ‘Jason’) 1.1.3.1 numeric (or double) The numeric data type is for numbers, with or without decimals. It the most commonly used and the default data type in R. The numeric datatype saves values in double precision (double number of bytes in memory), so the type is also double. # example of numeric type height &lt;- 1.70 height #&gt; [1] 1.7 class(height) #&gt; [1] &quot;numeric&quot; typeof(height) #&gt; [1] &quot;double&quot; 1.1.3.2 integer The integer is another data type used for whole numbers. You can use the capital ‘L’ notation as a suffix to specify a particular value as the integer data type. Also, you can convert a numeric into an integer type using the as.integer() function. # example of integer type siblings &lt;- 2L siblings #&gt; [1] 2 class(siblings) #&gt; [1] &quot;integer&quot; typeof(siblings) #&gt; [1] &quot;integer&quot; 1.1.3.3 logical In R, the logical data type takes either a value of TRUE or FALSE . A logical value is often generated when comparing variables. # example of logic type male&lt;-TRUE male #&gt; [1] TRUE typeof(male) #&gt; [1] &quot;logical&quot; 1.1.3.4 character In R, the character is a data type where you have all the alphabets and special characters. It stores character values or strings. Strings in R can contain alphabets, numbers, and symbols. The character type is usually denoted by wrapping the value inside single or double inverted commas. # example of character type name &lt;- &quot;Jason&quot; name #&gt; [1] &quot;Jason&quot; typeof(name) #&gt; [1] &quot;character&quot; It is possible to inter-convert between data types. This can be useful as certain operations or functions (we will formally define functions later) only works on certain data types. # example of converting data types weight &lt;- 65 typeof(weight) #&gt; [1] &quot;double&quot; weight &lt;- as.integer(weight) typeof(weight) #&gt; [1] &quot;integer&quot; score &lt;- &quot;9.5&quot; typeof(score) #&gt; [1] &quot;character&quot; score &lt;- as.numeric(score) typeof(score) #&gt; [1] &quot;double&quot; score #&gt; [1] 9.5 While R is quite flexible in converting data types, not all conversions are possible. You can experiment with different conversions and we can see what conversions are not permissible. # type your code here name &lt;- &quot;Jason&quot; typeof(name) #&gt; [1] &quot;character&quot; name &lt;- as.integer(name) #&gt; Warning: NAs introduced by coercion 1.1.4 Functions As the name suggests, functions are used to perform certain operations on some input. Perhaps without realising, you have already used functions multiple times above. For example, sin() and typeof(), etc. As one more example floor() is a function here to round a number down to the closest whole number weight &lt;- 65.6 floor(weight) #&gt; [1] 65 The functions that we have used above are already predefined by R. But generally, one of the major concepts in computer programming is to write your own functions. Once a function is defined, it can be used over and over again without needing to rewrite the code in the function every time. Consider the following example for make a function to calculate BMI: # First define a function to calculate BMI # weight in kg, height in metres calculate_bmi &lt;- function(weight, height){ bmi &lt;- weight/(height^2) return(bmi) } #Example usage: jason_weight &lt;- 66.2 jason_height &lt;-1.71 ray_weight &lt;- 68.1 ray_height &lt;- 1.85 calculate_bmi(jason_weight,jason_height) #&gt; [1] 22.63944 calculate_bmi(ray_weight,ray_height) #&gt; [1] 19.89774 Now try to write a function to calculate the area of a circle: # type your code here The reason why R is such a useful programming language is because many functions to do all kinds of things have already been written for us. We will learn how to access these through R Packages in the next lesson. 1.1.5 R scripts An R script is a plain text file that contains a series of R commands saved in order. Instead of typing commands one by one directly into the R console, you write them all in this text file. Then, you can run the whole script at once to execute all commands automatically. R scripts are essential for organsing your work, for automation and for sharing with others. Actually any of the code blocks above can be saved as a R script, but for clarity (since they are already embedded in this R markdown, I will save the following calculate bmi function separately in a R script file. # weight in kg, height in metres calculate_bmi &lt;- function(weight, height){ bmi &lt;- weight/(height^2) return(bmi) } # Example usage bmi&lt;-calculate_bmi(weight = 66.5,height = 1.72) print(bmi) #&gt; [1] 22.47837 Save this as calculate_bmi.R . To run the script in the console, we can type: source(&quot;calculate_bmi.R&quot;,echo=TRUE) If you get an error it is most likely the R script was saved in a directory different from the current directory. You can use getwd() to see whether the current directory the one where you saved the script. If not, you can either use setwd() to move to the directory where the script is, or alternatively, just resave the R script in the current directory. getwd() Now have a go at writing and saving your own R script that will contain a function to convert your heart rate from beats per minute (bpm) to beats per second (bps), and also print your name and your heart rate in both bpm and bps. 1.1.6 Resource links This notebook adapts contents from these resources: https://www.datamentor.io/r-programming/ https://www.geeksforgeeks.org/r-data-types/ https://data-flair.training/blogs/r-data-types/ http://www.sthda.com/english/wiki/ggplot2-essentials 1.1.6.1 Coding styling Elegant styling is crucial for maintenance and collaboration. Here is a highly recommended guideline: http://adv-r.had.co.nz/Style.html 1.2 Intro to R programming (Session 2) You can get this Rmd file on Moodle or here (right-click and “save link as” to download). In the last session, we started learning to use R through the Rstudio interface. We also learn about some fundamental programming concepts including Variables, Data types and Functions in R. We also learnt to save our code into Rscripts. In this lecture, we will delve into data structures in R. We will also learn to read and write files into R. 1.2.1 Data structures Data structures are one of the most important features in programming. They are used to organise and store data so that we can perform analysis on them quickly and efficiently. Each data structure have pros and cons depending on the type of data it is used to store and the analysis required. Main data structures used in R: Vector: A list of items of the same data type, like a series of numbers (numeric) or words (characters). Matrix: Tables of numbers with rows and columns, where all elements are the same type. List: Collections that can hold different types of data all together, such as numbers, words, or even other lists. Data Frame: Like a spreadsheet where each column can have different types of data, useful for tabular data 1.2.2 Vector Vector is a basic data structure in R. It contains elements in the same data type (no matter double, integer, character or others). You can check the data type by using typeof() function and the length of the vector by length() function. Since a vector has elements of the same type, this function will try and coerce elements to the same type, if they are different. Coercion is from lower to higher types, i.e., from logical to integer to double to a character. See more introduction here. # vector of numerics # c combines numbers in the brackets to a vector age &lt;- c(16, 20, 15, 17, 18) age #&gt; [1] 16 20 15 17 18 typeof (age) #&gt; [1] &quot;double&quot; length(age) #&gt; [1] 5 There are also different ways to automatically generate vectors # vectors can be created using existing variables jason &lt;- 1.69 ray &lt;- 1.78 yuanhua &lt;- 1.72 heights &lt;- c(jason, ray, yuanhua) names(heights) &lt;- c(&quot;Jason&quot;,&quot;Ray&quot;,&quot;Yuanhua&quot;) # you can optionally give names to items in a vector heights #&gt; Jason Ray Yuanhua #&gt; 1.69 1.78 1.72 typeof(heights) #&gt; [1] &quot;double&quot; # rep repeats the first number, n times based on the second value repeats &lt;- rep(3, 5) repeats #&gt; [1] 3 3 3 3 3 typeof(repeats) #&gt; [1] &quot;double&quot; # generates a vector of integers from 1 to 12 (inclusive) series &lt;- 1:12 series #&gt; [1] 1 2 3 4 5 6 7 8 9 10 11 12 typeof(series) #&gt; [1] &quot;integer&quot; # generate a vector of random integers by sampling 6 values between 1 and 49 (六合彩) mark6 &lt;- sample(1:49,6) mark6 #&gt; [1] 30 43 1 25 14 4 typeof(mark6) #&gt; [1] &quot;integer&quot; Vectors can also be combined from other vectors. # vectors can be combined new_vector &lt;- c(series,mark6,rep(50,3)) new_vector #&gt; [1] 1 2 3 4 5 6 7 8 9 10 11 12 30 43 1 25 14 4 50 50 50 typeof(new_vector) #&gt; [1] &quot;double&quot; Did you notice that new_vector is now a of type double ? You can spend a few minutes to create vectors and explore what happens to the data type of the vector when you combine them with others. # write your code here myVector &lt;- c(TRUE,2:3,4.0) myVector #&gt; [1] 1 2 3 4 typeof(myVector) #&gt; [1] &quot;double&quot; 1.2.2.1 Coercion Last week we explored converting the data type of variables using as.numeric , as.integer, as.character etc. These are done explicitly. When combining vectors with different data types they are automatically converted to the most general type in what is known as coercion. It occurs based on the following order: logical &lt; integer &lt; numeric &lt; character # coercion of different data types into one type coerced &lt;- c(TRUE, 2L, 3.4, &quot;4&quot;) coerced #&gt; [1] &quot;TRUE&quot; &quot;2&quot; &quot;3.4&quot; &quot;4&quot; typeof(coerced) #&gt; [1] &quot;character&quot; coerced &lt;- c(FALSE, 1L, 2.4) coerced #&gt; [1] 0.0 1.0 2.4 typeof(coerced) #&gt; [1] &quot;double&quot; 1.2.2.2 Mathematical operations Common mathematical operations can be directly performed on numeric or integer vectors: # example calculations on vectors steps_D1 &lt;- c(9783, 11233, 7844, 9331) steps_D2 &lt;- c(12432, 19931, 6833, 8322) total_steps &lt;- steps_D1 + steps_D2 total_steps #&gt; [1] 22215 31164 14677 17653 diff_steps &lt;- steps_D1 - steps_D2 diff_steps #&gt; [1] -2649 -8698 1011 1009 Can you write the code to calculate BMI from the vectors below? # complete the code to calculate BMI from the weights and heights vectors weights &lt;- c(54, 66, 78, 45) heights &lt;- c(1.55, 1.77, 1.75, 1.61) bmis &lt;- weights/heights^2 # &lt;--- write you code here bmis #&gt; [1] 22.47659 21.06674 25.46939 17.36044 1.2.2.3 Index Items in a vector can be directly retrieved by their index # Example of using index to retrieve items in a vector scores &lt;- c(44,56,63,75,77,78,81,90,93,99) scores #&gt; [1] 44 56 63 75 77 78 81 90 93 99 typeof(scores) #&gt; [1] &quot;double&quot; scores[5] # retrieve score at index 5 (i.e. 5th position) #&gt; [1] 77 scores[2:5] # retrieve score from index 2 to 5 #&gt; [1] 56 63 75 77 scores[c(2,5,7)] # retrieve scores from index 2, 5 and 7 #&gt; [1] 56 77 81 scores[-5] # retrieve all scores except index 5 #&gt; [1] 44 56 63 75 78 81 90 93 99 scores[c(TRUE,FALSE)] # retrieve scores in odd positions #&gt; [1] 44 63 77 81 93 Can you write the code to retrieve the scores from: the first position even positions every 3rd position scores &lt;- c(44,56,63,75,77,78,81,90,93,99) # write the code to retieve the 1st position scores[1] #&gt; [1] 44 # write the code to retrieve even positions scores[c(FALSE,TRUE)] #&gt; [1] 56 75 78 90 99 # write the code to retrieve every 3rd position scores[c(FALSE,FALSE,TRUE)] #&gt; [1] 63 78 93 1.2.2.4 Modifying a vector Using the index it is possible to modify specific items in a vector # example modifying items in a vector scores &lt;- c(44,56,63,75,77,78,81,90,93,99) scores #&gt; [1] 44 56 63 75 77 78 81 90 93 99 scores[5] &lt;- 50 # modify position 5 scores #&gt; [1] 44 56 63 75 50 78 81 90 93 99 scores[1:4] &lt;- c(30,40,50,60) # modify positions 1 to 4 scores #&gt; [1] 30 40 50 60 50 78 81 90 93 99 scores[1] &lt;- &quot;Twenty&quot; # modify position 1 and changes vector to character scores #&gt; [1] &quot;Twenty&quot; &quot;40&quot; &quot;50&quot; &quot;60&quot; &quot;50&quot; &quot;78&quot; &quot;81&quot; &quot;90&quot; #&gt; [9] &quot;93&quot; &quot;99&quot; 1.2.3 Matrix Matrix is a two-dimensional data structure. It is in principle built based on vector but has more functions for matrix computation. It has rows and columns, both of which can also have names. To check the dimensions, you can use the dim() function. See more introduction here. myMatrix &lt;- matrix(1:12, nrow=3) colnames(myMatrix) &lt;- c(&quot;C1&quot;,&quot;C2&quot;,&quot;C3&quot;,&quot;C4&quot;) # assign column names rownames(myMatrix) &lt;- c(&quot;R1&quot;,&quot;R2&quot;,&quot;R3&quot;) # assign row names myMatrix #&gt; C1 C2 C3 C4 #&gt; R1 1 4 7 10 #&gt; R2 2 5 8 11 #&gt; R3 3 6 9 12 dim(myMatrix) # prints dimension of the matrix #&gt; [1] 3 4 1.2.3.1 Indexing a matrix Accessing items in a matrix is similar to vector but in 2 dimensions myMatrix &lt;- matrix(1:12, nrow=3) colnames(myMatrix) &lt;- c(&quot;C1&quot;,&quot;C2&quot;,&quot;C3&quot;,&quot;C4&quot;) rownames(myMatrix) &lt;- c(&quot;R1&quot;,&quot;R2&quot;,&quot;R3&quot;) myMatrix[1, 2] # retrieve item in row 1 column 2 #&gt; [1] 4 myMatrix[&quot;R2&quot;, &quot;C2&quot;] # retrieve item by row and col name in R2, C2 #&gt; [1] 5 myMatrix[1, 1:2] # retrieve items in row 1 and col 1 and 2. #&gt; C1 C2 #&gt; 1 4 myMatrix[1:2, c(2, 3)] # retrieve items in row 1-2 and col 2-3 #&gt; C2 C3 #&gt; R1 4 7 #&gt; R2 5 8 1.2.3.2 Modify values Modifying items in a matrix is similar to vector but in 2 dimensions. Can you write the code to: replace row 3, column 3 with the value 7? replace row 1, column 4 using the row and col name with the value -5? delete row 2 myMatrix &lt;- matrix(1:12, nrow=3) colnames(myMatrix) &lt;- c(&quot;C1&quot;,&quot;C2&quot;,&quot;C3&quot;,&quot;C4&quot;) rownames(myMatrix) &lt;- c(&quot;R1&quot;,&quot;R2&quot;,&quot;R3&quot;) myMatrix #&gt; C1 C2 C3 C4 #&gt; R1 1 4 7 10 #&gt; R2 2 5 8 11 #&gt; R3 3 6 9 12 # write code to replace row 3, column 3 with the value 7 myMatrix[3,3] &lt;- 7 myMatrix #&gt; C1 C2 C3 C4 #&gt; R1 1 4 7 10 #&gt; R2 2 5 8 11 #&gt; R3 3 6 7 12 # write code to replace row 1, column 4 using the row and col name with the value -5 myMatrix[&quot;R1&quot;,&quot;C4&quot;] &lt;- -5 myMatrix #&gt; C1 C2 C3 C4 #&gt; R1 1 4 7 -5 #&gt; R2 2 5 8 11 #&gt; R3 3 6 7 12 # write code to delete row 2 myMatrix &lt;- myMatrix[-2, ] myMatrix #&gt; C1 C2 C3 C4 #&gt; R1 1 4 7 -5 #&gt; R3 3 6 7 12 1.2.4 List Unlike a vector , a list data structure can have components of mixed data types. More broadly, a list can contain a list of any data structure: value, vector, matrix, etc. We can use str() function to view the structure of a list (or any object). myList &lt;- list(1.70, TRUE, 1:3, &quot;Jason&quot;) str(myList) #&gt; List of 4 #&gt; $ : num 1.7 #&gt; $ : logi TRUE #&gt; $ : int [1:3] 1 2 3 #&gt; $ : chr &quot;Jason&quot; names(myList) &lt;- c(&quot;Height&quot;,&quot;Male&quot;,&quot;Workdays&quot;,&quot;Name&quot;) # give names to elements in list str(myList) #&gt; List of 4 #&gt; $ Height : num 1.7 #&gt; $ Male : logi TRUE #&gt; $ Workdays: int [1:3] 1 2 3 #&gt; $ Name : chr &quot;Jason&quot; 1.2.4.1 Indexing list Different from vector and matrix, for a list, you need to use double-layer square brackets, either by numeric index or name. Alternatively, you can also use $ symbol with the name. # example of retrieving items from a list myList[[4]] # using index #&gt; [1] &quot;Jason&quot; myList[[&quot;Name&quot;]] # using index of Name #&gt; [1] &quot;Jason&quot; myList$Name # using the Name key directly #&gt; [1] &quot;Jason&quot; Can you retrieve the Height and Name from myList? # write code to retrieve Height and Name from myList myList[c(1,4)] #&gt; $Height #&gt; [1] 1.7 #&gt; #&gt; $Name #&gt; [1] &quot;Jason&quot; myList[c(&quot;Height&quot;,&quot;Name&quot;)] # note that unlike the above, the retrieved values are still in a list as opposed to individual values like what you get with double brackets [[]] #&gt; $Height #&gt; [1] 1.7 #&gt; #&gt; $Name #&gt; [1] &quot;Jason&quot; 1.2.4.2 Converting to/from vector It is possible to inter-convert list to and from vectors. # example of interconverting list to vector and back myList &lt;- list(1.70, TRUE, 1:3, &quot;Jason&quot;) str(myList) #&gt; List of 4 #&gt; $ : num 1.7 #&gt; $ : logi TRUE #&gt; $ : int [1:3] 1 2 3 #&gt; $ : chr &quot;Jason&quot; myVector &lt;- unlist(myList) myVector #&gt; [1] &quot;1.7&quot; &quot;TRUE&quot; &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;Jason&quot; myList_cov &lt;- as.list(myVector) str(myList_cov) #&gt; List of 6 #&gt; $ : chr &quot;1.7&quot; #&gt; $ : chr &quot;TRUE&quot; #&gt; $ : chr &quot;1&quot; #&gt; $ : chr &quot;2&quot; #&gt; $ : chr &quot;3&quot; #&gt; $ : chr &quot;Jason&quot; 1.2.5 Data Frame Data frame is widely used for rectangular data, where each column has the same data type (vector) but different columns can have different data types (like Excel) The data frame is in fact a special type of list: A list of vectors with the same length. myDF &lt;- data.frame(&quot;Height&quot; = c(1.70, 1.80, 1.68, 1.72), &quot;Weight&quot; = c(67, 60, 55, 58), &quot;Name&quot; = c(&quot;Jason&quot;, &quot;Ray&quot;, &quot;Dora&quot;, &quot;Yuanhua&quot;), &quot;Gender&quot; = factor(c(&quot;male&quot;,&quot;male&quot;,&quot;female&quot;,&quot;male&quot;))) myDF #&gt; Height Weight Name Gender #&gt; 1 1.70 67 Jason male #&gt; 2 1.80 60 Ray male #&gt; 3 1.68 55 Dora female #&gt; 4 1.72 58 Yuanhua male myDF$Name[3] # print name at index 3 #&gt; [1] &quot;Dora&quot; levels(myDF$Gender) # print levels of gender (which is coded as a factor) #&gt; [1] &quot;female&quot; &quot;male&quot; 1.2.6 Reading/writing data from/to R Now that you know how data can be represented in R in different ways you can read in external data to take advantage of its powerful functions. #install tidyverse (package with lots of functions to handle data in R) if (!requireNamespace(&quot;tidyverse&quot;, quietly = TRUE)) { # checks if already installed install.packages(&quot;tidyverse&quot;) } library(tidyverse) # load the package #&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── #&gt; ✔ dplyr 1.1.4 ✔ readr 2.1.5 #&gt; ✔ forcats 1.0.0 ✔ stringr 1.5.1 #&gt; ✔ ggplot2 3.5.2 ✔ tibble 3.3.0 #&gt; ✔ lubridate 1.9.4 ✔ tidyr 1.3.1 #&gt; ✔ purrr 1.1.0 #&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── #&gt; ✖ dplyr::filter() masks stats::filter() #&gt; ✖ dplyr::lag() masks stats::lag() #&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors You can download myData.csv from Moodle or here. # example of reading in data getwd() #&gt; [1] &quot;Z:/jwhwong/programming/BMDS-book/notebooks/chapter1-R&quot; myData &lt;- read_csv(&quot;C:/Users/Jason/Downloads/myData.csv&quot;) # use the path where you saved the file on your own computer #&gt; Rows: 4 Columns: 4 #&gt; ── Column specification ──────────────────────────────────────────────────────── #&gt; Delimiter: &quot;,&quot; #&gt; chr (2): Name, Gender #&gt; dbl (2): Height, Weigth #&gt; #&gt; ℹ Use `spec()` to retrieve the full column specification for this data. #&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. myData #&gt; # A tibble: 4 × 4 #&gt; Height Weigth Name Gender #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 1.7 67 Jason male #&gt; 2 1.8 60 Ray male #&gt; 3 1.68 55 Dora female #&gt; 4 1.72 58 Yuanhua male We will modify the first row of data and then write it back to file. # example of writing data back to csv myData[1,] &lt;- list(1.5,42,&quot;Sarah&quot;,&quot;female&quot;) write_csv(myData, &quot;C:/Users/Jason/Downloads/myData2.csv&quot;) # uncomment after setting the path to output "],["introHypoTest.html", "Chapter 2 Introduction to Hypothesis testing 2.1 Hypothesis testing and p value 2.2 Permutation test 2.3 t test 2.4 regression-based test 2.5 Multiple testing 2.6 Explore power and sample size (optional) { power }", " Chapter 2 Introduction to Hypothesis testing 2.1 Hypothesis testing and p value If you haven’t heard much about hypothesis testing or p value, you will hear them again and again throughout of your whole journey of scientific study and research. This might be your first time study it, but very likely will not be the last time study it (via other courses or self-study). Its importance will not be over rated, as misuse or misunderstanding can easily lead to errors in scientific conclusions. Before we started with examples, I will concisely summarize the key elements here, so you can come back after read through this chapter (or anytime you need to review). When to perform hypothesis test - unsure if the observed results are caused by randomness (randomness are everywhere but understanding or even admitting them are not always easy) What are the key elements in the hypothesis test - choose a proper testing statistics - approximate the null distribution of the test statistic What is the major challenge? - understand the randomness of certain variables in the system / process - estimate the testing statistic under the null hypothesis - find a more effective test statistic (or the null distribution estimateion) That’s why you need to study more statistical course e.g., STAT2601 for the mathematical of common variable distribution (or its more application-focused alternative BIOF2013) and STAT2602 for advanced skills in deriving analytic or asymptotic null distribution. How surprising is my result? Calculating a p-value There are many circumstances where we simply want to check whether an observation looks like it is compatible with the null hypothesis, \\(H_{0}\\). Having decided on a significance level \\(\\alpha\\) and whether the situation warrants a one-tailed or a two-tailed test, we can use the cdf of the null distribution to calculate a p-value for the observation. Acknowledgement: examples are from Dr John Pinney link here 2.1.1 Example 1: probability of rolling a six? Your arch-nemesis Blofeld always seems to win at ludo, and you have started to suspect him of using a loaded die. You observe the following outcomes from 100 rolls of his die: data = c(6, 1, 5, 6, 2, 6, 4, 3, 4, 6, 1, 2, 5, 6, 6, 3, 6, 2, 6, 4, 6, 2, 5, 4, 2, 3, 3, 6, 6, 1, 2, 5, 6, 4, 6, 2, 1, 3, 6, 5, 4, 5, 6, 3, 6, 6, 1, 4, 6, 6, 6, 6, 6, 2, 3, 1, 6, 4, 3, 6, 2, 4, 6, 6, 6, 5, 6, 2, 1, 6, 6, 4, 3, 6, 5, 6, 6, 2, 6, 3, 6, 6, 1, 4, 6, 4, 2, 6, 6, 5, 2, 6, 6, 4, 3, 1, 6, 6, 5, 5) Do you have enough evidence to confront him? # We will work with the binomial distribution for the observed number of sixes # Write down the hypotheses # H0: p = 1/6 # H1: p &gt; 1/6 # choose a significance level # alpha = 0.01 # number of sixes # number of trials stat_k = sum(data == 6) trials = length(data) print(paste(&quot;number of sixes:&quot;, stat_k)) #&gt; [1] &quot;number of sixes: 43&quot; print(paste(&quot;number of trials:&quot;, trials)) #&gt; [1] &quot;number of trials: 100&quot; # test statistic: number of sixes out of 100 trials # null distribution: dbinom(x, size=100, prob=1/6) # calculate p value p_val = 1 - pbinom(stat_k - 1, size=trials, prob=1/6) print(paste(&quot;Observed statistic is&quot;, stat_k)) #&gt; [1] &quot;Observed statistic is 43&quot; print(paste(&quot;p value is&quot;, p_val)) #&gt; [1] &quot;p value is 5.43908695860296e-10&quot; 2.1.1.1 Visualize the null distribution and the test statistic # plot the probability mass function of null distribution x = seq(0, 101) pmf = dbinom(x, size=100, prob=1/6) df = data.frame(x=x, pmf=pmf, extreme=(x &gt;= stat_k)) library(ggplot2) ggplot(df, aes(x=x)) + geom_point(aes(y=pmf, color=extreme)) + scale_color_manual(values=c(&quot;black&quot;, &quot;red&quot;)) + xlab(&#39;Number of sixes&#39;) + ylab(&#39;Probability Mass Function&#39;) + ggtitle(&#39;Distribution of n_six under the null hypothesis&#39;) 2.2 Permutation test 2.2.1 Example 2: difference in birth weight The birth weights of babies (in kg) have been measured for a sample of mothers split into two categories: nonsmoking and heavy smoking. The two categories are measured independently from each other. Both come from normal distributions The two groups are assumed to have the same unknown variance. data_heavysmoking = c(3.18, 2.84, 2.90, 3.27, 3.85, 3.52, 3.23, 2.76, 3.60, 3.75, 3.59, 3.63, 2.38, 2.34, 2.44) data_nonsmoking = c(3.99, 3.79, 3.60, 3.73, 3.21, 3.60, 4.08, 3.61, 3.83, 3.31, 4.13, 3.26, 3.54) We want to know whether there is a significant difference in mean birth weight between the two categories. # Write down the hypotheses # H0: there is no difference in mean birth weight between groups: d == 0 # H1: there is a difference, d != 0 # choose a significance level # alpha = 0.05 # Define test statistic: difference of group mean stat_mu = mean(data_heavysmoking) - mean(data_nonsmoking) stat_mu #&gt; [1] -0.5156923 2.2.2 Null distribution approximated by resampling #&#39; Simple function to generate permutation distribution get_permutation_null &lt;- function(x1, x2, n_permute=1000) { n1 = length(x1) n2 = length(x2) # pool data sets x_pool = c(x1, x2) null_distr = rep(0, n_permute) for (i in seq(n_permute)) { # split idx = sample(n1 + n2, size=n1) x1_perm = x_pool[idx] x2_perm = x_pool[-idx] # calculate test statistic null_distr[i] = mean(x1_perm) - mean(x2_perm) } return(null_distr) } set.seed(1) perm_null = get_permutation_null(data_heavysmoking, data_nonsmoking) We can plot the histogram of the null distribution obtained by resampling. We can also add line(s) for the values as extreme as observed statistic mu, where we can consider one side or both side as extreme values. df_perm = data.frame(perm_null = perm_null) ggplot(df_perm, aes(x=perm_null)) + geom_histogram(bins=20) + geom_vline(xintercept=stat_mu, linetype=&quot;dashed&quot;, color=&quot;tomato&quot;) + geom_vline(xintercept=-stat_mu, linetype=&quot;dashed&quot;, color=&quot;tomato&quot;) + xlab(&#39;Difference of group mean&#39;) + ylab(&#39;Resampling frequency&#39;) + ggtitle(&#39;Distribution of mu under the null hypothesis&#39;) ## Two tailed p value p_two_tailed = mean(abs(perm_null) &gt;= abs(stat_mu)) p_one_tailed = mean(perm_null &lt; stat_mu) print(paste(&quot;Two tailed p value:&quot;, round(p_two_tailed, 5))) #&gt; [1] &quot;Two tailed p value: 0.003&quot; print(paste(&quot;One (left) tailed p value:&quot;, round(p_one_tailed, 5))) #&gt; [1] &quot;One (left) tailed p value: 0.002&quot; 2.3 t test 2.3.1 Derivation of t distribution Null distribution approximated by \\(t\\) distribution We use the t test to assess whether two samples taken from normal distributions have significantly different means. The test statistic follows a Student’s t-distribution, provided that the variances of the two groups are equal. Other variants of the t-test are applicable under different conditions. The test statistic is \\[ t = \\frac{\\bar{X}_{1} - \\bar{X}_{2}}{s_p \\cdot \\sqrt{\\frac{1}{n_{1}} + \\frac{1}{n_{2}}}} \\] where \\[ s_p = \\sqrt{\\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}} \\] is an estimator of the pooled standard deviation. Under the null hypothesis of equal means, the statistic follows a Student’s t-distribution with \\((n_{1} + n_{2} - 2)\\) degrees of freedom. # Same test statistic: difference of group mean stat_t = mean(data_heavysmoking) - mean(data_nonsmoking) stat_t #&gt; [1] -0.5156923 Calculate parameters for approximate t distribution n_ns = length(data_nonsmoking) n_hs = length(data_heavysmoking) s_ns = sd(data_nonsmoking) # degree of freedom: n-1 s_hs = sd(data_heavysmoking) # the pooled standard deviation sp = sqrt(((n_ns - 1)*s_ns**2 + (n_hs - 1)*s_hs**2) / (n_ns + n_hs - 2)) print(paste0(&quot;Pooled standard deviation:&quot;, sp)) #&gt; [1] &quot;Pooled standard deviation:0.428057812829366&quot; my_std = sp * sqrt(1/n_ns + 1/n_hs) print(paste(&quot;Estimated standard error of mean difference:&quot;, my_std)) #&gt; [1] &quot;Estimated standard error of mean difference: 0.162204962956089&quot; stat_t_scaled = stat_t / my_std print(paste(&quot;Rescaled t statistic:&quot;, stat_t_scaled)) #&gt; [1] &quot;Rescaled t statistic: -3.17926343494134&quot; print(paste(&quot;degree of freedom&quot;, n_hs+n_ns-2)) #&gt; [1] &quot;degree of freedom 26&quot; Here, we focusing the standardized \\(t\\) distribution, namely the variance=1, so let’s re-scale the test statistic by dividing the standard error my_std. xx = seq(-4.5, 4.5, 0.05) xx_pdf = dt(xx, df=n_hs+n_ns-2) df_t_dist = data.frame(x=xx, pdf=xx_pdf) ggplot(df_t_dist, aes(x=x)) + geom_line(aes(y=pdf)) + geom_vline(xintercept=stat_t_scaled, linetype=&quot;dashed&quot;, color=&quot;tomato&quot;) + geom_vline(xintercept=-stat_t_scaled, linetype=&quot;dashed&quot;, color=&quot;tomato&quot;) + xlab(&#39;Difference of group mean&#39;) + ylab(&#39;PDF approximated by t distr.&#39;) + ggtitle(&#39;Distribution of t under the null hypothesis&#39;) # Note, we used multiply 2 just because the t distribution is symmetric, # otherwise, we need calculate both side and add them. pval_t_twoside = pt(stat_t_scaled, df=n_hs+n_ns-2) * 2 print(paste(&#39;t-test p value (two-tailed):&#39;, round(pval_t_twoside, 6))) #&gt; [1] &quot;t-test p value (two-tailed): 0.003793&quot; 2.3.2 Direct use of t.test() In course and most of your future analyses, you can directly use the built-in t.test() function. # Note, we assumed the variance in both groups are the same, # we so need to set var.equal = TRUE t.test(data_nonsmoking, data_heavysmoking, var.equal = TRUE) #&gt; #&gt; Two Sample t-test #&gt; #&gt; data: data_nonsmoking and data_heavysmoking #&gt; t = 3.1793, df = 26, p-value = 0.003793 #&gt; alternative hypothesis: true difference in means is not equal to 0 #&gt; 95 percent confidence interval: #&gt; 0.1822752 0.8491094 #&gt; sample estimates: #&gt; mean of x mean of y #&gt; 3.667692 3.152000 2.4 regression-based test We can also perform t-test in a Generalised linear model (GLM) setting to test if a coefficient is zero or not. Here, we simply use the marketing dataset as an example. # Install datarium library if you haven&#39;t if (!requireNamespace(&quot;datarium&quot;, quietly = TRUE)) { install.packages(&quot;datarium&quot;) } library(datarium) # Load data: then we will have a data.frame with name marketing data(marketing) head(marketing) #&gt; youtube facebook newspaper sales #&gt; 1 276.12 45.36 83.04 26.52 #&gt; 2 53.40 47.16 54.12 12.48 #&gt; 3 20.64 55.08 83.16 11.16 #&gt; 4 181.80 49.56 70.20 22.20 #&gt; 5 216.96 12.96 70.08 15.48 #&gt; 6 10.44 58.68 90.00 8.64 ggplot(marketing, aes(x=newspaper, y=sales)) + geom_point() + geom_smooth(method=lm) #&gt; `geom_smooth()` using formula = &#39;y ~ x&#39; # Fit linear regression res.lm &lt;- lm(sales ~ newspaper, data = marketing) # We can check the test via the summary() function summary(res.lm) #&gt; #&gt; Call: #&gt; lm(formula = sales ~ newspaper, data = marketing) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -13.473 -4.065 -1.007 4.207 15.330 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 14.82169 0.74570 19.88 &lt; 2e-16 *** #&gt; newspaper 0.05469 0.01658 3.30 0.00115 ** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 6.111 on 198 degrees of freedom #&gt; Multiple R-squared: 0.05212, Adjusted R-squared: 0.04733 #&gt; F-statistic: 10.89 on 1 and 198 DF, p-value: 0.001148 glm_t_val = summary(res.lm)$coefficients[&quot;newspaper&quot;, &quot;t value&quot;] xx = seq(-5, 5, 0.01) yy = dt(xx, 198) df_ttest &lt;- data.frame(x=xx, PDF=yy) ggplot(df_ttest, aes(x=x, y=PDF)) + geom_line() + geom_vline(xintercept = glm_t_val, linetype=&quot;dashed&quot;, color=&quot;tomato&quot;) + geom_vline(xintercept = -glm_t_val, linetype=&quot;dashed&quot;, color=&#39;tomato&#39;) 2.5 Multiple testing Hypothetical null distribution. Feel feel to try any null distribution, examples below ## Example null distributions # t, normal or anything. we use chi-squared distribution as an example x_random = rchisq(n=1000, df=3) any_null_dist = dchisq(x_random, df=3) pvals_null = 1 - pchisq(x_random, df=3) 2.5.1 Null distribution (of test statistic) # Null distribution of test statistic hist(x_random) 2.5.2 Null distribution of p value # Null distribution of test statistic hist(pvals_null) 2.5.3 Minimal p values in 10 tests # We use matrix to group 100 trials into a column # We then use apply() to calculate min value for each column pval_null_mtx = matrix(pvals_null, nrow=10) p_min_in10 = apply(pval_null_mtx, MARGIN=2, FUN=min) hist(p_min_in10) print(paste(&#39;Proportion of tests with min(p) &lt; 0.05:&#39;, mean(p_min_in10 &lt; 0.05))) #&gt; [1] &quot;Proportion of tests with min(p) &lt; 0.05: 0.43&quot; 2.6 Explore power and sample size (optional) { power } Make a simulation of score: group A and B B follows normal(mean=0, std=1); A follows normal(mean=0.1, std=1) Generate 100 samples for each group, and do a t test, is difference significant? Please use set.seed(0) beforehand. Try 3) again but general 3,00 samples this time, later 1,000 samples. What do you find? Think the relation between power and sample size. set.seed(0) n_sample = 100 # change this value to 1000 and 10000 xB = rnorm(n_sample) xA = rnorm(n_sample, mean=0.1) t.test(xA, xB, var.equal = TRUE) #&gt; #&gt; Two Sample t-test #&gt; #&gt; data: xA and xB #&gt; t = 0.24294, df = 198, p-value = 0.8083 #&gt; alternative hypothesis: true difference in means is not equal to 0 #&gt; 95 percent confidence interval: #&gt; -0.2261882 0.2897482 #&gt; sample estimates: #&gt; mean of x mean of y #&gt; 0.05444844 0.02266845 "],["introLinearReg.html", "Chapter 3 Introduction to Linear Regression 3.1 Linear Regression Using Simulated Data 3.2 Least Squares Using Simulated Data 3.3 Diagnostic check of a fitted regression model 3.4 Simple Linear Regression with lm function 3.5 Multiple Regression with lm function", " Chapter 3 Introduction to Linear Regression Acknowledgements: this chapter is adapted and updated from the materials originally produced by STAT1005 teaching team, especially Prof. Jeff Yao. 3.1 Linear Regression Using Simulated Data Let’s first simulate some data and look at how the predicted values (Ye) differ from the actual value (Y). 3.1.1 Simulating data: For X, we generate 100 normally distributed random numbers with mean 1.5 and standard deviation 2.5. For predicted value Ye, we assume an intercept (α) of 2 and a slope (β) of 0.3 and we write \\(Y_e = 2 + 0.3 x\\) Later, we will estimate the values of α and β using the least squares method and see how that changes the efficacy of the model. Though we estimate \\(Y_e = \\alpha + \\beta X\\), in reality Y is rarely perfectly linear. It usually has an error component or residual: \\(Y = \\alpha + \\beta X + R\\), where R is a random variable and is assumed to be normally distributed. Therefore for the actual value Y, we add a residual term (res), a random variable distributed normally with mean 0 and a standard deviation of 0.5. The following cell shows the code snippet to generate these numbers and convert these three columns in a data frame. Read through the code carefully and run the cell to output a sample of our simulated data. # Fix seed: each run gives the same random numbers so the same outputs. # Commenting out this line would read similar but different outputs at each run. # Try it out! set.seed(0) # Generate data X = 2.5 * rnorm(100) + 1.5 # Array of 100 values with mean = 1.5, stddev = 2.5 ypred = 2 + 0.3 * X # Prediction of Y, assuming a = 2, b = 0.3 res = 0.5 * rnorm(100) # Generate 100 residual terms yact = 2 + 0.3 * X + res # Actual values of Y # Create dataframe to store our X, ypred, and yact values df = data.frame(&#39;X&#39; = X, &#39;ypred&#39; = ypred, &#39;yact&#39; = yact) # Show the first six rows of our dataframe head(df) #&gt; X ypred yact #&gt; 1 4.6573857 3.397216 3.788145 #&gt; 2 0.6844166 2.205325 1.816937 #&gt; 3 4.8244982 3.447349 3.139354 #&gt; 4 4.6810733 3.404322 3.427612 #&gt; 5 2.5366036 2.760981 2.195788 #&gt; 6 -2.3498751 1.295037 1.583397 Now let’s plot both the actual output (yact) and predicted output (ypred) against the input variable (X) to see what the difference between yact and ypred is, and therefore, to see how accurately the proposed equation (ypred = 2 + 0.3 * X) has been able to predict the value of the output: # You can use basic plotting functions # plot(x=df$X, y=df$yact, col=&quot;red&quot;) # lines(x=df$X, y=df$ypred, col=&quot;darkgreen&quot;) # But let&#39;s use ggplot2 for higher flexibility library(ggplot2) ggplot(df, aes(X)) + # basic graphical object geom_point(aes(y=yact), colour=&quot;black&quot;) + # first layer geom_line(aes(y=ypred), colour=&quot;darkgreen&quot;) + # second layer ggtitle(&#39;Actual vs Predicted values from the dummy dataset&#39;) 3.1.2 Model efficacy How do we know the values we calculate for α and β are giving us a good model? We can explain the total variability in our model with the Total Sum of Squares or SST: \\[SST = \\sum_{i=1}^n\\Bigl(\\text{yact}_i - \\text{yavg}\\Bigr)^2, \\qquad\\qquad \\text{yavg}=\\frac1n \\sum_{i=1}^n \\text{yact}_i\\] Mathematically, we have \\[ \\sum_{i=1}^n\\Bigl(\\text{yact}_i - \\text{yavg}\\Bigr)^2 = \\sum_{i=1}^n\\Bigl(\\text{ypred}_i -\\text{yavg} \\Bigr)^2 + \\sum_{i=1}^n\\Bigl(\\text{yact}_i - \\text{ypred}_i\\Bigr)^2\\] The identity reads as Sum of Squares Total = Sum of Squares Regression + Sum of Squares Error, or simply , SST = SSR + SSE. The Regression Sum of Squares or SSR measures the variation of the regression/predicted values, and the Sum of Squares Error SSE the variation between the actual and the predicted values. An alternative saying is that SSR is the difference explained by the model, SSE is the difference not explained by the model and is random, and SST is the total error. Note, we often use SSE (Sum of Squares Error) and SSD (Sum of Squares Difference) interchangeably. 3.1.3 R-Squared The higher the ratio of SSR to SST, the better the model is. This ratio is quantified by the coefficient of determination (also known as R2 or R-squared): \\[ R^2= \\frac{SSR}{SST}\\] Since \\(SST= SSR+SSE\\), \\(\\qquad 0\\le R^2\\le 1\\). The closer it is to 1, the better the model. Note that there are many other factors that we need to analyse before we can conclude a linear regression model is effective, but a high \\(R^2\\) is a pretty good indicator. Let’s see what the value of \\(R^2\\) is for our simulated dataset. # Calculate the mean of Y ymean = mean(df$yact) print(paste(&#39;Mean of Y =&#39;, ymean)) # paste brings a white space by default #&gt; [1] &quot;Mean of Y = 2.44422555811815&quot; # Calculate SSR and SST df[&#39;SSR&#39;] = (df[&#39;ypred&#39;] - ymean)**2 df[&#39;SST&#39;] = (df[&#39;yact&#39;] - ymean)**2 SSR = sum(df[&#39;SSR&#39;]) SST = sum(df[&#39;SST&#39;]) # Calculate R-squared R2 = SSR / SST print(paste(&#39;R2 =&#39;, R2)) #&gt; [1] &quot;R2 = 0.583160943681119&quot; The value of \\(R^2=0.583\\) suggests that ypred provides a decent prediction of the yact. We have randomly assumed some values for \\(\\alpha\\) and \\(\\beta\\), but these may or may not be the best values. In the next step, we will use the least sum of square method to calculate the optimum value for \\(\\alpha\\) and \\(\\beta\\) to see if there is an improvement in \\(R^2\\). To get started on the next step, open the notebook called 02-linearReg-02.Rmd. 3.2 Least Squares Using Simulated Data Now, using our simulated data from the previous step, let’s estimate the optimum values of our variable coefficients, \\(\\alpha\\) and \\(\\beta\\). Using the predictor variable, X, and the output variable, yact, we will calculate the values of \\(\\alpha\\) and \\(\\beta\\) using the Least Squares method described in the lecture. The cell below creates the same dataframe as previously. Run the cell to get started! set.seed(0) # Generate data X = 2.5 * rnorm(100) + 1.5 # Array of 100 values with mean = 1.5, stddev = 2.5 ypred = 2 + 0.3 * X # Prediction of Y, assuming a = 2, b = 0.3 res = 0.5 * rnorm(100) # Generate 100 residual terms yact = 2 + 0.3 * X + res # Actual values of Y # Create dataframe to store our X, ypred, and yact values df = data.frame(&#39;X&#39; = X, &#39;ypred&#39; = ypred, &#39;yact&#39; = yact) Just to reiterate, here are the formulas for \\(\\alpha\\) and \\(\\beta\\) again: \\[\\hat\\beta=\\frac{\\sum_{i=1}^n(X_i-\\bar X)(Y_i-\\bar Y)}{\\sum_{i=1}^n(X_i-\\bar X)^2}=\\frac{\\text{cov}(X,Y)}{\\text{var}(X)}\\] \\[\\hat\\alpha=\\bar Y-\\hat\\beta * \\bar X\\] To calculate these coefficients, we will create a few more columns in our df data frame. We need to calculate xmean and ymean to calculate the covariance of X and Y (xycov) and the variance of X (xvar) before we can work out the values for alpha and beta. # Calculate the mean of X and Y xmean = mean(X) ymean = mean(yact) # Calculate the terms needed for the numator and denominator of beta df[&#39;xycov&#39;] = (df[&#39;X&#39;] - xmean) * (df[&#39;yact&#39;] - ymean) df[&#39;xvar&#39;] = (df[&#39;X&#39;] - xmean)**2 # Calculate beta and alpha beta = sum(df[&#39;xycov&#39;]) / sum(df[&#39;xvar&#39;]) alpha = ymean - (beta * xmean) print(paste(&#39;alpha =&#39;, alpha, &#39;;&#39;, &#39;beta =&#39;, beta)) #&gt; [1] &quot;alpha = 1.93401265576322 ; beta = 0.327758955833308&quot; As we can see, the values are only a little different from what we had assumed earlier. Let’s see how the value of \\(R^2\\) changes if we use the new values of \\(\\alpha\\) and \\(\\beta\\). The equation for the new model can be written as: \\[ y=1.934 + 0.328 * x \\] Let’s create a new column in df to accommodate the values generated by this equation and call this ypred2, and calculate the new \\(R^2\\). # Create new column to store new predictions df[&#39;ypred2&#39;] = alpha + beta * df[&#39;X&#39;] # Calculate new SSR with new predictions of Y. # Note that SST remains the same since yact and ymean do not change. df[&#39;SSR2&#39;] = (df[&#39;ypred2&#39;] - ymean)**2 df[&#39;SST&#39;] = (df[&#39;yact&#39;] - ymean)**2 SSR2 = sum(df[&#39;SSR2&#39;]) SST = sum(df[&#39;SST&#39;]) # Calculate new R2 R2_2 = SSR2 / SST print(paste(&#39;New R2 =&#39;, R2_2)) #&gt; [1] &quot;New R2 = 0.69524214766491&quot; The new value of \\(R^2= 0.695\\) shows a slight improvement from the previous value of \\(R^2=0.583\\) (obtained with \\(\\alpha=2,~\\beta=0.3\\)). Let’s also plot our new prediction model against the actual values and our earlier assumed model, just to get a better visual understanding. library(ggplot2) # Put color into aes ggplot(df, aes(X)) + # basic graphical object geom_point(aes(y=yact), colour=&quot;black&quot;) + # first layer geom_line(aes(y=ypred, colour=&quot;Guess&quot;)) + # second layer geom_line(aes(y=ypred2, colour=&quot;OLS&quot;)) + # third layer scale_colour_manual(name=&quot;Models&quot;, values = c(&quot;Guess&quot;=&quot;darkgreen&quot;, &quot;OLS&quot;=&quot;red&quot;)) + ggtitle(&#39;Actual vs Predicted with guessed parameters vs Predicted with calculated parameters&#39;) As we can see, the ypred2 and ypred are more or less overlapping since the respective values of ɑ and β are not very different. Next, we will explore other methods of determining model efficacy by using the notebook called 02-linearReg-03.Rmd. 3.3 Diagnostic check of a fitted regression model Apart from the \\(R^2\\) statistic, there are other statistics and parameters that you need to look at in order to determine if the model is efficient. We will discuss some commonly used statistics – Residual Standard Errors, \\(p\\)-values, and \\(F\\)-statistics. 3.3.1 Residual Standard Errors (RSE) RSE is a common statistic used to calculate the accuracy of values predicted by a model. It is an estimate of the variance of the error term, res. For a simple linear regression model, RSE is defined as: \\[ RSE^2 = \\frac{SSE}{n-2} = \\frac1{n-2} \\sum_{i=1}^n \\Bigl(\\text{yact}_i - \\text{ypred}_i \\Bigr)^2. \\] In general, \\[ RSE^2 = \\frac{SSE}{n-p-1} = \\frac1{n-p-1} \\sum_{i=1}^n \\Bigl(\\text{yact}_i - \\text{ypred}_i \\Bigr)^2. \\] where \\(p\\) is the number of predictor variables in a model where we have more than one predictor variables. A multiple linear regression model is a linear regression model with multiple predictors, written as \\[ Y_e = \\alpha +\\beta_1 * X_1 +\\cdots +\\beta_p X_p. \\] As you see, the parameters and predictors are subscripted from 1 up to the number of predictors \\(p\\). In multiple regression, the value of RSE generally decreases as we add variables that are more significant predictors of the output variable. Using our simulated data from the previous steps, the following code snippet shows how the RSE for a model can be calculated: set.seed(0) # Generate data X = 2.5 * rnorm(100) + 1.5 # Array of 100 values with mean = 1.5, stddev = 2.5 res = 0.5 * rnorm(100) # Generate 100 residual terms yact = 2 + 0.3 * X + res # Actual values of Y # Create dataframe to store our X, ypred, and yact values df = data.frame(&#39;X&#39; = X, &#39;yact&#39; = yact) # Calculate the mean of X and Y xmean = mean(X) ymean = mean(yact) # Calculate the terms needed for the numator and denominator of beta df[&#39;xycov&#39;] = (df[&#39;X&#39;] - xmean) * (df[&#39;yact&#39;] - ymean) df[&#39;xvar&#39;] = (df[&#39;X&#39;] - xmean)**2 # Calculate beta and alpha beta = sum(df[&#39;xycov&#39;]) / sum(df[&#39;xvar&#39;]) alpha = ymean - (beta * xmean) print(paste(&#39;alpha =&#39;, alpha, &#39;;&#39;, &#39;beta =&#39;, beta)) #&gt; [1] &quot;alpha = 1.93401265576322 ; beta = 0.327758955833308&quot; # Store predictions as in previous step df[&#39;ypred&#39;] = alpha + beta * df[&#39;X&#39;] # Show first five rows of dataframe head(df) #&gt; X yact xycov xvar ypred #&gt; 1 4.6573857 3.788145 4.1671116 9.6144310 3.460513 #&gt; 2 0.6844166 1.816937 0.5471556 0.7608280 2.158336 #&gt; 3 4.8244982 3.139354 2.2715611 10.6786935 3.515285 #&gt; 4 4.6810733 3.427612 3.0724952 9.7618890 3.468276 #&gt; 5 2.5366036 2.195788 -0.2434518 0.9602676 2.765407 #&gt; 6 -2.3498751 1.583397 3.3628671 15.2611034 1.163820 # Calculate SSE df[&#39;SSE&#39;] = (df[&#39;yact&#39;] - df[&#39;ypred&#39;])**2 SSE = sum(df[&#39;SSE&#39;]) # Calculate RSE RSE = sqrt(SSE / 98) # n = 100 print(paste(&#39;RSE =&#39;, RSE)) #&gt; [1] &quot;RSE = 0.481279277134956&quot; The value of RSE comes out to be 0.48. As you might have guessed, the smaller the residual standard errors, the better the model is. The benchmark to compare this to is the mean of the actual values, yact. As shown previously, this value is ymean = 2.54. In plain English, this means we observe an error of 0.48 over 2.44 - approximately 19.69%. error = RSE / ymean print(paste(&#39;Mean Y =&#39;, ymean)) #&gt; [1] &quot;Mean Y = 2.44422555811815&quot; print(paste(&#39;Error =&#39;, error)) #&gt; [1] &quot;Error = 0.196904608716023&quot; 3.3.2 p-values The calculation of \\(\\alpha\\) and \\(\\beta\\) are estimates, not exact calculations. Whether their values are significant or not needs to be tested using a hypothesis test. In the equation, \\(Y = \\alpha + \\beta X\\), if we set \\(\\beta=0\\), there will be no relation between \\(Y\\) and \\(X\\). Therefore, the hypothesis tests whether the value of \\(\\beta\\) is non-zero or not. \\[\\begin{align*} \\text{Null hypothesis}~ H_0~:~ \\beta=0, &amp; \\quad \\text{versus} \\\\ \\text{Alternative hypothesis}~ H_1~:~ \\beta\\ne 0.&amp; \\end{align*} \\] Whenever a regression task is performed and \\(\\beta\\) is calculated, there will be an accompanying p-value corresponding to this hypothesis test. We will not go through how this is calculated in this course (you can learn more here), since it is calculated automatically by ready-made methods in R. If the p-value is less than a chosen significance level (e.g. 0.05) then the null hypothesis that \\(\\beta = 0\\) is rejected and \\(\\beta\\) is said to be significant and non-zero. In the case of multiple linear regression, the p-value associated with each \\(\\beta_k\\) can be used to weed out insignificant predictors from the model. The higher the p-value for \\(\\beta_k\\), the less significant \\(X_k\\) is to the model. 3.3.3 F-statistics In a multiple regression model, apart from testing the significance of individual variables by checking the p-values, it is also necessary to check whether, as a group all the predictors are significant. This can be done using the following hypothesis: \\[\\begin{align*} \\text{Null hypothesis}~ H_0~:~ &amp; \\beta_1=\\beta_2=\\cdots=\\beta_p=0, \\quad \\text{versus} \\\\ \\text{Alternative hypothesis}~ H_1~:~&amp; \\text{at least one of the} ~\\beta_k&#39;s ~ \\text{is non zero}. \\end{align*} \\] The statistic that is used to test this hypothesis is called the F-statistic and is defined as follows: \\[ F\\text{-statistic} = \\text{Fisher statistic}= \\frac{ (SST-SSE)/p}{ SSE/(n-p-1)} \\] where \\(n\\) = number of rows (sample points) in the dataset and \\(p\\) = number of predictor variables in the model. There is a \\(p\\)-value that is associated with this \\(F\\)-statistic. If the \\(p\\)-value is smaller than the chosen significance level, the null hypothesis can be rejected. It is important to look at the F-statistic because: p-values are about individual relationships between predictors and the outcome variable. However, one predictor’s relationship with the output might be impacted by the presence of other variables. When the number of predictors in the model is very large and all the \\(\\beta_i\\) are very close to zero, the individual p-values associated with the predictors might give very small values so we might incorrectly conclude that there is a relationship between the predictors and the outcome. 3.4 Simple Linear Regression with lm function There are a few R packages, e.g., the built-in stat package have a lm (linear model) function to fit linear regression very easy - much easier than implementing from scratch like we did in the last lesson. See more details in the lm manual. We will start with the datarium library which contain the advertising data. # Install datarium library if you haven&#39;t if (!requireNamespace(&quot;datarium&quot;, quietly = TRUE)) { install.packages(&quot;datarium&quot;) } library(datarium) # Load data: then we will have a data.frame with name marketing data(marketing) head(marketing) #&gt; youtube facebook newspaper sales #&gt; 1 276.12 45.36 83.04 26.52 #&gt; 2 53.40 47.16 54.12 12.48 #&gt; 3 20.64 55.08 83.16 11.16 #&gt; 4 181.80 49.56 70.20 22.20 #&gt; 5 216.96 12.96 70.08 15.48 #&gt; 6 10.44 58.68 90.00 8.64 We can also check summary statistics of each column summary(marketing) #&gt; youtube facebook newspaper sales #&gt; Min. : 0.84 Min. : 0.00 Min. : 0.36 Min. : 1.92 #&gt; 1st Qu.: 89.25 1st Qu.:11.97 1st Qu.: 15.30 1st Qu.:12.45 #&gt; Median :179.70 Median :27.48 Median : 30.90 Median :15.48 #&gt; Mean :176.45 Mean :27.92 Mean : 36.66 Mean :16.83 #&gt; 3rd Qu.:262.59 3rd Qu.:43.83 3rd Qu.: 54.12 3rd Qu.:20.88 #&gt; Max. :355.68 Max. :59.52 Max. :136.80 Max. :32.40 This dataset contains data about the advertising budget spent on YouTub, Radio, and Newspapers for a particular product and the resulting sales. We expect a positive correlation between such advertising costs and sales. Let’s start with YouTub advertising costs to create a simple linear regression model. First let’s plot the variables to get a better sense of their relationship: # Create scatter plot library(ggplot2) ggplot(marketing, aes(x=youtube, y=sales)) + geom_point(colour=&quot;black&quot;) + ggtitle(&#39;YouTube vs Sales&#39;) As YouTube advertisement cost increases, sales also increase – they are positively correlated! Now with the linear model lm function, let’s create a line of best fit using the least sum of square method. # Fit linear regression # By default it include an incepter, so it is equvialent to add &quot;+ 1&quot; # res.lm &lt;- lm(sales ~ youtube + 1, data = marketing) res.lm &lt;- lm(sales ~ youtube, data = marketing) In the above code, we used lm to fit our simple linear regression model. This takes the formula y ~ X, where X is the predictor variable (YouTube advertising costs) and y is the output variable (Sales). Then, this function will return fitted model via a ordinary least squares (OLS) method. The res.lm is a list, you can get the it attributes by e.g., res.lm$coefficients res.lm$coefficients #&gt; (Intercept) youtube #&gt; 8.43911226 0.04753664 In the notation that we have been using, \\(\\alpha\\) is the intercept and \\(\\beta\\) is the slope i.e.: \\(\\alpha = 8.439, \\quad \\beta = 0.048\\) Thus, the equation for the model will be: \\(\\text{Sales} = 8.439 + 0.048*\\text{YouTube}\\) Let’s also check an indicator of the model efficacy, R2. Luckily, summary function can calculate it from the lm output and gives us a ready-made method for doing this so we don’t need to code all the math ourselves: res_summary = summary(res.lm) # Again, res_summary is also a list res_summary$r.squared #&gt; [1] 0.6118751 We can also take a look at the model summary by writing this snippet: # Print out the summary summary(res.lm) #&gt; #&gt; Call: #&gt; lm(formula = sales ~ youtube, data = marketing) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -10.0632 -2.3454 -0.2295 2.4805 8.6548 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 8.439112 0.549412 15.36 &lt;2e-16 *** #&gt; youtube 0.047537 0.002691 17.67 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 3.91 on 198 degrees of freedom #&gt; Multiple R-squared: 0.6119, Adjusted R-squared: 0.6099 #&gt; F-statistic: 312.1 on 1 and 198 DF, p-value: &lt; 2.2e-16 There is a lot here. Of these results, we have discussed: R-squared F-statistic Prob (F-statistic) - this is the p-value of the F-statistic Intercept coef - this is alpha YouTub coef - this is beta for predictor YouTub P&gt;|t| - this is the p-value for our coefficients Now that we’ve fit a simple regression model, we can try to predict the values of sales based on the equation we just derived! sales_pred = predict(res.lm, newdata = marketing[c(&#39;youtube&#39;)]) marketing[&#39;sales_pred&#39;] = sales_pred The predict fucntion predicts sales value for each row based on the model equation using YouTub costs. This is the equivalent of manually typing out our equation: sales_pred = 8.439 + 0.048*(advert['youtube']). We can visualise our regression model by plotting sales_pred against the YouTube advertising costs to find the line of best fit: library(ggplot2) ggplot(marketing, aes(x=youtube)) + geom_point(aes(y=sales), colour=&quot;black&quot;) + geom_line(aes(y=sales_pred), colour=&quot;red&quot;) + ggtitle(&#39;YouTube vs Sales&#39;) In the next step, we will add more features as predictors and see whether it improves our model. Go to the the notebook called 02-linearReg-05.Rmd. 3.5 Multiple Regression with lm function A multiple linear regression is simply a linear regression that involves more than one predictor variable. It is represented as: \\[\\qquad Y_e = \\alpha + \\beta_1*X_1 + \\beta_2*X_2 + \\dots + \\beta_p*X_p\\] Each βi will be estimated using the least sum of squares method. The data set is \\[ \\begin{array} {~~} Y_1, &amp; X_1^{(1)}, &amp; \\ldots, &amp; X_p^{(1)} \\\\ Y_2, &amp; X_1^{(2)}, &amp; \\ldots, &amp; X_p^{(2)} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ Y_n, &amp; X_1^{(n)}, &amp; \\ldots, &amp; X_p^{(n)} \\end{array} \\] For each sample \\(i\\), the predicted value by the model is: \\(\\qquad Y_{i,e} = \\alpha + \\beta_1*X_1^{(i)} + \\beta_2*X_2^{(i)} + \\dots + \\beta_p*X_p^{(i)}\\) Define the sum of squares \\[ S(\\alpha,\\beta_1,\\ldots,\\beta_p) = \\sum_{i=1}^n \\left\\{ Y_i -Y_{i,e}\\right\\}^2 =\\sum_{i=1}^n \\left\\{ Y_i -\\left( \\alpha + \\beta_1*X_1^{(i)} + \\beta_2*X_2^{(i)} + \\dots + \\beta_p*X_p^{(i)}\\right)\\right\\}^2 \\] Least squares estimators: solve \\[ \\frac{\\partial S(\\alpha,\\beta_1,\\ldots,\\beta_p)}{\\partial \\alpha}=0,\\quad \\frac{\\partial S (\\alpha,\\beta_1,\\ldots,\\beta_p)}{\\partial \\beta_1}=0,\\quad \\ldots,\\quad \\frac{\\partial S (\\alpha,\\beta_1,\\ldots,\\beta_p)}{\\partial \\beta_p}=0. \\] to obtain the least squares estimators of the parameters \\[ \\hat\\alpha, \\hat\\beta_1,\\ldots,\\hat\\beta_p. \\] Note that be definition, \\[ SSE = S(\\hat\\alpha, \\hat\\beta_1,\\ldots,\\hat\\beta_p). \\] In other words, the fitted SSE (sum of squares error) is the minimized value of the sum squares with the estimated values of the parameters. The more varibles, the smaller the \\(R^2\\) Consider two regression models \\(\\quad ~ Y_e = \\alpha + \\beta_1*X_1\\) \\(\\quad \\tilde Y_e = \\alpha + \\beta_1*X_1 + \\beta_2*X_2\\) The model (II) has one more input variable \\(X_2\\). The \\(SSE_I\\) of Model (I) is the minimum of \\[ S_I(\\alpha,\\beta_1) = \\sum_{i=1}^n \\left\\{ Y_i -\\left( \\alpha + \\beta_1*X_1^{(i)} \\right)\\right\\}^2 \\] over all possible values of \\((\\alpha,\\beta_1)\\). The \\(SSE_{II}\\) of Model (II) is the minimum of \\[ S_{II}(\\alpha,\\beta_1,\\beta_2) = \\sum_{i=1}^n \\left\\{ Y_i -\\left( \\alpha + \\beta_1*X_1^{(i)} +\\beta_2*X_2^{(i)} \\right)\\right\\}^2. \\] over all possible values of \\((\\alpha,\\beta_1,\\beta_2)\\). Because \\(\\quad S_I(\\alpha,\\beta_1) = S_{II}(\\alpha,\\beta_1,\\beta_2=0 )\\), we find that \\(SSE_{II}\\le SSE_I\\), so \\[ R^2_{II} = SST - SSE_{II} \\ge SST - SSE_{I} = R^2_{I}. \\] With this simple dataset of three predictor variables, there can be seven possible models: Sales ~ YouTube Sales ~ Newspaper Sales ~ Facebook Sales ~ YouTube + Facebook Sales ~ YouTube + Newspaper Sales ~ Newspaper + Facebook Sales ~ YouTube + Facebook + Newspaper Generally, if there are p possible predictor variables, there can be (2p - 1) possible models – this can get large very quickly! Thankfully, there are a few guidelines to filter some of these and then navigate towards the most efficient one. Keep variables with low p-values and eliminate ones with high p-values Keep variables that increase the value of adjusted-R2 – this penalizes the model for adding insignificant variables and increases when we add significant variables. It is calculated by: \\[ R^2_{adj} = 1- (1-R^2) \\frac{n-1}{n-p-1}\\] Based on these guidelines, there are two approaches to select the predictor variables in the final model: Forward selection: start with a null model (no predictors), then add predictors one by one. If the p-value for the variable is small enough and the value of the adjusted-R2 goes up, the predictor is included in the model. Otherwise, it is not included. Backward selection: starts with a model that has all the possible predictors and discard some of them. If the p-value of a predictor variable is large and adjusted-R2 is lower when removed, it is discarded from the model. Otherwise, it remains a part of the model. Many statistical programs give us an option to select from these approaches while implementing multiple linear regression. For now, let’s manually add a few variables and see how it changes the model parameters and efficacy. First, add the newspaper variable to the model: library(datarium) data(marketing) head(marketing) #&gt; youtube facebook newspaper sales #&gt; 1 276.12 45.36 83.04 26.52 #&gt; 2 53.40 47.16 54.12 12.48 #&gt; 3 20.64 55.08 83.16 11.16 #&gt; 4 181.80 49.56 70.20 22.20 #&gt; 5 216.96 12.96 70.08 15.48 #&gt; 6 10.44 58.68 90.00 8.64 res_lm2 = lm(sales ~ youtube + newspaper, data=marketing) summary(res_lm2) #&gt; #&gt; Call: #&gt; lm(formula = sales ~ youtube + newspaper, data = marketing) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -10.3477 -2.0815 -0.1138 2.2711 10.1415 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 6.929938 0.630405 10.993 &lt; 2e-16 *** #&gt; youtube 0.046901 0.002581 18.173 &lt; 2e-16 *** #&gt; newspaper 0.044219 0.010174 4.346 2.22e-05 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 3.745 on 197 degrees of freedom #&gt; Multiple R-squared: 0.6458, Adjusted R-squared: 0.6422 #&gt; F-statistic: 179.6 on 2 and 197 DF, p-value: &lt; 2.2e-16 As you see, the p-values for the coefficients are very small, suggesting that all the estimates are significant. The equation for this model will be: \\[ \\text{Sales} = 6.93+0.046* \\text{YouTube} + 0.044 * \\text{Newspaper}\\] The values of R2 and adjusted R2 are 0.646 and 0.642, which is just a minor improvement from before (0.612 and 0.610, respectively). Similarly for RSE (3.745). Only a small decrease in RSE and error… Let’s take a closer look at the summary above. The Adj-R2 increases slightly, but the F-statistic decreases (from 312.1 to 179.6), as does the associated p-value. This suggests that adding newspaper didn’t improve the model significantly. Let’s try adding facebook instead: # Initialise and fit new model with TV and Radio as predictors # model3 = smf.ols(&#39;Sales ~ TV + Radio&#39;, data=advert).fit() # print(model3.summary()) res_lm3 = lm(sales ~ youtube + facebook, data=marketing) summary(res_lm3) #&gt; #&gt; Call: #&gt; lm(formula = sales ~ youtube + facebook, data = marketing) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -10.5572 -1.0502 0.2906 1.4049 3.3994 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 3.50532 0.35339 9.919 &lt;2e-16 *** #&gt; youtube 0.04575 0.00139 32.909 &lt;2e-16 *** #&gt; facebook 0.18799 0.00804 23.382 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 2.018 on 197 degrees of freedom #&gt; Multiple R-squared: 0.8972, Adjusted R-squared: 0.8962 #&gt; F-statistic: 859.6 on 2 and 197 DF, p-value: &lt; 2.2e-16 This gives us the model: \\[ \\text{Sales} = 3.51+0.046* \\text{YouTube} + 0.188 * \\text{Facebook}\\] The adjusted R2 value has improved considerably, as did the RSE and F-statistic, indicating an efficient model. Thus, we can conclude that facebook is a great addition to the model. YouTube and facebook advertising costs together are able to predict sales well. But, can we improve it a bit further by combining all three predictor variables? Try it out: see if you can figure out how to do this on your own! # Initialise and fit new model with TV, Newspaper, and Radio as predictors # Print summary of regression results # Calculate RSE - don&#39;t forget that the number of predictors p is now 3 You should get the equation: \\[ \\text{Sales} = 3.53+0.046*\\text{YouTube} -0.001*\\text{Newspaper} +0.188*\\text{Facebook}\\] You should also find that: RSE increases slightly, the coefficient for newspaper is negative, and the F-statistic decreases considerably from 859.6 to 570.3. All these suggest that the model actually became less efficient on addition of newspaper. Why? This step shows clearly that adding one more input variable Newspaper in Model 3 does not lead to any improvement. "],["introClassifier.html", "Chapter 4 Introduction to Classification 4.1 Visualise logistic and logit functions 4.2 Logistic regression on Diabetes 4.3 Cross-validation 4.4 More assessment metrics", " Chapter 4 Introduction to Classification 4.1 Visualise logistic and logit functions In this chapter, we will focus on logistic regression for classification. Let’s first look at what logistic and logit function look like. 4.1.1 Logistic function Let’s write our first function logestic() as follows. # Write your first function logistic &lt;- function(y) { exp(y) / (1 + exp(y)) } # Try it with different values: logistic(0.1) #&gt; [1] 0.5249792 logistic(c(-3, -2, 0.5, 3, 5)) #&gt; [1] 0.04742587 0.11920292 0.62245933 0.95257413 0.99330715 This is the equivalent to the built-in plogis() function in the stat package for the logistic distribution: plogis(0.1) #&gt; [1] 0.5249792 plogis(c(-3, -2, 0.5, 3, 5)) #&gt; [1] 0.04742587 0.11920292 0.62245933 0.95257413 0.99330715 4.1.2 Logit function Now, let look at the logistic’s inverse function logit(), and let’s define it manually. Note, this function only support input between 0 and 1. # Write your first function logit &lt;- function(x) { log(x / (1 - x)) } # Try it with different values: logit(0.4) #&gt; [1] -0.4054651 logit(c(0.2, 0.3, 0.5, 0.7, 0.9)) #&gt; [1] -1.3862944 -0.8472979 0.0000000 0.8472979 2.1972246 logit(c(-1, 2, 0.4)) #&gt; Warning in log(x/(1 - x)): NaNs produced #&gt; [1] NaN NaN -0.4054651 Again, the built-in stat package’s logistic distribution has an equivalent function qlogis(), though with a different name. qlogis(0.4) #&gt; [1] -0.4054651 qlogis(c(0.2, 0.3, 0.5, 0.7, 0.9)) #&gt; [1] -1.3862944 -0.8472979 0.0000000 0.8472979 2.1972246 qlogis(c(-1, 2, 0.4)) #&gt; Warning in qlogis(c(-1, 2, 0.4)): NaNs produced #&gt; [1] NaN NaN -0.4054651 4.1.3 Visualise the distribution Logisitc function # You can use seq() function to generate a vector # Check how to use it by help(seq) or ?seq x = seq(-7, 7, 0.3) df = data.frame(&#39;x&#39;=x, &#39;logistic&#39;=plogis(x)) # You can plot by plot function # plot(x=df$x, y=df$logistic, type=&#39;o&#39;) # Or ggplot2 library(ggplot2) ggplot(df, aes(x=x, y=logistic)) + geom_point() + geom_line() Logit function x = seq(0.001, 0.999, 0.01) df = data.frame(&#39;x&#39;=x, &#39;logit&#39;=qlogis(x)) ggplot(df, aes(x=x, y=logit)) + geom_point() + geom_line() 4.2 Logistic regression on Diabetes 4.2.1 Load Pima Indians Diabetes Database This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage. The datasets consist of several medical predictor (independent) variables and one target (dependent) variable, Outcome. Independent variables include the number of pregnancies the patient has had, their BMI, insulin level, age, and so on. Acknowledgement: This notebook is adapted and updated from STAT1005. # Install the mlbench library for loading the datasets if (!requireNamespace(&quot;mlbench&quot;, quietly = TRUE)) { install.packages(&quot;mlbench&quot;) } # Load data library(mlbench) data(PimaIndiansDiabetes) # Check the first few lines dim(PimaIndiansDiabetes) #&gt; [1] 768 9 head(PimaIndiansDiabetes) #&gt; pregnant glucose pressure triceps insulin mass pedigree age diabetes #&gt; 1 6 148 72 35 0 33.6 0.627 50 pos #&gt; 2 1 85 66 29 0 26.6 0.351 31 neg #&gt; 3 8 183 64 0 0 23.3 0.672 32 pos #&gt; 4 1 89 66 23 94 28.1 0.167 21 neg #&gt; 5 0 137 40 35 168 43.1 2.288 33 pos #&gt; 6 5 116 74 0 0 25.6 0.201 30 neg Now, let’s check two potential features: glucose and age, colored by the diabetes labels. library(ggplot2) ggplot(data=PimaIndiansDiabetes, aes(x=glucose, y=age)) + geom_point(aes(color=diabetes)) Before we start fit models, let’s split the data into training and test sets in a 4:1 ratio. Let define it manually, though there are functions to do it automatically. set.seed(0) idx_train = sample(nrow(PimaIndiansDiabetes), size=0.75*nrow(PimaIndiansDiabetes), replace = FALSE) df_train = PimaIndiansDiabetes[idx_train, ] df_test = PimaIndiansDiabetes[-idx_train, ] # recall the meaning of negative symbol 4.2.2 Fit logistic regression In logistic regression, the predicted probability to be class 1 is: \\[P(y=1|X, W) = \\sigma(w_0, x_1 * w_1 + ... + x_p * w_p)\\] where the \\(\\sigma()\\) denotes the logistic function. In R, the built-in package stats already have functions to fit generalised linear model (GLM), including logistic regression, a type of GML. Here, let’s start with the whole dataset to fit a logistic regression. Note, we will specify the model family as binomial, as the likelihood we are using in logistic regression is a Bernoulli likelihood, a special case of binomial likelihood when the total trial n=1. # Define formula in different ways # my_formula = as.formula(diabetes ~ glucose + age) # my_formula = as.formula(paste(colnames(PimaIndiansDiabetes)[1:8], collapse= &quot; + &quot;)) # my_formula = as.formula(diabetes ~ .) # Fit logistic regression glm_res &lt;- glm(diabetes ~ ., data=df_train, family = binomial) # We can use the logLik() function to obtain the log likelihood logLik(glm_res) #&gt; &#39;log Lik.&#39; -281.9041 (df=9) We can use summary() function to see more details about the model fitting. summary(glm_res) #&gt; #&gt; Call: #&gt; glm(formula = diabetes ~ ., family = binomial, data = df_train) #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -8.044602 0.826981 -9.728 &lt; 2e-16 *** #&gt; pregnant 0.130418 0.036080 3.615 0.000301 *** #&gt; glucose 0.032196 0.004021 8.007 1.18e-15 *** #&gt; pressure -0.017158 0.006103 -2.811 0.004934 ** #&gt; triceps -0.003425 0.007659 -0.447 0.654752 #&gt; insulin -0.001238 0.001060 -1.169 0.242599 #&gt; mass 0.104029 0.018119 5.741 9.39e-09 *** #&gt; pedigree 0.911030 0.344362 2.646 0.008156 ** #&gt; age 0.012980 0.010497 1.237 0.216267 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 756.83 on 575 degrees of freedom #&gt; Residual deviance: 563.81 on 567 degrees of freedom #&gt; AIC: 581.81 #&gt; #&gt; Number of Fisher Scoring iterations: 5 4.2.3 Assess on test data Now, we can evaluate the accuracy of the model on the 25% test data. # Train the full model on the training data glm_train &lt;- glm(diabetes ~ ., data=df_train, family = binomial) # Predict the probability of being diabeties on test data # We can also set a threshold, e.g., 0.5 for the predicted label pred_prob = predict(glm_train, df_test, type = &quot;response&quot;) pred_label = pred_prob &gt;= 0.5 # Observed label obse_label = df_test$diabetes == &#39;pos&#39; # Calculate the accuracy on test data # think how accuracy is defined # we can use (TN + TP) / (TN + TP + FN + FP) # we can also directly compare the proportion of correctness accuracy = mean(pred_label == obse_label) print(paste(&quot;Accuracy on test set:&quot;, accuracy)) #&gt; [1] &quot;Accuracy on test set: 0.796875&quot; 4.2.4 Model selection and diagnosis 4.2.4.1 Model2: New feature set by removing triceps # Train the full model on the training data glm_mod2 &lt;- glm(diabetes ~ pregnant + glucose + pressure + insulin + mass + pedigree + age, data=df_train, family = binomial) logLik(glm_mod2) #&gt; &#39;log Lik.&#39; -282.0038 (df=8) summary(glm_mod2) #&gt; #&gt; Call: #&gt; glm(formula = diabetes ~ pregnant + glucose + pressure + insulin + #&gt; mass + pedigree + age, family = binomial, data = df_train) #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -8.0317567 0.8251403 -9.734 &lt; 2e-16 *** #&gt; pregnant 0.1308094 0.0361230 3.621 0.000293 *** #&gt; glucose 0.0324606 0.0039854 8.145 3.80e-16 *** #&gt; pressure -0.0175651 0.0060269 -2.914 0.003563 ** #&gt; insulin -0.0014402 0.0009593 -1.501 0.133291 #&gt; mass 0.1018155 0.0173811 5.858 4.69e-09 *** #&gt; pedigree 0.9000134 0.3428652 2.625 0.008665 ** #&gt; age 0.0131238 0.0105147 1.248 0.211982 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 756.83 on 575 degrees of freedom #&gt; Residual deviance: 564.01 on 568 degrees of freedom #&gt; AIC: 580.01 #&gt; #&gt; Number of Fisher Scoring iterations: 5 # Predict the probability of being diabeties on test data # We can also set a threshold, e.g., 0.5 for the predicted label pred_prob2 = predict(glm_mod2, df_test, type = &quot;response&quot;) pred_label2 = pred_prob2 &gt;= 0.5 accuracy2 = mean(pred_label2 == obse_label) print(paste(&quot;Accuracy on test set with model2:&quot;, accuracy2)) #&gt; [1] &quot;Accuracy on test set with model2: 0.807291666666667&quot; 4.2.4.2 Model3: New feature set by removing triceps and insulin # Train the full model on the training data glm_mod3 &lt;- glm(diabetes ~ pregnant + glucose + pressure + mass + pedigree + age, data=df_train, family = binomial) logLik(glm_mod3) #&gt; &#39;log Lik.&#39; -283.1342 (df=7) summary(glm_mod3) #&gt; #&gt; Call: #&gt; glm(formula = diabetes ~ pregnant + glucose + pressure + mass + #&gt; pedigree + age, family = binomial, data = df_train) #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -7.797803 0.802287 -9.719 &lt; 2e-16 *** #&gt; pregnant 0.130990 0.035957 3.643 0.00027 *** #&gt; glucose 0.030661 0.003755 8.164 3.23e-16 *** #&gt; pressure -0.017847 0.005953 -2.998 0.00272 ** #&gt; mass 0.097356 0.016969 5.737 9.61e-09 *** #&gt; pedigree 0.824150 0.338299 2.436 0.01484 * #&gt; age 0.015134 0.010426 1.452 0.14663 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 756.83 on 575 degrees of freedom #&gt; Residual deviance: 566.27 on 569 degrees of freedom #&gt; AIC: 580.27 #&gt; #&gt; Number of Fisher Scoring iterations: 5 # Predict the probability of being diabeties on test data # We can also set a threshold, e.g., 0.5 for the predicted label pred_prob3 = predict(glm_mod3, df_test, type = &quot;response&quot;) pred_label3 = pred_prob3 &gt;= 0.5 accuracy3 = mean(pred_label3 == obse_label) print(paste(&quot;Accuracy on test set with model3:&quot;, accuracy3)) #&gt; [1] &quot;Accuracy on test set with model3: 0.786458333333333&quot; 4.3 Cross-validation In last section, we split the whole dataset into 75% for training and 25% for testing. However, when the dataset is small, the test set may not be big enough and introduce high variance on the assessment. One way to reduce this variance in assessment is performing cross-validation, where we split the data into K folds and use K-1 folds for training and the remaining fold for testing. This procedure will be repeated for fold 1 to fold K as testing fold and all folds will be aggregated for joint assessment. K is usually taken 3, 5 or 10. In extreme case that K=n_sample, we call it leave-one-out cross-validation (LOOCV). Let’s load the dataset (again) first. # Load data library(mlbench) data(PimaIndiansDiabetes) Besides implement the cross-validation from scratch, there are packages supporting it well, including caret package. We will install it and use it for cross-validation here. Note, in order to calculate ROC curve later, we need to keep the predicted probability by using classProbs = TRUE # Install the caret library for cross-validation if (!requireNamespace(&quot;caret&quot;, quietly = TRUE)) { install.packages(&quot;caret&quot;) } library(caret) #&gt; Loading required package: lattice #&gt; #&gt; Attaching package: &#39;caret&#39; #&gt; The following object is masked from &#39;package:purrr&#39;: #&gt; #&gt; lift # Define training control # We also want to have savePredictions=TRUE &amp; classProbs=TRUE set.seed(0) my_trControl &lt;- trainControl(method = &quot;cv&quot;, number = 5, classProbs = TRUE, savePredictions = TRUE) # Train the model cv_model &lt;- train(diabetes ~ ., data = PimaIndiansDiabetes, method = &quot;glm&quot;, family=binomial(), trControl = my_trControl) # Summarize the results print(cv_model) #&gt; Generalized Linear Model #&gt; #&gt; 768 samples #&gt; 8 predictor #&gt; 2 classes: &#39;neg&#39;, &#39;pos&#39; #&gt; #&gt; No pre-processing #&gt; Resampling: Cross-Validated (5 fold) #&gt; Summary of sample sizes: 615, 614, 615, 614, 614 #&gt; Resampling results: #&gt; #&gt; Accuracy Kappa #&gt; 0.7708344 0.4695353 We can also access to detailed prediction results after concatenating the K folds: head(cv_model$pred) #&gt; pred obs neg pos rowIndex parameter Resample #&gt; 1 neg neg 0.9656694 0.03433058 4 none Fold1 #&gt; 2 neg neg 0.8581071 0.14189290 6 none Fold1 #&gt; 3 neg pos 0.9508306 0.04916940 7 none Fold1 #&gt; 4 neg pos 0.6541361 0.34586388 17 none Fold1 #&gt; 5 neg pos 0.7675666 0.23243342 20 none Fold1 #&gt; 6 neg pos 0.6132685 0.38673152 26 none Fold1 We can double check the accuracy: CV_acc = mean(cv_model$pred$pred == cv_model$pred$obs) print(paste(&quot;Accuracy via 5-fold cross-validation&quot;, CV_acc)) #&gt; [1] &quot;Accuracy via 5-fold cross-validation 0.770833333333333&quot; 4.4 More assessment metrics 4.4.1 Two types of error In the above sections, we used the accuracy to perform model diagnosis, either only on one testing dataset or aggregating cross multiple folds in cross- validation. Accuracy is a widely used metric for model evaluation, on the averaged error rate. However, this metric still have limitations when assessing the model performance, especially the following two: When the samples are highly imbalance, high accuracy may not mean a good model. For example, for a sample with 990 negative samples and 10 positive samples, a simple model by predicting for all sample as negative will give an accuracy of 0.99. Thus, for highly imbalanced samples, we should be careful when interpreting the accuracy. In many scenarios, our tolerance on false positive errors and false negative errors may be different and we want to know both for a certain model. They are often called as type I and II errors: Type I error: false positive (rate) Type II error: false negative (rate) - a joke way to remember what type II mean Negative has two stripes. Here, we use the diabetes dataset and their cross-validation results above to illustrate the two types of errors and the corresponding model performance evaluation. # Let&#39;s start to define the values for the confusion matrix first # Recall what the difference between &amp; vs &amp;&amp; # Read more: https://stat.ethz.ch/R-manual/R-devel/library/base/html/Logic.html TP = sum((cv_model$pred$obs == &#39;pos&#39;) &amp; (cv_model$pred$pred == &#39;pos&#39;)) FN = sum((cv_model$pred$obs == &#39;pos&#39;) &amp; (cv_model$pred$pred == &#39;neg&#39;)) FP = sum((cv_model$pred$obs == &#39;neg&#39;) &amp; (cv_model$pred$pred == &#39;pos&#39;)) TN = sum((cv_model$pred$obs == &#39;neg&#39;) &amp; (cv_model$pred$pred == &#39;neg&#39;)) print(paste(&#39;TP, FN, FP, TN:&#39;, TP, FN, FP, TN)) #&gt; [1] &quot;TP, FN, FP, TN: 151 117 59 441&quot; We can also use the table() function to get the whole confusion matrix. Read more about the table function for counting the frequency of each element. A similar way is the confusionMatrix() in caret package. # Calculate confusion matrix confusion_mtx = table(cv_model$pred[, c(&quot;obs&quot;, &quot;pred&quot;)]) confusion_mtx #&gt; pred #&gt; obs neg pos #&gt; neg 441 59 #&gt; pos 117 151 # similar function confusionMatrix # conf_mat = confusionMatrix(cv_model$pred$pred, cv_model$pred$obs) # conf_mat$table We can also plot out the confusion matrix # Change to data.frame before using ggplot confusion_df = as.data.frame(confusion_mtx) ggplot(confusion_df, aes(pred, obs, fill= Freq)) + geom_tile() + geom_text(aes(label=Freq)) + scale_fill_gradient(low=&quot;white&quot;, high=&quot;darkgreen&quot;) Also the false positive rate, false negative rate and true negative rate. Note, the denominator is always the number of observed samples with the same label, namely they are a constant for a specific dataset. FPR = FP / sum(cv_model$pred$obs == &#39;neg&#39;) FNR = FN / sum(cv_model$pred$obs == &#39;pos&#39;) TPR = TP / sum(cv_model$pred$obs == &#39;pos&#39;) print(paste(&quot;False positive rate:&quot;, FPR)) #&gt; [1] &quot;False positive rate: 0.118&quot; print(paste(&quot;False negative rate:&quot;, FNR)) #&gt; [1] &quot;False negative rate: 0.436567164179104&quot; print(paste(&quot;True positive rate:&quot;, TPR)) #&gt; [1] &quot;True positive rate: 0.563432835820896&quot; 4.4.2 ROC curve In the above assessment, we only used \\(P&gt;0.5\\) to denote predicted label as positive. We can imagine if we a lower cutoff lower, we will have more false positives and fewer false negatives. Indeed, in different scenarios, people may choose different level of cutoff for their tolerance of different types of errors. Let’s try cutoff \\(P&gt;0.4\\). Think what will you expect. # Original confusion matrix table(cv_model$pred[, c(&quot;obs&quot;, &quot;pred&quot;)]) #&gt; pred #&gt; obs neg pos #&gt; neg 441 59 #&gt; pos 117 151 # New confusion matrix with cutoff 0.4 cv_model$pred$pred_new = as.integer(cv_model$pred$pos &gt;= 0.4) table(cv_model$pred[, c(&quot;obs&quot;, &quot;pred_new&quot;)]) #&gt; pred_new #&gt; obs 0 1 #&gt; neg 408 92 #&gt; pos 89 179 Therefore, we may want to assess the model performance by varying the cutoffs and obtain a more systematic assessment. Actually, the Receiver operating characteristic (ROC) curve is what you need. It presents the TPR (sensitivity) vs the FPR (i.e., 1 - TNR or 1 - specificity) when varying the cutoffs. In order to achieve this, we can calculate FPR and TPR manually by varying the cutoff through a for loop. Read more about for loop and you may try write your own and here is an example from the cardelino package. For simplicity, let use an existing tool implemented in the pROC package to obtain the key information for making ROC curves: sensitivities (TPR) and specificities (1-FPR) for a list of thesholds: # Install the pROC library for plotting ROC curve if (!requireNamespace(&quot;pROC&quot;, quietly = TRUE)) { install.packages(&quot;pROC&quot;) } # library(pROC) roc_res &lt;- pROC::roc(cv_model$pred$obs == &#39;pos&#39;, cv_model$pred$pos) #&gt; Setting levels: control = FALSE, case = TRUE #&gt; Setting direction: controls &lt; cases print(paste(&quot;The AUC score of this ROC curve is&quot;, roc_res$auc)) #&gt; [1] &quot;The AUC score of this ROC curve is 0.829082089552239&quot; roc_table &lt;- as.data.frame( roc_res[c(&quot;sensitivities&quot;, &quot;specificities&quot;, &quot;thresholds&quot;)] ) head(roc_table) #&gt; sensitivities specificities thresholds #&gt; 1 1.0000000 0.000 -Inf #&gt; 2 1.0000000 0.002 0.001848584 #&gt; 3 1.0000000 0.004 0.003167924 #&gt; 4 1.0000000 0.006 0.004588208 #&gt; 5 1.0000000 0.008 0.005750560 #&gt; 6 0.9962687 0.008 0.007297416 With this roc_table at hand, we can find which threshold we should use for a certain desired FPR or TPR, for example to have FPR = 0.25: FPR = 1 - roc_table$specificities idx = which.min(abs(FPR - 0.25)) roc_table[idx, ] #&gt; sensitivities specificities thresholds #&gt; 444 0.7462687 0.75 0.3278307 print(paste(&quot;When FPR is closest to 0.25, the threshold is&quot;, roc_table[idx, 3])) #&gt; [1] &quot;When FPR is closest to 0.25, the threshold is 0.327830708502213&quot; print(paste(&quot;When FPR is closest to 0.25, the TPR is&quot;, roc_table[idx, 1])) #&gt; [1] &quot;When FPR is closest to 0.25, the TPR is 0.746268656716418&quot; By showing all thresholds, we can also directly use ggplot2 to make an ROC curve: library(ggplot2) # You can set the n.cuts to show the cutoffs on the curve g = ggplot(roc_table, aes(x = 1 - specificities, y = sensitivities)) + geom_line(color = &quot;blue&quot;, size = 1) + geom_abline(linetype = &quot;dashed&quot;, color = &quot;grey&quot;) + labs(title = &quot;ROC Curve&quot;, x = &quot;FPR (1 - Specificity)&quot;, y = &quot;TPR (Sensitivity)&quot;) + coord_equal() #&gt; Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. #&gt; ℹ Please use `linewidth` instead. #&gt; This warning is displayed once every 8 hours. #&gt; Call `lifecycle::last_lifecycle_warnings()` to see where this warning was #&gt; generated. # Display the plot with more annotations g + annotate(&quot;text&quot;, x=0.8, y=0.1, label=paste(&quot;AUC =&quot;, round(roc_res$auc, 4))) + geom_point(x = 1 - roc_table[idx, 2], y = roc_table[idx, 1]) + geom_text(x = 1 - roc_table[idx, 2], y = roc_table[idx, 1], label = round(roc_table[idx, 3], 3), color = &#39;black&#39;, hjust = -0.3) 4.4.3 Homework Now, try another model with removing triceps and plot the ROC curve and calculate the AUC score. Is it higher or lower than using the full features? "],["image-digital.html", "Chapter 5 Medical Image and Digital Health", " Chapter 5 Medical Image and Digital Health Contents to be added. "],["cancer.html", "Chapter 6 Cancer genomics 6.1 Case study 1: analysis of cBioportal mutation data", " Chapter 6 Cancer genomics 6.1 Case study 1: analysis of cBioportal mutation data Non-small cell lung cancer is a deadly disease, and we would like to identify mutations in genes that drive its formation and progression. George et al. (2015) has sequenced 120 small cell lung cancer samples and this data is available from cBioportal. The original data is in the MAF, and we have selected a subset of the columns, normalized the column names, and made the data available as an RDS file. In this exercise, we will explore the mutation data and examine genes that are frequently mutated in non-small cell lung cancer samples. We will implement a simple method for identifying candidate tumour suppressors based on loss-of-function mutations. 6.1.1 Exploratory analysis Q1. Load the data named mutations_sclc_ucologne_2015.rds into R. Examine it using head. In this data.frame, each row is a mutation detected in a particular gene from a particular sample in the study. For more information on the columns, see the MAF specification. ?readRDS ?head Q2. Identify the top 10 most frequently mutated genes. # calculate count frequencies using table ?table # freqs &lt;- ??? # sort the frequency vector in decreasing order ?sort # freqs &lt;- ??? # select the first 10 genes in the freqs # freqs.top &lt;- ??? # obtain the gene names # genes &lt;- names(freqs.top); # print(freqs.top) Q3. Tabulate the count frequencies of variant_class. What proportion of variants are Silent or inside an Intron? ?table Q4. A variant class value frame_shift_del appears to be misspelt due to case sensitivity. Correct this error. # x$variant_class[x$variant_class == &quot;frame_shift_del&quot;] &lt;- ???; Q5. To help us simplify the variant classes into loss-of-function or neutral, let us import the mutation_effects.tsv data. ?read.table # Hint: You need to set the `header` and `sep` arguments Q6. Define a new column in x that converts the variant classes to effects. ?match Q7. Create a subset of the data for the most frequently mutated genes, and tabulate the mutation count frequencies of variant classes for each gene. # x.sub &lt;- x[???, ] # order the factor levels based on `genes`, # which was previously sorted in decreasing order of frequency # x.sub$gene &lt;- factor(x.sub$gene, levels=genes); ?table Q8. Determine the number of mutations in each effect category, across all genes. Save the result in a variable named overall.counts. # overall.counts &lt;- table(???); # overall.counts Q9. Tabulate the mutation count frequencies for each gene and effect. Save the results in a variable named gene.counts. Look up the frequently mutated genes in this matrix. # gene.counts &lt;- table(???, ???); # gene.counts[???, ] 6.1.2 Statistical analysis Here, we implemented a simple statistical test for assessing whether a gene is significantly frequently targeted by loss-of-function mutations, based on the Fisher’s exact test. # Test whether a query gene has significantly more loss-of-function # mutations compared with other genes. # Requires global variables `gene.counts` and `overall.counts` lof_test &lt;- function(gene) { # Construct the following contingency table: # neutral lof # other genes a b # query gene c d cols &lt;- c(&quot;neutral&quot;, &quot;loss_of_function&quot;); gene.counts.sel &lt;- gene.counts[gene, cols]; ct &lt;- matrix( c( overall.counts[cols] - gene.counts.sel, gene.counts.sel ), byrow=TRUE, ncol=2 ); # Test whether the lof mutations are greater in frequency compared # to neutral mutations in the query gene, in comparison to all other genes fisher.test(ct, alternative=&quot;greater&quot;) } Q10. Subset the gene.counts table for the TP53 gene. Run lof_test on this gene. # gene.counts[???, ] # lof_test(???) Q11. Now, test to see if TTN and MUC16 are significantly frequently targeted by loss-of-function mutations. # similar to above Q12. Apply the loss-of-function test to all frequently mutated genes. ?lapply Q13. Extract odds ratio and p-values from the test results. # odds.ratios &lt;- vapply(hs, function(h) h$estimate, 0); # ps &lt;- vapply(hs, function(h) ???, 0); Q14. Since we tested many genes, we need to adjust for multiple hypothesis testing so that we can control the false discovery rate. So, adjust the p-values to obtain q-values. ?p.adjust Q15. Construct a results data.frame with gene names, odds ratio, p-values, and q-values that summarize the loss-of-function test results. # res &lt;- data.frame( # gene = ???, # odds_ratio = ???, # p = ???, # q = ??? # ); ?order #res &lt;- res[order(???), ]; Q16. Identify the significant genes from the results data.frame at a false discovery rate of 5% (i.e. q-value threshold of 0.05). Q17. Look up the significant genes in the gene.counts matrix. 6.1.3 Literature search Q18. Do the significant genes appear to be involved in cancer? What general roles do they play? Q19. How do these genes contribute specifically to the formation or progression of small cell lung cancer? Q20. What are some possible ways of identifying oncogenes that are activated by mutations or other genomic alterations? "],["epidemiology.html", "Chapter 7 Epidemiology", " Chapter 7 Epidemiology Contents to be added. "],["pop-genetics.html", "Chapter 8 Population Genetics 8.1 Case study 1: Heritability and human traits", " Chapter 8 Population Genetics 8.1 Case study 1: Heritability and human traits 8.1.1 Part 1 Scenario: You are a researcher working on a twin study on cardiovascular traits to assess the genetic and environmental contribution relevant to metabolism and cardiovascular disease risk. You have recruited a cohort of volunteer adult twins of the same ancestry. The volunteers have undergone a series of baseline clinical evaluations and performed genotyping on a panel of single nucleotide polymorphisms that may be associated with the traits. 8.1.1.1 Questions for Discussion Q1. Besides the clinical measurements, what data do you need to collect from the subjects? Answers: Sex Age Other confounding factors, e.g. BMI, blood pressure, smoking status, etc. Q2. How is genotype data represented for statistical genetic analysis? Answers: Allele: 0/1, 1/2, A/C, etc Genotype: 0 0, 0 1, 1 0, 1 1 Genotype probabilities: P(0/0)=0, P(0/1)=1, P(1/1)=0 Genotype dosage: 0/1/2, 0.678 (continuous from 0-1 or 0-2) Q3. How can you test for association between genotypes and phenotypes (binary and quantitative)? Answers: Allelic chi-square test Fisher’s exact test Linear/Logistic regression Linear mixed model 8.1.1.2 Hands-on exercise : Association test Now, you are given a dataset of age- and sex-matched twin cohort with two cardiovascular phenotypes and 5 quantitative trait loci (QTL). Data set and template notebook are available on Moodle (recommended) and also on this GitHub Repo. The information for columns: zygosity: 1 for monozygotic (MZ) and 2 for dizygotic (DZ) twin T1QTL_A[1-5] and T2QTL_A[1-5]: 5 quantitative loci (A1-A5) in additive coding for Twin 1 (T1) and Twin 2 (T2) respectively The same 5 QTL (D1-D5) in dominance coding for T1 and T2 Phenotype scores of T1 and T2 for the two quantitative cardiovascular traits Download the data dataTwin2024.dat to your working directory. Start the RStudio program and set the working directory. dataTwin &lt;- read.table(&quot;dataTwin2024.dat&quot;,h=T) Exploratory analysis Q1. How many MZ and DZ volunteers are there? Answers: 1000 MZ and 1000 DZ Q2. How are the genotypes represented? Answers: Dosage/Count of non-reference allele : 0, 1, and 2 A1-5: The QTLs are biallelic with two alleles A and a. The genotypes aa, Aa, and AA are coded additively as 0 (aa), 1 (Aa) and 2 (AA). D1-5: The genotypes aa, Aa, and AA are coded as 0 (aa), 1 (Aa) and 0 (AA). Q3. Are the QTL independent of each other? Answers: Yes. The pairwise correlations are low (&lt;0.2). Q4. Are there outliers in phenotypes? Answers: Yes. T2 individual 1303 has phenotype score (-4.21) being 4 SD below the mean. table(dataTwin$zygosity) # Q1: shows number of MZ and DZ twin pairs #&gt; #&gt; 1 2 #&gt; 1000 1000 table(dataTwin$T1QTL_A1) # Q2: shows the distribution of QTL_A1 #&gt; #&gt; 0 1 2 #&gt; 474 1021 505 table(dataTwin$T1QTL_D1) # Q2: shows the distribution of QTL_D1 #&gt; #&gt; 0 1 #&gt; 979 1021 table(dataTwin$T1QTL_A1, dataTwin$T1QTL_D1) # Q2: shows the distribution of QTL_A1 in relation to QTL_D1 #&gt; #&gt; 0 1 #&gt; 0 474 0 #&gt; 1 0 1021 #&gt; 2 505 0 cor(dataTwin[,2:11]) # Q3: shows the correlation between QTL_As #&gt; T1QTL_A1 T1QTL_A2 T1QTL_A3 T1QTL_A4 T1QTL_A5 #&gt; T1QTL_A1 1.00000000 -0.005470340 0.021705688 0.01940408 0.016278190 #&gt; T1QTL_A2 -0.00547034 1.000000000 0.017344822 -0.01421677 -0.008678746 #&gt; T1QTL_A3 0.02170569 0.017344822 1.000000000 0.01335711 -0.036751338 #&gt; T1QTL_A4 0.01940408 -0.014216767 0.013357109 1.00000000 0.074899996 #&gt; T1QTL_A5 0.01627819 -0.008678746 -0.036751338 0.07490000 1.000000000 #&gt; T2QTL_A1 0.53243815 0.004201635 -0.013909013 0.03252724 0.020081970 #&gt; T2QTL_A2 -0.04561174 0.464131160 -0.005044127 0.01324172 -0.003012277 #&gt; T2QTL_A3 0.03316574 -0.003552831 0.521253656 0.02045423 0.009081830 #&gt; T2QTL_A4 0.03271254 -0.033419904 0.020422583 0.48641289 0.019247531 #&gt; T2QTL_A5 -0.01285323 0.030413269 -0.045121964 0.08288145 0.457962222 #&gt; T2QTL_A1 T2QTL_A2 T2QTL_A3 T2QTL_A4 T2QTL_A5 #&gt; T1QTL_A1 0.532438150 -0.045611740 0.033165736 0.032712539 -0.01285323 #&gt; T1QTL_A2 0.004201635 0.464131160 -0.003552831 -0.033419904 0.03041327 #&gt; T1QTL_A3 -0.013909013 -0.005044127 0.521253656 0.020422583 -0.04512196 #&gt; T1QTL_A4 0.032527239 0.013241725 0.020454234 0.486412895 0.08288145 #&gt; T1QTL_A5 0.020081970 -0.003012277 0.009081830 0.019247531 0.45796222 #&gt; T2QTL_A1 1.000000000 0.006179257 -0.013129314 0.048294183 -0.01325839 #&gt; T2QTL_A2 0.006179257 1.000000000 -0.020860987 0.002164782 -0.01131418 #&gt; T2QTL_A3 -0.013129314 -0.020860987 1.000000000 -0.010583797 -0.02101270 #&gt; T2QTL_A4 0.048294183 0.002164782 -0.010583797 1.000000000 0.04350925 #&gt; T2QTL_A5 -0.013258394 -0.011314179 -0.021012699 0.043509251 1.00000000 cor(dataTwin[,2:11])&gt;0.2 #&gt; T1QTL_A1 T1QTL_A2 T1QTL_A3 T1QTL_A4 T1QTL_A5 T2QTL_A1 T2QTL_A2 #&gt; T1QTL_A1 TRUE FALSE FALSE FALSE FALSE TRUE FALSE #&gt; T1QTL_A2 FALSE TRUE FALSE FALSE FALSE FALSE TRUE #&gt; T1QTL_A3 FALSE FALSE TRUE FALSE FALSE FALSE FALSE #&gt; T1QTL_A4 FALSE FALSE FALSE TRUE FALSE FALSE FALSE #&gt; T1QTL_A5 FALSE FALSE FALSE FALSE TRUE FALSE FALSE #&gt; T2QTL_A1 TRUE FALSE FALSE FALSE FALSE TRUE FALSE #&gt; T2QTL_A2 FALSE TRUE FALSE FALSE FALSE FALSE TRUE #&gt; T2QTL_A3 FALSE FALSE TRUE FALSE FALSE FALSE FALSE #&gt; T2QTL_A4 FALSE FALSE FALSE TRUE FALSE FALSE FALSE #&gt; T2QTL_A5 FALSE FALSE FALSE FALSE TRUE FALSE FALSE #&gt; T2QTL_A3 T2QTL_A4 T2QTL_A5 #&gt; T1QTL_A1 FALSE FALSE FALSE #&gt; T1QTL_A2 FALSE FALSE FALSE #&gt; T1QTL_A3 TRUE FALSE FALSE #&gt; T1QTL_A4 FALSE TRUE FALSE #&gt; T1QTL_A5 FALSE FALSE TRUE #&gt; T2QTL_A1 FALSE FALSE FALSE #&gt; T2QTL_A2 FALSE FALSE FALSE #&gt; T2QTL_A3 TRUE FALSE FALSE #&gt; T2QTL_A4 FALSE TRUE FALSE #&gt; T2QTL_A5 FALSE FALSE TRUE apply(dataTwin[22:25],2,function(x){ any(x &lt; (mean(x) - 4*sd(x))) }) # Q4: any outlier &lt; 4 SD from the mean for the two quantitative phenotypes #&gt; pheno1_T1 pheno1_T2 pheno2_T1 pheno2_T2 #&gt; FALSE FALSE FALSE TRUE apply(dataTwin[22:25],2,function(x){ any(x &gt; (mean(x) + 4*sd(x))) }) # Q4: any outlier &gt; 4 SD from the mean for the two quantitative phenotypes #&gt; pheno1_T1 pheno1_T2 pheno2_T1 pheno2_T2 #&gt; FALSE FALSE FALSE FALSE # remove the phenotype score of the outlier (T2) for the phenotype 2 (pheno2_T2) outlier&lt;- which(dataTwin$pheno2_T2 &lt; (mean(dataTwin$pheno2_T2) - 4*sd(dataTwin$pheno2_T2) )) outlier #&gt; [1] 1303 dataTwin$pheno2_T2[outlier] #&gt; [1] -4.21 dataTwin$pheno2_T2[outlier] &lt;- NA Association test Test for association between QTL and pheno1 for T1 Regress pheno1_T1 on T1QTL_A1 to estimate the proportion of variance explained (R2). Model: pheno1_T1 = b0 + b1* T1QTL_A1 + e Calculate the conditional mean of phenotype (i.e. phenotypic mean conditional genotype) If the relationship between the QTL and the phenotype is perfectly linear, the regression line should pass through the conditional means (c_means), and the differences between the conditional means should be about equal. Q5. What are the values of b0, b1? Is QTL1 significant associated with the phenotype at alpha&lt;0.01 (multiple testing of 5 loci)? Answers: b0 = 4.1464 b1 = 0.9180 QTL1 is significantly associated with the phenotype with \\(P = 1.02\\times 10^{-13}\\) Q6. What is the proportion of phenotypic variance explained? Answers: Proportion of phenotypic variance explained = 0.027 linA1 &lt;- lm(pheno1_T1~T1QTL_A1, data=dataTwin) summary(linA1) #&gt; #&gt; Call: #&gt; lm(formula = pheno1_T1 ~ T1QTL_A1, data = dataTwin) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -15.1225 -2.4435 0.1105 2.7775 12.2555 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 4.1464 0.1511 27.438 &lt; 2e-16 *** #&gt; T1QTL_A1 0.9180 0.1226 7.491 1.02e-13 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 3.834 on 1998 degrees of freedom #&gt; Multiple R-squared: 0.02732, Adjusted R-squared: 0.02683 #&gt; F-statistic: 56.11 on 1 and 1998 DF, p-value: 1.022e-13 summary(linA1)$r.squared # proportion of explained variance by additive component #&gt; [1] 0.02731675 c_means &lt;- by(dataTwin$pheno1_T1,dataTwin$T1QTL_A1,mean) plot(dataTwin$pheno1_T1 ~ dataTwin$T1QTL_A1, col=&#39;grey&#39;, ylim=c(3,7)) lines(c(0,1,2), c_means, type=&quot;p&quot;, col=6, lwd=8) lines(sort(dataTwin$T1QTL_A1),sort(linA1$fitted.values), type=&#39;b&#39;, col=&quot;dark green&quot;, lwd=3) To test for the non-linearity, we can use the dominance coding of the QTL and add the dominance term to the regression model. Model: pheno1_T1 = b0 + b1* T1QTL_A1 + b2* T1QTL_D1 + e Repeat for T2. Q7. Why can’t we analyse T1 and T2 together? Answers: As T1 and T2 are biologically related as MZ or DZ twins, the genotypes of the QTLs are not independent. Treating the genotypes of T1 and T2 as independent observations will introduce bias. Q8. Is there a dominance effect? Answers: Yes. The model with dominance provides a better goodness of fit (lower p-value) linAD1 &lt;- lm(pheno1_T1 ~ T1QTL_A1 + T1QTL_D1, data=dataTwin) summary(linAD1) # results lm(phenoT1~T1QTL_A1+T1QTL_D1) #&gt; #&gt; Call: #&gt; lm(formula = pheno1_T1 ~ T1QTL_A1 + T1QTL_D1, data = dataTwin) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -14.7524 -2.5131 0.0586 2.8215 11.8894 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 3.7522 0.1753 21.405 &lt; 2e-16 *** #&gt; T1QTL_A1 0.9301 0.1220 7.622 3.83e-14 *** #&gt; T1QTL_D1 0.7483 0.1708 4.382 1.24e-05 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 3.816 on 1997 degrees of freedom #&gt; Multiple R-squared: 0.03658, Adjusted R-squared: 0.03562 #&gt; F-statistic: 37.91 on 2 and 1997 DF, p-value: &lt; 2.2e-16 plot(dataTwin$pheno1_T1 ~ dataTwin$T1QTL_A1, col=&#39;grey&#39;, ylim=c(3,7)) abline(linA1, lwd=3) lines(c(0,1,2), c_means, type=&#39;p&#39;, col=6, lwd=8) lines(sort(dataTwin$T1QTL_A1),sort(linA1$fitted.values), type=&#39;b&#39;, col=&quot;dark green&quot;, lwd=3) lines(sort(dataTwin$T1QTL_A1),sort(linAD1$fitted.values), type=&#39;b&#39;, col=&quot;blue&quot;, lwd=3) Q9. Repeat for the other 4 QTL and determine which QTL shows strongest association with the phenotype T1 Answers: QTL3 with \\(P = 7.77 \\times 10^{-25}\\) for model with dominance allQTL_A_T1 &lt;- 2:6 cpheno1_T1 &lt;- which(colnames(dataTwin)==&quot;pheno1_T1&quot;) ## Additive cbind(lapply(allQTL_A_T1,function(x){ fstat&lt;- summary(lm(pheno1_T1 ~ ., data=dataTwin[,c(x,cpheno1_T1)]))$fstatistic; pf(fstat[1],fstat[2],fstat[3],lower.tail = F) })) #&gt; [,1] #&gt; [1,] 1.021942e-13 #&gt; [2,] 8.329416e-15 #&gt; [3,] 1.007527e-13 #&gt; [4,] 4.523758e-18 #&gt; [5,] 8.207842e-13 ## Dominance cbind(lapply(allQTL_A_T1,function(x){ fstat&lt;- summary(lm(pheno1_T1 ~ ., data=dataTwin[,c(x,x+10,cpheno1_T1)]))$fstatistic; pf(fstat[1],fstat[2],fstat[3],lower.tail = F) })) #&gt; [,1] #&gt; [1,] 6.907834e-17 #&gt; [2,] 2.166957e-22 #&gt; [3,] 7.771588e-25 #&gt; [4,] 8.124437e-25 #&gt; [5,] 4.312127e-21 #Q9: QTL3 shows the strongest association with P=7.771588e-25 linAD3 &lt;- lm(pheno1_T1 ~ T1QTL_A3 + T1QTL_D3, data=dataTwin) summary(linAD3) # results lm(phenoT1~T1QTL_A1+T1QTL_D1) #&gt; #&gt; Call: #&gt; lm(formula = pheno1_T1 ~ T1QTL_A3 + T1QTL_D3, data = dataTwin) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -14.4988 -2.5701 0.1843 2.6991 11.5974 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 3.5437 0.1684 21.038 &lt; 2e-16 *** #&gt; T1QTL_A3 0.9076 0.1181 7.683 2.43e-14 *** #&gt; T1QTL_D3 1.2714 0.1692 7.515 8.55e-14 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 3.782 on 1997 degrees of freedom #&gt; Multiple R-squared: 0.05408, Adjusted R-squared: 0.05313 #&gt; F-statistic: 57.09 on 2 and 1997 DF, p-value: &lt; 2.2e-16 If the subjects with top 10% of the phenotype score are considered as cases, perform case-control association test for most significant SNP (from Q9) and interpret the result. Q10. What are the odds ratio, p-value, and 95% confidence interval (CI)? Answers: Odds ratio is 4.28 and the 95% CI is (2.54, 7.20) quant10 &lt;- quantile(c(dataTwin$pheno1_T1),seq(0,1,0.1)) dataTwin$CaseT1 &lt;- as.numeric(dataTwin$pheno1_T1&gt;quant10[10]) dataTwin$T1QTL_AD3 &lt;- (dataTwin$T1QTL_A3 + dataTwin$T1QTL_D3)/2 logisticAD3 &lt;- summary(glm(CaseT1 ~ T1QTL_AD3, data=dataTwin, family=&quot;binomial&quot;)) exp(logisticAD3$coefficients[2,1]) # odds ratio #&gt; [1] 4.277439 exp(logisticAD3$coefficients[2,1]-1.96*logisticAD3$coefficients[2,2]) # lower 95% confidence interval #&gt; [1] 2.539663 exp(logisticAD3$coefficients[2,1]+1.96*logisticAD3$coefficients[2,2]) # upper 95% confidence interval #&gt; [1] 7.204297 8.1.2 Part 2 Scenario: You are asked to estimate the additive genetic variance, dominance genetic variance and/or shared environmental variance using regression-based method and a classical twin design. \\[\\begin{align*} \\text{For ADE model : }~ &amp; \\sigma^{2}_{P} = \\sigma^{2}_{A} + \\sigma^{2}_{D} + \\sigma^{2}_{E}\\\\ \\text{For ACE model : }~ &amp; \\sigma^{2}_{P} = \\sigma^{2}_{A} + \\sigma^{2}_{C} + \\sigma^{2}_{E}, \\quad \\text{where} \\\\ \\sigma^{2}_{P} &amp; \\text{ is the phenotypic variance}, \\\\ \\sigma^{2}_{A} &amp; \\text{ is additive genetic variance}, \\\\ \\sigma^{2}_{D} &amp; \\text{ is dominance genetic variance}, \\\\ \\sigma^{2}_{C} &amp; \\text{ is shared environmental variance, and} \\\\ \\sigma^{2}_{E} &amp; \\text{ is unshared environmental variance.} \\end{align*}\\] For ADE model, given the standardization of the phenotype, \\[\\begin{align*} cov(MZ) = cor(MZ) &amp; = rMZ = \\sigma^{2}_{A} + \\sigma^{2}_{D} \\\\ cov(DZ) = cor(DZ) &amp; = rDZ = 0.5 * \\sigma^{2}_{A} + 0.25 * \\sigma^{2}_{D} \\quad \\text{ , where} \\\\ \\end{align*}\\] the coefficients 1/2 and 1/4 are based on quantitative genetic theory (Mather &amp; Jinks, 1971). By solving the unknowns, the variance explained by different components for the ADE model: \\[\\begin{align*} \\sigma^{2}_{A} &amp; = 4*rDZ - rMZ \\\\ \\sigma^{2}_{D} &amp; = 2*rMZ - 4*rDZ \\\\ \\sigma^{2}_{E} &amp; = 1 - \\sigma^{2}_{A} - \\sigma^{2}_{D} \\\\ \\end{align*}\\] For ACE model, \\[\\begin{align*} cov(MZ) = cor(MZ) &amp; = rMZ= \\sigma^{2}_{A} + \\sigma^{2}_{C} \\\\ cov(DZ) = cor(DZ) &amp; = rDZ = 0.5 * \\sigma^{2}_{A} + \\sigma^{2}_{C} \\quad \\text{ , where} \\\\ \\end{align*}\\] By solving the unknowns, the variance explained by different components for the ACE model: \\[\\begin{align*} \\sigma^{2}_{A} &amp; = 2*(rMZ - rDZ) \\\\ \\sigma^{2}_{C} &amp; = 2*rDZ - rMZ \\\\ \\sigma^{2}_{E} &amp; = 1 - \\sigma^{2}_{A} - \\sigma^{2}_{C} = 1 - rMZ \\end{align*}\\] 8.1.2.1 Hands-on exercise : variance explained using regression-based method Q1. What is the variance of the phenotype? Q2. Compute the explained variance attributable to the additive genetic component of the QTL with strongest association in Part 1. Q3. Compute the explained variance attributable to the dominance genetic component of the QTL with strongest association in Part 1. R2 from the regression represents the proportion of phenotypic variance explained; thus the raw explained variance component is R2 times the variance of the phenotype (var_pheno). Answers The proportion of explained variance are 0.0273 (additive) and 0.0541 (total: additive + dominance). As the predictors are uncorrelated, the proportion of explained variance by dominance = 0.0541 - 0.0273 = 0.0267 Given the phenotypic variance of 15.102, then Total genetic: 0.0541*15.102 = 0.8168 Additive genetic: 0.0273*15.102 = 0.4128 Dominance genetic: 0.0267*15.102 = 0.4040 var_pheno &lt;- var(dataTwin$pheno1_T1) # the variance of the phenotype var_pheno #&gt; [1] 15.10257 linAD3 &lt;- lm(pheno1_T1 ~ T1QTL_A3 + T1QTL_D3, data=dataTwin) linA3 &lt;- lm(pheno1_T1 ~ T1QTL_A3, data=dataTwin) summary(linAD3)$r.squared # proportion of explained variance by total genetic component #&gt; [1] 0.05408025 summary(linA3)$r.squared # proportion of explained variance by additive component #&gt; [1] 0.02733034 summary(linAD3)$r.squared*var_pheno # (raw) variance component of total genetic component #&gt; [1] 0.8167509 summary(linA3)$r.squared*var_pheno # (raw) variance component of additive genetic component #&gt; [1] 0.4127585 (summary(linAD3)$r.squared-summary(linA3)$r.squared)*var_pheno # (raw) variance component of dominance genetic component #&gt; [1] 0.4039924 Q4. Estimate the variance explained by all the QTL using linear regression. Answers Proportion of variance explained by all 5 QTLs with dominance = 0.23 and the total variance explained = 3.52. # compute for all 5 QTL linAD5=(lm(pheno1_T1 ~ T1QTL_A1 + T1QTL_A2 + T1QTL_A3 + T1QTL_A4 + T1QTL_A5 + T1QTL_D1 + T1QTL_D2 + T1QTL_D3 + T1QTL_D4 + T1QTL_D5, data=dataTwin)) summary(linAD5)$r.squared # proportion of explained variance by total genetic component #&gt; [1] 0.2330307 summary(linAD5)$r.squared*var_pheno # (raw) variance component of total genetic component #&gt; [1] 3.519363 8.1.2.2 Hands-on exercise : variance explained using a classical twin design. Based on our regression results, we have estimates of the total genetic variance as well as the A and D components for phenotype 1 explained by the QTLs. In practice, it is impossible to know all the variants associated with any polygenic trait. Alternatively, we can use ADE or ACE models to estimate the A (additive genetic) and D (dominance)/C (shared environmental) variance with the classical twin design for phenotype 1 without genotypes. Q5. Which model should be used? ACE or ADE? Let’s first compute rMZ and rDZ. Answers rMZ = 0.5434 rDZ = 0.1904 As \\(rMZ &gt; 2*rDZ\\), ADE model should be used. dataMZ = dataTwin[dataTwin$zygosity==1, c(&#39;pheno1_T1&#39;, &#39;pheno1_T2&#39;)] # MZ data frame dataDZ = dataTwin[dataTwin$zygosity==2, c(&#39;pheno1_T1&#39;, &#39;pheno1_T2&#39;)] # DZ data frame rMZ=cor(dataMZ)[2,1] # element 2,1 in the MZ correlation matrix rDZ=cor(dataDZ)[2,1] # element 2,1 in the DZ correlation matrix rMZ rDZ Q6. Estimate the proportion of additive and dominance genetic variances using the ADE model. Answers \\(\\sigma^{2}_{A} = 0.2181\\) \\(\\sigma^{2}_{D} = 0.3253\\) \\(\\sigma^{2}_{E} = 0.4566\\) sA2 = 4*rDZ - rMZ sD2 = 2*rMZ - 4*rDZ sE2 = 1 - sA2 - sD2 print(c(sA2, sD2, sE2)) Similarly, for phenotype 2, we can estimate the proportion of additive and/or dominance genetic variances as well as shared environmental variance using the classical twin design. Q7. Which model (ACE or ADE) should be considered for phenotype 2? Answers ACE as \\(rMZ &lt; 2*rDZ\\) Q8. Estimate the proportion of A, C/D and E variance components for phenotype 2. Answers \\(\\sigma^{2}_{A} = 0.3526\\) \\(\\sigma^{2}_{C} = 0.1610\\) \\(\\sigma^{2}_{E} = 0.4864\\) dataMZ = dataTwin[dataTwin$zygosity==1, c(&#39;pheno2_T1&#39;, &#39;pheno2_T2&#39;)] # MZ data frame dataDZ = dataTwin[dataTwin$zygosity==2, c(&#39;pheno2_T1&#39;, &#39;pheno2_T2&#39;)] # DZ data frame rMZ=cor(dataMZ, use=&quot;complete.obs&quot;)[2,1] # element 2,1 in the MZ correlation matrix rDZ=cor(dataDZ, use=&quot;complete.obs&quot;)[2,1] # element 2,1 in the DZ correlation matrix rMZ rDZ sA2 = 2*(rMZ - rDZ) sC2 = 2*rDZ - rMZ sE2 = 1 - rMZ print(c(sA2, sC2, sE2)) 8.1.3 References Evans DM, Gillespie NA, Martin NG. Biometrical genetics. Biol Psychol. 2002 Oct;61(1-2):33-51. doi: 10.1016/s0301-0511(02)00051-0. PMID: 12385668. [Review article] Falconer, D.S. and Mackay, T.F.C. (1996) Introduction to Quantitative Genetics. 4th Edition, Addison Wesley Longman, Harlow. [Most classical; a lot of online version] Neale, B., Ferreira, M., Medland, S., &amp; Posthuma, D. (Eds.). (2007). Statistical Genetics: Gene Mapping Through Linkage and Association (1st ed.). Taylor &amp; Francis. https://doi.org/10.1201/9780203967201 [chapter on biometrical genetics; can be borrowed from HKU lib] https://ibg.colorado.edu/cdrom2020/dolan/biometricalGenetics/biom_gen_2020.pdf [Course material of the Boulder IBG workshop co-organized by top statistical geneticists] "],["cancer-case2.html", "Chapter 9 Cancer Epidemiology", " Chapter 9 Cancer Epidemiology by Prof. Jason Wong Date: 31-10-2024 The RMarkdown notebook to run your own code can be downloaded here [right click -&gt; save link as] 9.0.1 Scenario You are a grants officer working for the Hong Kong Health Bureau. The HK government has recently announced new special funding in cancer research to be administered by the Bureau. You are tasked with coming up with a proposal for distribution of funding to specific cancer types that are in most need of research. Discussion points: What is important to consider when selecting a cancer type in need of research? What type of data is required? 9.0.2 Hong Kong population You are aware that, generally, cancer is a disease that affects the elderly more than the young. You decide first to take a closer look at the structure of the population of Hong Kong. Historical population of Hong Kong can be obtained from the Census and Statistics Department. An abridged version of the full historical population of Hong Kong is provided here, containing the population of Hong Kong from 1965, 1975, 1985, 1995, 2005, 2015 and 2022 categorised by sex and age (0-19, 20-44, 45-64, 65+). Discussion points: What is the trend in Hong Kong’s population over the past ~60 years? What is the best way to visualise this data? 9.0.2.1 Download population data HKPop&lt;- read.table(&quot;https://github.com/StatBiomed/BMDS-book/raw/main/notebooks/module5-epidemi/HK_population_1965-2022.txt&quot;, sep = &quot;\\t&quot;, header = TRUE, stringsAsFactor=FALSE, check.names = FALSE) HKPop #&gt; Sex Age 1965 1975 1985 1995 2005 2015 2022 #&gt; 1 male 0-19 901.9 989.7 909.0 835.6 719.0 613.8 530.6 #&gt; 2 male 20-44 607.4 795.6 1210.2 1363.7 1263.7 1146.9 1039.2 #&gt; 3 male 45-64 266.5 414.3 525.9 615.7 896.6 1084.9 1046.2 #&gt; 4 male 65+ 42.2 84.6 170.5 269.3 384.7 520.0 713.6 #&gt; 5 female 0-19 865.6 938.8 840.3 780.7 684.1 575.0 502.1 #&gt; 6 female 20-44 539.3 685.6 1093.7 1423.6 1530.4 1531.8 1338.7 #&gt; 7 female 45-64 286.3 400.7 470.7 535.0 884.7 1224.3 1314.7 #&gt; 8 female 65+ 88.7 152.3 235.9 332.5 450.0 594.6 806.5 9.0.2.2 Format data for plotting Here we convert the original dataframe into simplified format for ggplot2. male&lt;-data.frame(year = as.numeric(colnames(HKPop[1,3:9])), `0-19` = as.numeric(HKPop[1,3:9]), `20-44` = as.numeric(HKPop[2,3:9]), `45-64` = as.numeric(HKPop[3,3:9]), `65+` = as.numeric(HKPop[4,3:9]), check.names = FALSE) female&lt;-data.frame(year = as.numeric(colnames(HKPop[1,3:9])), `0-19` = as.numeric(HKPop[5,3:9]), `20-44` = as.numeric(HKPop[6,3:9]), `45-64` = as.numeric(HKPop[7,3:9]), `65+` = as.numeric(HKPop[8,3:9]), check.names = FALSE) if (!require(&quot;tidyverse&quot;)) install.packages(&quot;tidyverse&quot;) male&lt;-as_tibble(male) %&gt;% select(year,`0-19`,`20-44`,`45-64`,`65+`) %&gt;% gather (key=&quot;age&quot;,value=&quot;population (&#39;000s)&quot;,-year) female&lt;-as_tibble(female) %&gt;% select(year,`0-19`,`20-44`,`45-64`,`65+`) %&gt;% gather (key=&quot;age&quot;,value=&quot;population (&#39;000s)&quot;,-year) male #&gt; # A tibble: 28 × 3 #&gt; year age `population (&#39;000s)` #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 1965 0-19 902. #&gt; 2 1975 0-19 990. #&gt; 3 1985 0-19 909 #&gt; 4 1995 0-19 836. #&gt; 5 2005 0-19 719 #&gt; 6 2015 0-19 614. #&gt; 7 2022 0-19 531. #&gt; 8 1965 20-44 607. #&gt; 9 1975 20-44 796. #&gt; 10 1985 20-44 1210. #&gt; # ℹ 18 more rows female #&gt; # A tibble: 28 × 3 #&gt; year age `population (&#39;000s)` #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 1965 0-19 866. #&gt; 2 1975 0-19 939. #&gt; 3 1985 0-19 840. #&gt; 4 1995 0-19 781. #&gt; 5 2005 0-19 684. #&gt; 6 2015 0-19 575 #&gt; 7 2022 0-19 502. #&gt; 8 1965 20-44 539. #&gt; 9 1975 20-44 686. #&gt; 10 1985 20-44 1094. #&gt; # ℹ 18 more rows 9.0.2.3 Plotting population data Uses ggplot2 and gridExtra to make line plot of male and female population data side-by-side. if (!require(&quot;ggplot2&quot;)) install.packages(&quot;gglot2&quot;) if (!require(&quot;gridExtra&quot;)) install.packages(&quot;gridExtra&quot;) library(ggplot2) library(gridExtra) #Import the necessary packages and libraries pmale&lt;-ggplot(male,aes(x=year,y=`population (&#39;000s)`,group=age))+ geom_line(aes(color=age))+ geom_point(aes(color=age))+ scale_color_brewer(palette=&quot;Spectral&quot;)+ theme_classic()+ ylim(0,1600)+ theme(legend.position=&quot;none&quot;)+ scale_x_continuous(breaks = seq(1965, 2022, by = 10))+ ggtitle(&quot;male&quot;) pfemale&lt;-ggplot(female,aes(x=year,y=`population (&#39;000s)`,group=age))+ geom_line(aes(color=age))+ geom_point(aes(color=age))+ scale_color_brewer(palette=&quot;Spectral&quot;)+ theme_classic()+ ylim(0,1600)+ theme(legend.position=&quot;right&quot;,axis.title.y = element_blank())+ scale_x_continuous(breaks = seq(1965, 2022, by = 10))+ ggtitle(&quot;female&quot;) grid.arrange(pmale,pfemale,ncol=2,widths=c(3,3.75)) 9.0.3 Cancer registry data It is clear that Hong Kong has an aging population, thus cancer incidence would also likely increase. To examine cancer incidence and mortality in Hong Kong you obtain data from the Hong Kong Cancer Registry, which is maintained by the Hospital Authority. Cancer incidence data was summarised for the last three decades (1990-1999, 2000-2009 and 2010-2019). Discussion points: Has incidence been increasing for most cancers? How about mortality? How has cancer risk and mortality rate changed in the past 3 decades? Has the incidence-to-mortality ratio been decreasing generally? Is it statistically significant? Which cancer type has the highest incidence in children (0-19) when compared with the elderly (65+). Is this statistically significantly different to incidence of children versus elderly cancers in general? 9.0.3.1 Download cancer registry data HKCancer&lt;- read.table(&quot;https://github.com/StatBiomed/BMDS-book/raw/main/notebooks/module5-epidemi/HK_cancer_incidence_mortality_1990-2020.txt&quot;, sep = &quot;\\t&quot;, header = TRUE, stringsAsFactor=FALSE) HKCancer #&gt; Type Sex Age Year Biliary Bladder Brain Breast Cervix #&gt; 1 incidence male 0-19 1990-1999 0 6 212 0 0 #&gt; 2 incidence male 20-44 1990-1999 36 165 368 6 0 #&gt; 3 incidence male 45-64 1990-1999 322 1266 393 28 0 #&gt; 4 incidence male 65+ 1990-1999 830 2920 318 35 0 #&gt; 5 incidence male 0-19 2000-2009 0 2 206 0 0 #&gt; 6 incidence male 20-44 2000-2009 31 103 267 9 0 #&gt; 7 incidence male 45-64 2000-2009 297 900 372 52 0 #&gt; 8 incidence male 65+ 2000-2009 1134 3103 299 96 0 #&gt; 9 incidence male 0-19 2010-2019 0 2 155 0 0 #&gt; 10 incidence male 20-44 2010-2019 28 18 276 14 0 #&gt; 11 incidence male 45-64 2010-2019 530 648 511 72 0 #&gt; 12 incidence male 65+ 2010-2019 1568 2399 380 107 0 #&gt; 13 incidence female 0-19 1990-1999 0 2 149 6 0 #&gt; 14 incidence female 20-44 1990-1999 52 62 283 4228 1263 #&gt; 15 incidence female 45-64 1990-1999 246 246 238 5311 1839 #&gt; 16 incidence female 65+ 1990-1999 909 1199 284 4122 1574 #&gt; 17 incidence female 0-19 2000-2009 0 0 132 4 4 #&gt; 18 incidence female 20-44 2000-2009 34 39 223 5887 1204 #&gt; 19 incidence female 45-64 2000-2009 287 179 265 11833 1676 #&gt; 20 incidence female 65+ 2000-2009 1190 1090 237 5774 1329 #&gt; 21 incidence female 0-19 2010-2019 0 2 119 1 0 #&gt; 22 incidence female 20-44 2010-2019 36 17 223 6658 1294 #&gt; 23 incidence female 45-64 2010-2019 452 188 394 22096 2288 #&gt; 24 incidence female 65+ 2010-2019 1500 872 283 10337 1269 #&gt; 25 mortality male 0-19 1990-1999 0 0 76 0 0 #&gt; Colorectum Eye Hodgkin.lymphoma Kaposi Kidney Larynx Leukaemia Liver Lung #&gt; 1 15 29 35 0 24 1 356 58 6 #&gt; 2 1087 8 64 0 141 86 509 1741 1036 #&gt; 3 4644 9 47 0 530 824 453 5702 8420 #&gt; 4 8058 21 41 0 721 1265 623 4806 15152 #&gt; 5 5 38 32 0 20 0 329 33 3 #&gt; 6 957 8 119 9 221 37 437 1116 772 #&gt; 7 6418 18 77 4 1071 658 642 6015 7909 #&gt; 8 13352 13 77 8 1275 1125 864 5808 18870 #&gt; 9 6 26 53 0 31 0 338 24 2 #&gt; 10 1005 10 163 25 293 29 401 659 541 #&gt; 11 10034 28 87 18 1978 684 1033 6500 9523 #&gt; 12 18003 15 114 12 1977 1043 1381 6716 20588 #&gt; 13 2 33 10 0 25 0 296 37 7 #&gt; 14 932 11 61 0 98 13 393 353 597 #&gt; 15 3166 9 19 0 256 59 312 1093 2629 #&gt; 16 7728 16 33 0 503 130 534 2244 8428 #&gt; 17 5 14 30 0 15 0 219 13 5 #&gt; 18 938 12 122 2 120 5 405 212 625 #&gt; 19 4430 10 33 0 482 37 469 1041 3397 #&gt; 20 10810 8 35 0 830 85 723 2637 9517 #&gt; 21 5 14 32 0 19 0 237 21 0 #&gt; 22 1057 9 174 0 135 7 443 168 647 #&gt; 23 7200 16 58 4 887 47 786 1168 6312 #&gt; 24 13119 20 58 5 1203 84 913 3007 10872 #&gt; 25 10 3 0 0 2 0 120 36 1 #&gt; Melanoma Mesothelioma Multiple.myeloma Nasal Nasopharynx #&gt; 1 3 0 1 4 33 #&gt; 2 50 0 43 53 3049 #&gt; 3 82 0 210 109 3724 #&gt; 4 107 0 421 120 1212 #&gt; 5 1 0 1 7 16 #&gt; 6 41 5 38 38 2114 #&gt; 7 98 34 326 161 3750 #&gt; 8 137 77 674 137 1177 #&gt; 9 0 0 0 3 21 #&gt; 10 47 4 26 41 1339 #&gt; 11 186 51 525 174 3653 #&gt; 12 195 154 910 154 1207 #&gt; 13 2 0 1 1 13 #&gt; 14 44 0 19 31 1380 #&gt; 15 58 0 150 57 1157 #&gt; 16 102 0 417 93 536 #&gt; 17 2 0 0 5 6 #&gt; 18 41 8 21 36 942 #&gt; 19 102 12 185 74 1187 #&gt; 20 118 14 559 92 465 #&gt; 21 3 0 0 6 6 #&gt; 22 70 10 24 37 583 #&gt; 23 128 34 399 105 1145 #&gt; 24 179 24 636 102 389 #&gt; 25 2 0 0 0 6 #&gt; Non.Hodgkin.lymphoma Non.melanoma.skin Oesophagus Oral Ovary Pancreas Penis #&gt; 1 137 8 0 18 0 4 2 #&gt; 2 596 211 158 324 0 106 19 #&gt; 3 997 613 1873 1165 0 514 78 #&gt; 4 1210 968 2203 1055 0 887 184 #&gt; 5 111 4 0 6 0 1 0 #&gt; 6 454 220 82 312 0 71 18 #&gt; 7 1216 917 1528 1354 0 780 71 #&gt; 8 1756 1835 2126 1416 0 1440 199 #&gt; 9 106 1 0 9 0 1 0 #&gt; 10 487 291 54 304 0 109 21 #&gt; 11 1910 1782 1321 1941 0 1402 154 #&gt; 12 2633 2975 1982 1813 0 2302 279 #&gt; 13 75 2 0 21 77 2 0 #&gt; 14 444 156 48 241 854 60 0 #&gt; 15 594 374 254 364 970 290 0 #&gt; 16 1120 1249 761 485 776 875 0 #&gt; 17 43 4 1 20 108 0 0 #&gt; 18 461 182 13 254 1299 62 0 #&gt; 19 888 580 190 454 1846 461 0 #&gt; 20 1409 2476 725 768 891 1329 0 #&gt; 21 57 4 0 9 84 3 0 #&gt; 22 475 303 16 259 1466 92 0 #&gt; 23 1561 1168 174 920 3166 923 0 #&gt; 24 1883 3268 624 1091 1128 2101 0 #&gt; 25 25 0 0 1 0 3 0 #&gt; Placenta Prostate Sarcoma Small.intestine Stomach Testis Thymus Thyroid #&gt; 1 0 0 137 0 5 61 12 22 #&gt; 2 0 10 320 31 429 275 71 250 #&gt; 3 0 481 281 102 1990 80 70 288 #&gt; 4 0 3073 261 131 3634 129 59 194 #&gt; 5 0 0 118 0 6 57 16 21 #&gt; 6 0 6 307 34 331 395 96 333 #&gt; 7 0 1588 367 113 1954 65 137 447 #&gt; 8 0 8676 316 163 4256 44 94 278 #&gt; 9 0 1 110 1 2 53 23 17 #&gt; 10 0 18 299 46 192 587 68 496 #&gt; 11 0 4128 527 293 2158 100 137 834 #&gt; 12 0 14703 507 336 4720 15 85 424 #&gt; 13 1 0 128 1 1 0 3 80 #&gt; 14 13 0 283 22 492 0 38 1294 #&gt; 15 0 0 232 53 826 0 38 788 #&gt; 16 0 0 232 116 2319 0 38 479 #&gt; 17 0 0 105 0 2 0 1 81 #&gt; 18 8 0 322 24 379 0 64 1659 #&gt; 19 3 0 300 80 971 0 116 1570 #&gt; 20 0 0 247 153 2512 0 65 595 #&gt; 21 0 0 86 1 0 0 5 103 #&gt; 22 12 0 330 38 318 0 30 2291 #&gt; 23 3 0 509 221 1653 0 81 3200 #&gt; 24 0 0 351 251 2809 0 67 940 #&gt; 25 0 0 41 1 0 2 7 1 #&gt; Uterus Vulva #&gt; 1 0 0 #&gt; 2 0 0 #&gt; 3 0 0 #&gt; 4 0 0 #&gt; 5 0 0 #&gt; 6 0 0 #&gt; 7 0 0 #&gt; 8 0 0 #&gt; 9 0 0 #&gt; 10 0 0 #&gt; 11 0 0 #&gt; 12 0 0 #&gt; 13 2 1 #&gt; 14 478 49 #&gt; 15 1362 96 #&gt; 16 729 235 #&gt; 17 2 2 #&gt; 18 821 44 #&gt; 19 3071 147 #&gt; 20 1130 336 #&gt; 21 3 1 #&gt; 22 1165 57 #&gt; 23 6582 235 #&gt; 24 1864 486 #&gt; 25 0 0 #&gt; [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 23 rows ] 9.0.3.2 a. Visualise changes in incidence and mortality # first plot incidence for each cancer type #Import the necessary packages and libraries if (!require(&quot;ggplot2&quot;)) install.packages(&quot;gglot2&quot;) if (!require(&quot;gridExtra&quot;)) install.packages(&quot;gridExtra&quot;) if (!require(&quot;ggpubr&quot;)) install.packages(&quot;ggpubr&quot;) library(ggplot2) library(gridExtra) library(ggpubr) HKCancer_inc &lt;- HKCancer[HKCancer$Type==&#39;incidence&#39;,] p&lt;-list() for (i in 1:(ncol(HKCancer_inc)-4)){ p[[i]]&lt;-ggplot(HKCancer_inc,aes_string(fill=names(HKCancer_inc)[3],x=names(HKCancer_inc)[4], y=names(HKCancer_inc)[i+4],group=names(HKCancer_inc)[3]))+ geom_bar(position=&quot;dodge&quot;,stat=&quot;identity&quot;)+ facet_wrap(~Sex) + theme_classic()+ scale_fill_brewer(palette=&quot;Spectral&quot;)+ theme(legend.position=&quot;none&quot;)+ theme(text = element_text(size = 10))+ ggtitle(names(HKCancer_inc)[i+4])+ ylab(&quot;Incidence&quot;) } do.call(&#39;grid.arrange&#39;,c(p,ncol=3,nrow=12)) # now plot mortality for each cancer type #Import the necessary packages and libraries if (!require(&quot;ggplot2&quot;)) install.packages(&quot;gglot2&quot;) if (!require(&quot;gridExtra&quot;)) install.packages(&quot;gridExtra&quot;) if (!require(&quot;ggpubr&quot;)) install.packages(&quot;ggpubr&quot;) if (!require(&quot;dplyr&quot;)) install.packages(&quot;dplyr&quot;) library(ggplot2) library(gridExtra) library(ggpubr) library(dplyr) HKCancer_mort &lt;- HKCancer[HKCancer$Type==&#39;mortality&#39;,] p&lt;-list() #(ncol(HKCancer_inc)-4) for (i in 1:(ncol(HKCancer_mort)-4)){ p[[i]]&lt;-ggplot(HKCancer_mort,aes_string(fill=names(HKCancer_mort)[3],x=names(HKCancer_mort)[4], y=names(HKCancer_mort)[i+4],group=names(HKCancer_mort)[3]))+ geom_bar(position=&quot;dodge&quot;,stat=&quot;identity&quot;)+ facet_wrap(~Sex) + theme_classic()+ scale_fill_brewer(palette=&quot;Spectral&quot;)+ theme(legend.position=&quot;none&quot;)+ theme(text = element_text(size = 10))+ ggtitle(names(HKCancer_mort)[i+4])+ ylab(&quot;Mortality&quot;) } do.call(&#39;grid.arrange&#39;,c(p,ncol=3,nrow=12)) 9.0.3.3 b. Calculate cancer risk and mortality rate To calculate disease risk we need to calculated the number of new cases over the number of persons at risk over a specific time period. We have the incidence for each decade and can estimate the number of persons at risk based on the population in 1995, 2005 and 2015. # cancer risk calculation HKCancer_inc &lt;- HKCancer[HKCancer$Type==&#39;incidence&#39;,] HKCancer_inc_risk &lt;- HKCancer_inc[,1:5] risk &lt;- function(x,age,sex,year){ if (is.integer(x)){ pop_n &lt;- HKPop %&gt;% filter(Sex==sex &amp; Age==age) if (year == &quot;1990-1999&quot;){ return (as.double(x)/as.double(pop_n$`1995`)) } else if (year == &quot;2000-2009&quot;) {return (as.double(x)/as.double(pop_n$`2005`))} else { return (as.double(x)/as.double(pop_n$`2015`)) } } return (x) } for (i in 5:ncol(HKCancer_inc)){ HKCancer_inc_risk[names(HKCancer_inc)[i]] &lt;- mapply(risk,HKCancer_inc[,i],HKCancer_inc[,3],HKCancer_inc[,2],HKCancer_inc[,4]) } #visualise cancer risk p&lt;-list() for (i in 1:(ncol(HKCancer_inc_risk)-4)){ p[[i]]&lt;-ggplot(HKCancer_inc_risk,aes_string(fill=names(HKCancer_inc_risk)[3],x=names(HKCancer_inc_risk)[4], y=names(HKCancer_inc_risk)[i+4],group=names(HKCancer_inc_risk)[3]))+ geom_bar(position=&quot;dodge&quot;,stat=&quot;identity&quot;)+ facet_wrap(~Sex) + theme_classic()+ scale_fill_brewer(palette=&quot;Spectral&quot;)+ theme(legend.position=&quot;none&quot;)+ theme(text = element_text(size = 10))+ ggtitle(names(HKCancer_inc)[i+4])+ ylab(&quot;incidence per 1000&quot;) } do.call(&#39;grid.arrange&#39;,c(p,ncol=3,nrow=12)) # mortality rate calculation HKCancer_mort &lt;- HKCancer[HKCancer$Type==&#39;mortality&#39;,] HKCancer_mort_risk &lt;- HKCancer_mort[,1:5] risk &lt;- function(x,age,sex,year){ if (is.integer(x)){ pop_n &lt;- HKPop %&gt;% filter(Sex==sex &amp; Age==age) if (year == &quot;1990-1999&quot;){ return (as.double(x)/as.double(pop_n$`1995`)) } else if (year == &quot;2000-2009&quot;) {return (as.double(x)/as.double(pop_n$`2005`))} else { return (as.double(x)/as.double(pop_n$`2015`)) } } return (x) } for (i in 5:ncol(HKCancer_mort)){ HKCancer_mort_risk[names(HKCancer_mort)[i]] &lt;- mapply(risk,HKCancer_mort[,i],HKCancer_mort[,3],HKCancer_mort[,2],HKCancer_mort[,4]) } #visualise mortality rate p&lt;-list() for (i in 1:(ncol(HKCancer_mort_risk)-4)){ p[[i]]&lt;-ggplot(HKCancer_mort_risk,aes_string(fill=names(HKCancer_mort_risk)[3],x=names(HKCancer_mort_risk)[4], y=names(HKCancer_mort_risk)[i+4],group=names(HKCancer_mort_risk)[3]))+ geom_bar(position=&quot;dodge&quot;,stat=&quot;identity&quot;)+ facet_wrap(~Sex) + theme_classic()+ scale_fill_brewer(palette=&quot;Spectral&quot;)+ theme(legend.position=&quot;none&quot;)+ theme(text = element_text(size = 10))+ ggtitle(names(HKCancer_inc)[i+4])+ ylab(&quot;mortality per 1000&quot;) } do.call(&#39;grid.arrange&#39;,c(p,ncol=3,nrow=12)) 9.0.3.4 c. Mortality-incidence ratio Cancer research can be focused on improving cancer outcomes in a number of ways. For example cancer prevention research that seeks to reduce cancer incidence which would also ultimately reduce cancer mortality. Another area is cancer therapy which would not affect incidence but seeks to reduce mortality, or at least prolong survival. We don’t go into survival analysis in this tutorial, but a way to get an idea whether treatment is improving by looking at the mortality-incidence ratio. HKCancer_inc &lt;- HKCancer[HKCancer$Type==&#39;incidence&#39;,] HKCancer_mort &lt;- HKCancer[HKCancer$Type==&#39;mortality&#39;,] HKCancer_mort_inc &lt;- HKCancer_mort[,1:5] risk &lt;- function(mort,inc){ if (inc == 0){ return (0) } return (as.double(mort)/as.double(inc)) } for (i in 5:ncol(HKCancer_mort)){ HKCancer_mort_inc[names(HKCancer_mort)[i]] &lt;- mapply(risk,HKCancer_mort[,i],HKCancer_inc[,i]) } #visualise mortality incidence ratio p&lt;-list() for (i in 1:(ncol(HKCancer_mort_inc)-4)){ p[[i]]&lt;-ggplot(HKCancer_mort_inc,aes_string(fill=names(HKCancer_mort_inc)[3],x=names(HKCancer_mort_inc)[4], y=names(HKCancer_mort_inc)[i+4],group=names(HKCancer_mort_inc)[3]))+ geom_bar(position=&quot;dodge&quot;,stat=&quot;identity&quot;)+ facet_wrap(~Sex) + theme_classic()+ scale_fill_brewer(palette=&quot;Spectral&quot;)+ theme(legend.position=&quot;none&quot;)+ theme(text = element_text(size = 10))+ ggtitle(names(HKCancer_inc)[i+4])+ ylab(&quot;mortality-incidence ratio&quot;) } do.call(&#39;grid.arrange&#39;,c(p,ncol=3,nrow=12)) 9.0.3.5 d. Paired t-test on mortality-incidence ratio change In general, across the different cancer types is cancer treatment improving? We can use a paired t-test comparing the mortality-incidence ratio of cancers from the 1990-1999 period with the 2010-2019 period. #First sum up all incidence and mortality data for each cancer type across age and sex HKCancer_inc_sum &lt;- aggregate(HKCancer_inc[,-(1:4)],list(HKCancer_inc$Year),FUN=sum) HKCancer_mort_sum &lt;- aggregate(HKCancer_mort[,-(1:4)],list(HKCancer_mort$Year),FUN=sum) HKCancer_mort_inc_year &lt;- data.frame(Year=HKCancer_mort_sum[,1]) risk &lt;- function(mort,inc){ return (as.double(mort)/as.double(inc)) } for (i in 2:ncol(HKCancer_mort_sum)){ HKCancer_mort_inc_year[names(HKCancer_mort_sum)[i]] &lt;- mapply(risk,HKCancer_mort_sum[,i],HKCancer_inc_sum[,i]) } HKCancer_mort_inc_year_m &lt;-data.frame(`1990-1999` = as.numeric(HKCancer_mort_inc_year[1,2:37]), `2000-2009` = as.numeric(HKCancer_mort_inc_year[2,2:37]), `2010-2019` = as.numeric(HKCancer_mort_inc_year[3,2:37]), check.names = FALSE) HKCancer_mort_inc_year_m$Cancer &lt;- names(HKCancer_mort_inc_year)[-1] HKCancer_mort_inc_year_t&lt;-as_tibble(HKCancer_mort_inc_year_m) %&gt;% select(`Cancer`,`1990-1999`,`2000-2009`,`2010-2019`)%&gt;% gather (key=&quot;Year&quot;,value=&quot;MIR&quot;,-Cancer) plot&lt;-ggplot(HKCancer_mort_inc_year_t,aes(x=Year,y=MIR, color=Year))+ geom_boxplot(na.rm=T) + theme_classic()+ scale_color_brewer(palette=&quot;Dark2&quot;)+ geom_jitter(shape=16, position=position_jitter(0.2),na.rm=T)+ ylab(&quot;mortality-incidence ratio&quot;)+ ylim(0,1.2) + geom_signif(comparisons = list(c(&quot;1990-1999&quot;, &quot;2010-2019&quot;)), map_signif_level=F, test= &quot;t.test&quot;,test.args = list(paired = TRUE), na.rm = T, y_position = c(1.1, 1.3)) + geom_signif(comparisons = list(c(&quot;1990-1999&quot;, &quot;2000-2009&quot;)), map_signif_level=F, test= &quot;t.test&quot;,test.args = list(paired = TRUE), na.rm = T) + geom_signif(comparisons = list(c(&quot;2000-2009&quot;, &quot;2010-2019&quot;)), map_signif_level=F, test= &quot;t.test&quot;,test.args = list(paired = TRUE), na.rm = T) plot p&lt;-list() p[[1]]&lt;-t.test(HKCancer_mort_inc_year_m$`1990-1999`,HKCancer_mort_inc_year_m$`2000-2009`,paired=TRUE,alternative_m = &quot;two.sided&quot;) p[[2]]&lt;-t.test(HKCancer_mort_inc_year_m$`1990-1999`,HKCancer_mort_inc_year_m$`2010-2019`,paired=TRUE,alternative_m = &quot;two.sided&quot;) p[[3]]&lt;-t.test(HKCancer_mort_inc_year_m$`2000-2009`,HKCancer_mort_inc_year_m$`2010-2019`,paired=TRUE,alternative_m = &quot;two.sided&quot;) p #&gt; [[1]] #&gt; #&gt; Paired t-test #&gt; #&gt; data: HKCancer_mort_inc_year_m$`1990-1999` and HKCancer_mort_inc_year_m$`2000-2009` #&gt; t = 0.79695, df = 33, p-value = 0.4312 #&gt; alternative hypothesis: true mean difference is not equal to 0 #&gt; 95 percent confidence interval: #&gt; -0.01415536 0.03238636 #&gt; sample estimates: #&gt; mean difference #&gt; 0.009115498 #&gt; #&gt; #&gt; [[2]] #&gt; #&gt; Paired t-test #&gt; #&gt; data: HKCancer_mort_inc_year_m$`1990-1999` and HKCancer_mort_inc_year_m$`2010-2019` #&gt; t = 2.1861, df = 33, p-value = 0.036 #&gt; alternative hypothesis: true mean difference is not equal to 0 #&gt; 95 percent confidence interval: #&gt; 0.002055217 0.057209924 #&gt; sample estimates: #&gt; mean difference #&gt; 0.02963257 #&gt; #&gt; #&gt; [[3]] #&gt; #&gt; Paired t-test #&gt; #&gt; data: HKCancer_mort_inc_year_m$`2000-2009` and HKCancer_mort_inc_year_m$`2010-2019` #&gt; t = 1.5413, df = 35, p-value = 0.1322 #&gt; alternative hypothesis: true mean difference is not equal to 0 #&gt; 95 percent confidence interval: #&gt; -0.005438019 0.039734021 #&gt; sample estimates: #&gt; mean difference #&gt; 0.017148 9.0.3.6 e. Childhood versus elderly cancers Although it is clear that the incidence of cancer is typically higher in the elderly, some cancers affect children as well. What cancer types disproportionate affect children? For each cancer type, compare the proportion of 0-19 versus 65+ incidence against the 0-19 versus 65+ incidence for all other cancer types. # Examine the proportion of childhood HKCancer_inc &lt;- HKCancer[HKCancer$Type==&#39;incidence&#39;,] HKCancer_inc_age_sum &lt;- aggregate(HKCancer_inc[,-(1:4)],list(HKCancer_inc$Age),FUN=sum) HKCancer_inc_age_sum$Total&lt;- rowSums(HKCancer_inc_age_sum[,-1]) HKCancer_inc_age_sum_csq &lt;- data.frame(cancer=names(HKCancer_inc_age_sum[,2:ncol(HKCancer_inc_age_sum)])) pval &lt;- list() ratio &lt;- list() for (i in 2:(ncol(HKCancer_inc_age_sum))){ val = Map(&#39;-&#39;,HKCancer_inc_age_sum$Total,HKCancer_inc_age_sum[,i]) dat &lt;- data.frame(cancer=HKCancer_inc_age_sum[c(1,4),i], other =c(val[[1]],val[[2]])) pval &lt;- append(pval,fisher.test(dat)$p.val) ratio&lt;- append(ratio,log(as.double(dat[1,1]+0.1)/as.double(dat[2,1]+0.1))) } HKCancer_inc_age_sum_csq$pval &lt;- pval HKCancer_inc_age_sum_csq$ratio &lt;- ratio plot_child&lt;-ggplot(HKCancer_inc_age_sum_csq[-37,],aes(x=reorder(cancer,-as.numeric(ratio)),y=as.numeric(ratio)),fill=cancer)+ geom_bar(stat=&quot;identity&quot;,fill=&quot;red&quot;)+ geom_hline(yintercept=-4.12132318942113, linetype=&quot;dashed&quot;, color = &quot;black&quot;, linewidth=0.5)+ theme_classic()+ scale_fill_brewer(palette=&quot;Spectral&quot;)+ theme(legend.position=&quot;none&quot;)+ #theme(text = element_text(size = 10))+ theme(axis.title.x=element_blank())+ theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + ylab(&quot;log((child+0.1)/(elderly+0.1)&quot;) plot_child HKCancer_inc_age_sum_csq #&gt; cancer pval ratio #&gt; 1 Biliary 1.21618e-216 -11.17481 #&gt; 2 Bladder 9.601084e-314 -6.711128 #&gt; 3 Brain 0 -0.615666 #&gt; 4 Breast 0 -7.519824 #&gt; 5 Cervix 1.635299e-126 -6.925188 #&gt; 6 Colorectum 0 -7.531208 #&gt; 7 Eye 2.085337e-112 0.5039276 #&gt; 8 Hodgkin.lymphoma 3.658856e-81 -0.6227962 #&gt; 9 Kaposi 0.41662 -5.525453 #&gt; 10 Kidney 1.560619e-71 -3.882371 #&gt; 11 Larynx 1.963055e-113 -8.129416 #&gt; 12 Leukaemia 0 -1.043172 #&gt; 13 Liver 0 -4.909033 #&gt; 14 Lung 0 -8.191896 #&gt; 15 Melanoma 6.246412e-15 -4.324192 #&gt; 16 Mesothelioma 5.396388e-09 -7.897668 #&gt; 17 Multiple.myeloma 6.126613e-106 -7.062026 #&gt; 18 Nasal 0.0001285609 -3.286427 #&gt; 19 Nasopharynx 4.628193e-74 -3.95948 #&gt; 20 Non.Hodgkin.lymphoma 4.513518e-11 -2.940272 #&gt; 21 Non.melanoma.skin 0 -6.315107 #&gt; 22 Oesophagus 1.377432e-251 -8.943186 #&gt; 23 Oral 5.100139e-108 -4.379029 #&gt; 24 Ovary 0.0002847234 -2.34054 #&gt; 25 Pancreas 3.11451e-246 -6.690686 #&gt; 26 Penis 3.023792e-18 -5.753479 #&gt; 27 Placenta 0.07013259 2.397895 #&gt; 28 Prostate 0 -10.08778 #&gt; 29 Sarcoma 9.774701e-213 -1.028899 #&gt; 30 Small.intestine 1.479593e-31 -5.916202 #&gt; 31 Stomach 0 -7.137096 #&gt; 32 Testis 1.871184e-97 -0.09472556 #&gt; 33 Thymus 6.748934e-06 -1.915502 #&gt; 34 Thyroid 7.821825e-09 -2.194891 #&gt; 35 Uterus 1.457974e-105 -6.262217 #&gt; 36 Vulva 1.521831e-27 -5.552298 #&gt; 37 Total 1 -4.121323 9.0.4 Existing cancer funding and publication data The Hong Kong government established in Health and Medical Research Fund (HMRF) in 2011 to specifically provide research funding for health and medical research in Hong Kong. Since 2016 over 370 projects in the category of Cancer has been funded for a total of ~$400 M dollars. A list of all funded projects can be found on the Health Bureau webpage. You would like to use this data to see if there is any association between previous project funding and the epidemiology of cancers in Hong Kong. We can also do a similar thing with publications and ask if the research publications in Hong Kong have been aligned with the incidence and mortality. We can obtain this data from PubMed using the following terms: (“Hong Kong”[Affiliation]) AND (neoplasms[MeSH Terms]) The data has been predownloaded as the Pubmed API via R is a bit slow. Discussion points: Why has research publications increased dramatically in recent years? Is there something unusual with the dataset? What are the main cancer types being researched in Hong Kong? Is there any correlation between funding and cancer incidence and mortality? 9.0.4.1 Download HMRF grants and Pubmed data HMRF&lt;- read.delim(&quot;https://github.com/StatBiomed/BMDS-book/raw/main/notebooks/module5-epidemi/HMRF_cancer_grants.txt&quot;, sep = &quot;\\t&quot;, header = TRUE, stringsAsFactor=FALSE, check.names = TRUE) #HMRF pubmed&lt;- read.delim(&quot;https://github.com/StatBiomed/BMDS-book/raw/main/notebooks/module5-epidemi/pubmed__neoplasm_hongkong.txt&quot;, sep = &quot;\\t&quot;, header = TRUE, stringsAsFactor=FALSE, check.names = TRUE) #pubmed hist(pubmed$Publication.Year) 9.0.4.2 Make word cloud for grants 9.0.4.3 Compare grant funding with incidence and mortality if (!require(&quot;ggrepel&quot;)) install.packages(&quot;ggrepel&quot;) if (!require(&quot;ggplot2&quot;)) install.packages(&quot;gglot2&quot;) if (!require(&quot;gridExtra&quot;)) install.packages(&quot;gridExtra&quot;) library(ggrepel) library(ggplot2) library(gridExtra) grantpmsum&lt;- read.delim(&quot;https://github.com/StatBiomed/BMDS-book/raw/main/notebooks/module5-epidemi/Grants_pubmed_summary.txt&quot;, sep = &quot;\\t&quot;, header = TRUE, stringsAsFactor=FALSE, check.names = TRUE) grantpmsum #&gt; Cancer Grants Pubmed #&gt; 1 Biliary 0 72 #&gt; 2 Bladder 0 141 #&gt; 3 Brain 2 294 #&gt; 4 Breast 34 1147 #&gt; 5 Cervix 9 408 #&gt; 6 Colorectum 40 857 #&gt; 7 Eye 2 27 #&gt; 8 Hodgkin lymphoma 0 21 #&gt; 9 Kaposi 0 6 #&gt; 10 Kidney 0 133 #&gt; 11 Larynx 0 8 #&gt; 12 Leukaemia 18 515 #&gt; 13 Liver 97 2077 #&gt; 14 Lung 22 773 #&gt; 15 Melanoma 0 99 #&gt; 16 Mesothelioma 6 58 #&gt; 17 Multiple myeloma 0 104 #&gt; 18 Nasal 0 14 #&gt; 19 Nasopharynx 24 1113 #&gt; 20 Non-Hodgkin lymphoma 0 55 #&gt; 21 Skin 0 32 #&gt; 22 Oesophagus 7 404 #&gt; 23 Oral 5 229 #&gt; 24 Ovary 14 354 #&gt; 25 Pancreas 2 141 #&gt; 26 Penis 0 7 #&gt; 27 Placenta 0 9 #&gt; 28 Prostate 6 322 #&gt; 29 Sarcoma 2 216 #&gt; 30 Small intestine 0 11 #&gt; 31 Stomach 5 455 #&gt; 32 Testis 0 18 #&gt; 33 Thymus 0 3 #&gt; 34 Thyroid 2 231 #&gt; 35 Uterus 0 72 #&gt; 36 Vulva 0 8 HKCancer_inc &lt;- HKCancer[HKCancer$Type==&#39;incidence&#39;,] HKCancer_inc_sum &lt;- data.frame(incidence=colSums(HKCancer_inc[,-(1:4)])) HKCancer_mort &lt;- HKCancer[HKCancer$Type==&#39;mortality&#39;,] HKCancer_mort_sum &lt;- data.frame(mortality=colSums(HKCancer_inc[,-(1:4)])) HKCancer_compare &lt;-data.frame(cancer=grantpmsum$Cancer, grants=grantpmsum$Grants, pubmed=grantpmsum$Pubmed, incidence=colSums(HKCancer_inc[,-(1:4)]), mortality=colSums(HKCancer_mort[,-(1:4)])) gp&lt;-ggplot(HKCancer_compare, aes(y=grants, x=pubmed)) + geom_point() + scale_color_brewer(palette=&quot;Dark2&quot;) + theme_classic() + geom_label_repel(data=HKCancer_compare, aes(label=cancer), nudge_x = 2, size = 3.5,label.size = NA ,min.segment.length =unit(0, &#39;lines&#39;), max.overlaps = 45, box.padding = 0.4) + stat_cor(method = &quot;pearson&quot;, label.x = 3, label.y = 90) pinc&lt;-ggplot(HKCancer_compare, aes(y=pubmed, x=incidence)) + geom_point() + scale_color_brewer(palette=&quot;Dark2&quot;) + theme_classic() + geom_label_repel(data=HKCancer_compare, aes(label=cancer), nudge_x = 2, size = 3.5,label.size = NA ,min.segment.length =unit(0, &#39;lines&#39;), max.overlaps = 15, box.padding = 0.5)+ stat_cor(method = &quot;pearson&quot;, label.x = 40000, label.y = 0) pmor&lt;-ggplot(HKCancer_compare, aes(y=pubmed, x=mortality)) + geom_point() + scale_color_brewer(palette=&quot;Dark2&quot;) + theme_classic() + geom_label_repel(data=HKCancer_compare, aes(label=cancer), nudge_x = 2, size = 3.5,label.size = NA ,min.segment.length =unit(0, &#39;lines&#39;), max.overlaps = 15, box.padding = 0.5) + stat_cor(method = &quot;pearson&quot;, label.x = 40000, label.y = 0) ginc&lt;-ggplot(HKCancer_compare, aes(y=grants, x=incidence)) + geom_point() + scale_color_brewer(palette=&quot;Dark2&quot;) + theme_classic() + geom_label_repel(data=HKCancer_compare, aes(label=cancer), nudge_x = 2, size = 3.5,label.size = NA ,min.segment.length =unit(0, &#39;lines&#39;), max.overlaps = 40, box.padding = 0.4) + stat_cor(method = &quot;pearson&quot;, label.x = 40000, label.y = 0) gmor&lt;-ggplot(HKCancer_compare, aes(y=grants, x=mortality)) + geom_point() + scale_color_brewer(palette=&quot;Dark2&quot;) + theme_classic() + geom_label_repel(data=HKCancer_compare, aes(label=cancer), nudge_x = 2, size = 3.5,label.size = NA ,min.segment.length =unit(0, &#39;lines&#39;), max.overlaps = 40, box.padding = 0.4) + stat_cor(method = &quot;pearson&quot;, label.x = 40000, label.y = 0) grid.arrange(gp,ginc,gmor,pinc, pmor, ncol=3) 9.0.5 Open discussion As a group discuss what cancer type would be most worthy of funding in Hong Kong. Discuss what statistics and figures could be used to support your decision. If possible also discuss other data/analyses that can be performed and/or other diseases that are also in need of funding in Hong Kong. "],["genetic-seq.html", "Chapter 10 Genetic sequence 10.1 Case study 1: Genetic sequence analysis", " Chapter 10 Genetic sequence 10.1 Case study 1: Genetic sequence analysis 10.1.1 Sequence motif and k-mer Scenario: You are a biomedical data science trainee, and you want to use some real case studies to understand how to perform sequence analysis. From these practices, you hope to understand how the sickle cell disease happens and how the choice of gene editing site makes a potential therapy. From the lecture, you already understood that: the sickle cell disease happens due to a single nucleotide mutation on HBB ( coding the hemoglobin bata chain for adult use), causing a missense mutation on amino acid 7 from E to V. You can view more from ClinVar RCV000016574 and OMIM 603903. the therapy was to re-activate another gene HBG (gamma chain) that was used in the fetus. The HBG gene was repressed by a transcription factor BCL11A (as a protein) that serves as a repressor to HBG. Q1. How can we understand where the protein of the BCL11A gene prefers to bind on DNA? Answers: Using the binding motif database JASPAR: BCL11A’s motif is MA2324.1 It collected the sequences of many binding sites already, check the FASTA file or the HTML file. It’s Motif can be viewed as this file Q2. What is the meaning of position frequency matrix (PFM) and how to convert it into a position probability matrix (PPM, often used as position weight matrix PWM)? MA2324_PFM &lt;- matrix(c( 343, 180, 125, 5724, 155, 143, 4495, 5024, 120, 281, 92, 5855, 5964, 641, 591, 165, 5747, 100, 141, 55, 385, 307, 5800, 112, 349, 114, 103, 744), nrow = 4, byrow = TRUE ) rownames(MA2324_PFM) = c(&quot;A&quot;, &quot;C&quot;, &quot;G&quot;, &quot;T&quot;) MA2324_PFM #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] #&gt; A 343 180 125 5724 155 143 4495 #&gt; C 5024 120 281 92 5855 5964 641 #&gt; G 591 165 5747 100 141 55 385 #&gt; T 307 5800 112 349 114 103 744 # TODO: define position probability matrix HowTo: MA2324_PWM = MA2324_PFM / sum(MA2324_PFM[, 1]) MA2324_PWM #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] #&gt; A 0.05474860 0.02873105 0.01995211 0.91364725 0.02474062 0.022825219 0.71747805 #&gt; C 0.80191540 0.01915403 0.04485235 0.01468476 0.93455706 0.951955307 0.10231445 #&gt; G 0.09433360 0.02633679 0.91731844 0.01596169 0.02250599 0.008778931 0.06145251 #&gt; T 0.04900239 0.92577813 0.01787709 0.05570630 0.01819633 0.016440543 0.11875499 Q3. Let’s make the Motif logo ourselves by using weblogo.threeplusone.com You may use the first 500 lines of the sequences that we compiled: MA2324.1.sites-h500.txt HowTo: Copy and paste to weblogo.threeplusone.com Check which position has the highest information content in bit. Q4. Given the motif, how good is a certain sequence in terms of consistency? Take these two sequences as examples: sequence 1: CGGACCA (&gt;hg38_chr1:1000830-1000836(+)) sequence 2: CTGACCG (hg38_chr1:940785-940791(-)) # Use this One hot encoding function Onehot_Encode &lt;- function(sequence) { idx = match(strsplit(sequence, &#39;&#39;)[[1]], c(&quot;A&quot;, &quot;C&quot;, &quot;G&quot;, &quot;T&quot;)) seq_mat = matrix(0, 4, length(idx)) for (i in 1:length(idx)) seq_mat[idx[i], i] = 1 seq_mat } seq_vec1 = Onehot_Encode(&quot;CGGACCA&quot;) seq_vec2 = Onehot_Encode(&quot;CTGACCG&quot;) seq_vec2 #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] #&gt; [1,] 0 0 0 1 0 0 0 #&gt; [2,] 1 0 0 0 1 1 0 #&gt; [3,] 0 0 1 0 0 0 1 #&gt; [4,] 0 1 0 0 0 0 0 To calculate the consistency score of a certain sequence \\(S\\) to a given motif (\\(M\\) for the motif position weight matrix), you may recall what we introduced in the lecture (same as the reference papers [1, 2]): \\(P(S|M) = \\prod_{i=1}^k{M[s_i, i]}\\) Alternatively, you can also transform the sequence via our one hot encoding. Now, please try and see if you can calculate the motif score for the above two sequences (possibly in log10-scale). Example solution scripts: Meaning of motif score function: the log-likelihood of seeing a binding site sequence given the binding motif. Manual calculation step by step: colSums(seq_vec1 * MA2324_PWM) #&gt; [1] 0.80191540 0.02633679 0.91731844 0.91364725 0.93455706 0.95195531 0.71747805 colSums(seq_vec2 * MA2324_PWM) #&gt; [1] 0.80191540 0.92577813 0.91731844 0.91364725 0.93455706 0.95195531 0.06145251 seq_score1 = sum(log10(colSums(seq_vec1 * MA2324_PWM))) seq_score2 = sum(log10(colSums(seq_vec2 * MA2324_PWM))) c(seq_score1, seq_score2) #&gt; [1] -1.946979 -1.468304 Q5. Given a segment of a long sequence, how to find the potential motif sites? Scan the whole sequence Find the best matched position Here, we will take the gene HBG2 as an example, with the initial sequence segment as follows: ref|NC_000011.10|:c5255019-5254782 Homo sapiens chromosome 11, GRCh38.p14 Primary Assembly ATAAAAAAAATTAAGCAGCAGTATCCTCTTGGGGGCCCCTTCCCCACACTATCTCAATGCAAATATCTGT CTGAAACGGTCCCTGGCTAAACTCCACCCATGGGTTGGCCAGCCTTGCCTTGACCAATAGCCTTGACAAG GCAAACTTGACCAATAGTCTTAGAGTATCCAGTGAGGCCAGGGGCCGGCGGCTGGCTAGGGATGAAGAAT AAAAGGAAGCACCCTTCAGCAGTTCCAC We may load the sequence into R by our Onehot_Encode function: HBG2_segment &lt;- paste0( &quot;ATAAAAAAAATTAAGCAGCAGTATCCTCTTGGGGGCCCCTTCCCCACACTATCTCAATGCAAATATCTGT&quot;, &quot;CTGAAACGGTCCCTGGCTAAACTCCACCCATGGGTTGGCCAGCCTTGCCTTGACCAATAGCCTTGACAAG&quot;, &quot;GCAAACTTGACCAATAGTCTTAGAGTATCCAGTGAGGCCAGGGGCCGGCGGCTGGCTAGGGATGAAGAAT&quot;, &quot;AAAAGGAAGCACCCTTCAGCAGTTCCAC&quot; ) HBG2_segment #&gt; [1] &quot;ATAAAAAAAATTAAGCAGCAGTATCCTCTTGGGGGCCCCTTCCCCACACTATCTCAATGCAAATATCTGTCTGAAACGGTCCCTGGCTAAACTCCACCCATGGGTTGGCCAGCCTTGCCTTGACCAATAGCCTTGACAAGGCAAACTTGACCAATAGTCTTAGAGTATCCAGTGAGGCCAGGGGCCGGCGGCTGGCTAGGGATGAAGAATAAAAGGAAGCACCCTTCAGCAGTTCCAC&quot; HBG2_seg_vec = Onehot_Encode(HBG2_segment) # HBG2_seg_vec Example solution scripts: # TODO: try it yourself Q6. With the same sequence above, count the frequency of each 3-mer: List the all 3-mer Count each of the 3-mer in the sequence Example solution scripts: # TODO: try it yourself # Option 1: keep using one hot encoding matrix # Option 2: substr(x, start, stop) # you may make a vector with k-mer as name and use the above substr as index # this is related to dictionary data structure k = 3 bases = c(&quot;A&quot;, &quot;C&quot;, &quot;G&quot;, &quot;T&quot;) kmer_mat = unique(t(combn(rep(bases, k), m = k))) kmer = apply(kmer_mat, 1, paste, collapse=&#39;&#39;) kmer_count = rep(0, length(kmer)) names(kmer_count) = kmer # Keep trying 10.1.2 Functional mapping In this part, we will see how the sequence features may help predict molecular phenotype, namely splicing efficiency. You may read more in the original paper Hou &amp; Huang 2021 [4], if you are interested. Let’s look at the data plicing_efficiency_pred.csv columns 1 &amp; 2: gene ID and gene name (1850 genes) column 3: splicing efficiency (log scale) columns 4 to 99: 96 octamer features (the normalized frequency in intron sequence) Q7. Now, we can try using the multiple linear regression model to check how predictive the octamer frequencies are for splicing efficiency. Load the data (scripts below) Linear regression model Using 10-fold cross-validation to evaluate the model You may propose other alternative methods for assessing the association between octamer and splicing efficiency. library(caret) df_splicing &lt;- read.csv(&quot;plicing_efficiency_pred.csv&quot;) # Define training control # We also want to have savePredictions=TRUE &amp; classProbs=TRUE set.seed(0) my_trControl &lt;- trainControl(method = &quot;cv&quot;, number = 10, savePredictions = TRUE) # TODO: try it yourself to fill the rest 10.1.3 References Crooks, G. E., Hon, G., Chandonia, J. M., &amp; Brenner, S. E. (2004). WebLogo: a sequence logo generator. Genome research, 14(6), 1188-1190. Schneider, T. D., &amp; Stephens, R. M. (1990). Sequence logos: a new way to display consensus sequences. Nucleic acids research, 18(20), 6097-6100. Canver, M. C., Smith, E. C., Sher, F., Pinello, L., Sanjana, N. E., Shalem, O., … &amp; Bauer, D. E. (2015). BCL11A enhancer dissection by Cas9-mediated in situ saturating mutagenesis. Nature, 527(7577), 192-197. Hou, R., &amp; Huang, Y. (2022). Genomic sequences and RNA-binding proteins predict RNA splicing efficiency in various single-cell contexts. Bioinformatics, 38(12), 3231-3237. Splicing efficiency prediction dataset at https://github.com/StatBiomed/scRNA-efficiency-prediction. scripts to subset the data : df_Y = read.csv(paste0( &quot;https://github.com/StatBiomed/scRNA-efficiency-prediction/&quot;, &quot;raw/refs/heads/main/data/estimated/sck562_all_stochastical_gamma.csv&quot; ), row.names = 1) df_X0 = read.csv(paste0( &quot;https://github.com/StatBiomed/scRNA-efficiency-prediction/&quot;, &quot;raw/refs/heads/main/data/features/humanproteincode_octamer_gene_level.csv&quot; )) df_use = merge(df_Y, df_X0, by=&quot;gene_id&quot;, incomparables = NA) df_use$velocity_gamma = -log(df_use$velocity_gamma + 0.01) colnames(df_use)[3] = &quot;splicing_efficiency&quot; # df_use = df_use[-1600, ] df_use[, 4:ncol(df_use)] = df_use[, 4:ncol(df_use)] + 0.00001 df_use[, 3:ncol(df_use)] = round(df_use[, 3:ncol(df_use)], digits=6) options(digits = 3) write.csv(df_use, &quot;plicing_efficiency_pred.csv&quot;, quote = FALSE, row.names = FALSE) 10.1.3.1 Potential resources Scan motif # For Q5 motif_score_all = rep(NA, ncol(HBG2_seg_vec)-ncol(MA2324_PWM)) for (i in 1:(ncol(HBG2_seg_vec) - ncol(MA2324_PWM))) { motif_score_all[i] = sum(log10(colSums( HBG2_seg_vec[, i:(i + ncol(MA2324_PWM) - 1)] * MA2324_PWM ))) } motif_score_all[1:5] sort(motif_score_all, decreasing = TRUE)[1:7] # For Q4 # Potential motif score function # motif_score &lt;- function(sequence, motif_PWM) { # seq_vec = Onehot_Encode(sequence) # sum(log(colSums(seq_vec * motif_PWM))) # } K-mer counting # For Q6 kmer_count = rep(0, length(kmer)) names(kmer_count) = kmer for (i in 1:(nchar(HBG2_segment) - k + 1)) { a_kmer = substr(HBG2_segment, i, i+2) kmer_count[a_kmer] = kmer_count[a_kmer] + 1 } kmer_count Cross-validation for linear regression # For Q7 # Train the model cv_model &lt;- train(splicing_efficiency ~ ., data = df_splicing[, 3:ncol(df_splicing)], method = &quot;lm&quot;, trControl = my_trControl) # Check model performance print(cv_model) cor(cv_model$pred$obs, cv_model$pred$pred) ggplot(cv_model$pred, aes(x=obs, y=pred)) + geom_point() "],["genomics-regression.html", "Chapter 11 Genomics and prediction 11.1 Case study 1: splicing fedility prediction", " Chapter 11 Genomics and prediction 11.1 Case study 1: splicing fedility prediction Scenario: As a computational biology researcher, you want to find out what sequence-related features function as a regulator for splicing fidelity. You conducted a deep RNA-seq experience and quantified the splicing error frequency (SEF) for 141 splicing error events in yeast. Based on your prior knowledge and curiosity, you plan to further define a set of sequence-related features and test how predictive they are. 11.1.1 Questions for Discussion Q1. What sequence-related features you would think of? Q2. Given a set of candidate features, what statistical methods can you use to determine if a feature genuinely contributes to splicing fidelity? Q3. Given a candidate feature set, what could be the best subset of features for predicting the splicing fidelity and in what sense? 11.1.2 Hands-on with regression model Now, you are given a dataset with 141 splicing error events containing Y for splicing error frequency (SEF) and 14 predictors X. Data set and template notebook are available on Moodle (recommended) and also on this GitHub Repo. The information for columns: RPG: Ribosomal Protein Gene event: splicing error event type shift: nucleotide shift between canonical (ref, reference) and erroneous (alt, alternative) splice sites SEF: splicing error frequency intronL_ref: intron length for canonical (ref) intron 5ss_bpL_ref: length between 5 splice site to branch point DeltaG_intron_ref: the energy score in the secondary RNA structure, calculated by RNAFold. The lower the more stable. 5ss_motif_ref: the motif score of a sequence. The motif position weight matrix (PWM) is obtained by observed genes. The motif score is calculated as the log likelihood of each sequence given the PWM, followed by resealing to 100 for the maximum value. You can refer to slide 7 in lecture topic 1. intronL_alt: intron length for erroneous (alt) intron df = read.table(&quot;df_SEF_prediction.tsv&quot;, sep=&quot;\\t&quot;, header=1) head(df) #&gt; gene RPG event shift SEF intronL_ref X5ss_bpL_ref DeltaG_intron_ref #&gt; 1 TFC3 non_RPG down3ss 17 0.479869423 90 77 -0.11444 #&gt; 2 SEC17 non_RPG down3ss 47 0.021937843 116 77 -0.17155 #&gt; 3 ERD2 non_RPG up3ss 4 0.059303591 97 63 -0.16804 #&gt; 4 LSM2 non_RPG up5ss 56 0.019908116 128 92 -0.13906 #&gt; 5 LSM2 non_RPG up3ss 47 0.077444760 128 92 -0.13906 #&gt; 6 LSM2 non_RPG down3ss 40 0.002844017 128 92 -0.13906 #&gt; DeltaG_5ss_bp_ref DeltaG_bp_3ss_ref DeltaG_3ss_ref X5ss_motif_ref bp_motif_ref X3ss_motif_ref #&gt; 1 -0.13377 0.0 -6.2 0.7213 0.7703 0.0769 #&gt; 2 -0.10779 -10.5 -20.3 0.7695 0.9942 0.1461 #&gt; 3 -0.16190 -3.9 -8.7 0.8495 0.8835 0.1496 #&gt; 4 -0.12065 -0.9 -7.0 0.8305 0.6667 0.3677 #&gt; 5 -0.12065 -0.9 -7.0 0.8305 0.6667 0.3677 #&gt; 6 -0.12065 -0.9 -7.0 0.8305 0.6667 0.3677 #&gt; intronL_alt DeltaG_intron_alt DeltaG_3ss_alt X5ss_motif_alt X3ss_motif_alt #&gt; 1 107 -0.10841 -6.8 77.00 0.5223 #&gt; 2 163 -0.19202 -13.0 70.99 0.2926 #&gt; 3 93 -0.17527 -8.3 86.10 0.3329 #&gt; 4 184 -0.15217 -7.0 64.20 0.3955 #&gt; 5 81 -0.10864 -8.9 80.76 0.6179 #&gt; 6 168 -0.11667 -4.3 80.76 0.3341 11.1.2.1 A single feature Q4. Let’s start with a single feature, bp_motif_ref, is there a linear relationship with the output variable SEF Y? How do you determine it? Hint: try scatter plot to visualise the data. See example we mentioned before https://statbiomed.github.io/BMDS-book/introR.html#scatter-plot-1 # consider visualise it # df$SEF and df$bp_motif_ref library(ggplot2) # Write your codes here How about doing a log transform for SEF score? Consider adding a pseudo count, e.g., 0.0001. df[&quot;SEF_log&quot;] = log10(df$SEF + 0.0001) # visualize it again 11.1.2.2 All features with lm Q5. Can you build a linear model to perform the prediction by using all features? How to obtain the assessment statistics? Hint: consider using the lm function. See examples we mentioned before https://statbiomed.github.io/BMDS-book/introLinearReg.html#simple-linear-regression-with-lm-function # For the formula, you can use this one, but you can also write your own: lr_fm1 = as.formula(paste(&quot;SEF_log ~ 1 + &quot;, paste(colnames(df)[6:19], collapse=&quot; + &quot;))) # Write your codes for fitting the model here 11.1.2.3 Assessment on test data Q6. Can you perform the assessment on test data by splitting the data into training and test sets? We didn’t introduce how perform training &amp; test splitting in linear regression, but you can adapt the scripts that we introduced for logistic regression. Hint: you can recall the logistic regress chapter for the training &amp; test spiting: https://statbiomed.github.io/BMDS-book/introClassifier.html#logistic-regression-on-diabetes You can also just use the codes below, but think what is the ratio between training and test sets here: set.seed(0) idx_train = sample(nrow(df), size=0.5*nrow(df), replace = FALSE) df_train = df[idx_train, ] df_test = df[-idx_train, ] # recall the meaning of negative symbol Now, start training the model on the training set and predict on the test set. Hint: follow the codes here: https://statbiomed.github.io/BMDS-book/introClassifier.html#logistic-regression-on-diabetes # recall the predict function # lr_train = lm() # res_test = predict() For calculating R^2, another quick way is using the Pearson’s correlation coefficient R, as it gives equivalent values. We can do it with the following codes with the cor() function. # Please uncomment the codes below: # R_sqr = cor(res_test, df_test$SEF_log)**2 # R_sqr # Alternative function # cor.test(res_test, df_test$SEF_log) 11.1.2.4 Cross-validation for regression Q7. Can you perform a 10-fold cross validation, calculate the R^2 between predicted and observed log(SEF), and plot it with ggplot? Hint: We didn’t mention cross-validation for linear regression but again we can adapt the codes from the logistic regression chapter: https://statbiomed.github.io/BMDS-book/introClassifier.html#cross-validation You can also use the codes below, but try to fill the missing part (with question marks ???) yourself. library(caret) # Define training control # We also want to have savePredictions=TRUE &amp; classProbs=TRUE set.seed(0) my_trControl &lt;- trainControl(method = &quot;cv&quot;, number = 10, savePredictions = TRUE) # Train the model (uncomment the codes below) # cv_model &lt;- train(???, data = ???, # method = &quot;lm&quot;, # trControl = my_trControl) # Summarize the results # mean(cv_model$resample$Rsquared) # print(cv_model) By using print(cv_model), we can see the Rsquared value, which is an average of all R^2 acoss K folds (each fold has its own R^2 on the test set). Now, let’s manually calculate the R^2 between observed and predicted Y for all samples in an aggregated manner, despite that the Ys are predicted from different models in different folds. # Uncomment the codes below to view the first a few lines of the data frame # head(cv_model$pred) # Uncomment the codes below to calculate the R^2 # cor(cv_model$pred$pred, cv_model$pred$obs)**2 11.1.2.5 A subset of features Q8. Can you try a subset of features and see if you can improve the performance? Hint: You can consider forward method as mentioned here by checking the \\(p\\) value and only keep the significant ones (think which significance level \\(\\alpha\\) you would like to allow: https://statbiomed.github.io/BMDS-book/introLinearReg.html#multiple-regression-with-lm-function library(caret) # Define your new formular: # lr_fm2 = as.formula() set.seed(0) # Try the 10-fold cross-validation with the new feature set yourself # cv_model2 = ??? # Uncomment the codes below to calculate the R^2 # cor(cv_model2$pred$pred, cv_model2$pred$obs)**2 11.1.3 Open question Q9. Can you plot a scatter plot between log(SEF) vs X3ss_motif_ref, and colored by Ribosomal Protein Genes (RPG)? What did you see? Hint: follow the example on the diabetes data: https://statbiomed.github.io/BMDS-book/introClassifier.html#load-pima-indians-diabetes-database # Write you codes here with ggplot Q10. Is the SEF score mainly driven by the RPG label or is it genuinely related to the 3’ss motif? How to test it? Hint: consider adding RPG as a covariate. Is 3’ss motif still significant? # Define your new model lr_fm3 = as.formula(paste(&quot;SEF_log ~&quot;, paste(colnames(df)[c(2,6:19)], collapse=&quot; + &quot;))) # Write you codes below to fit the model Acknowledgement: this case study is adapted from the Figure 4 for the following research article: Aslanzadeh et al. Transcription rate strongly affects splicing fidelity and cotranscriptionality in budding yeast. Genome Research, 2018, 28 (2), 203-213 "],["install.html", "Appendix A: Install R &amp; RStudio A.1 Install R (&gt;=4.3.1) A.2 Install RStudio A.3 Use R inside RStudio A4. Cloud computing", " Appendix A: Install R &amp; RStudio This manual covers the installation of both R and RStudio for three different operating systems: Windows, macOS and Ubuntu. You only need to follow the one that you are using on your computer. Differences between R and RStudio R is the backbone of R programming. Once R is installed, you can use it via its built-in R Console (self-contained), terminal or any third-party integrated development environment (IDE), e.g., RStudio. RStudio is a multi-faceted and user-friendly IDE that can make R programming and data analysis in one place and easy to manage. We recommend using RStudio and only demonstrate with it, while you are free to use any other alternative. Acknowledgements This manual is adapted and updated from the materials produced by Xiunan Fang and other team members in Dr Joshua Ho’s lab. A.1 Install R (&gt;=4.3.1) R on Windows Open an internet browser and go to https://cran.r-project.org/. Click on the Download R for Windows link at the top of the page. Choose the base and then Click on the Download R 4.5.1 for Windows link at the top of the page (or a new version if this manual is outdated). Once the download is finished, you will obtain a file named R-4.5.1-win.exe or similar depending on the version that you download. Most of the time, you will likely want to go with the defaults, so click the button Next until the process is complete. R on macOS Open an internet browser and go to https://cran.r-project.org/. Click on the Download R for macOS link at the top of the page. Click on the file containing the latest version of R under the Latest release. Save the R-4.5.1-**.pkg file, double-click it to open, and follow the installation instructions. Note, there are two versions of the .pkg installation file according to the CPU model: Intel Macs (Intel-based) or M1/M2/.. Macs (ARM-based). Please choose accordingly. R on Linux (Ubuntu) As commonly used in Ubuntu, prior to installing R, let us update the system package index and upgrade all our installed packages using the following two commands: sudo apt update sudo apt -y upgrade After that, all that you have to do is run the following in the command line to install base R. sudo apt -y install r-base A.2 Install RStudio Now that R is installed, you need to download and install RStudio. The installation of RStudio is more straightforward and very similar across the three Operating Systems. Go to https://posit.co/download/rstudio-desktop/#download. We are using `RStudio Desktop Free version. Click on the right file for your OS (e.g., .exe file for Windows or .dmg for MacOS) The installation process is very straightforward as the figure below. A.3 Use R inside RStudio R studio RStudio is a very powerful IDE and provides many useful tools through a four-pane workspace. By default, the four panels are placed as follows (you can also change the setting for your own preference): Top-left panel: Your scripts of the R codes, script is good to keep a record of your work and also convenient for command execution. You can create a new script by File –&gt; New –&gt; R Script Bottom-left panel: R console for R commands, where you actually run the R codes. Top-right panel: Workspace tab: All the data(more specifically, R objects) you have created in the Workspace and all previous commands you previously ran in the History. Bottom-right panel: Files in your working directory(you probably should also set your working directory) in Files, and the plots you have created in Plots. Set working directory Create a folder named “biof_Rdir” in your preferred directory Create a “data” folder in the “biof_Rdir” From RStudio, use the menu to change your working directory under Session &gt; Set Working Directory &gt; Choose Directory Choose the directory to “biof_Rdir” Or you can type in the console: setwd(&quot;/yourdirectory/biof_Rdir&quot;) For Windows, the command might look like : setwd(&quot;c:/yourdirectory/biof_Rdir&quot;) Some general knowledge R is case-sensitive Type enter to run R code in the console pane Ctrl-enter or Cmd-return if the code is in the scripts pane. Comments come after # will not be treated as codes R has some pre-loaded data sets, to see the list of pre-loaded data, type data() In R, a function is an object, a basic syntax of an R function looks like something below: function_name &lt;- function(arg_1, arg_2, ...) { actual function codes } For example: my_average &lt;- function(x){ sum(x)/length(x) } my_average(c(1, 4, 5, 7)) #&gt; [1] 4.25 R contains a lot of built-in functions, you can use ? or help() to see the documentation of a function, there are also a lot of external libraries with specific functions. To use a library, we do: install.packages(&#39;package_name&#39;) library(package_name) Install packages There are several packages used in this workshop, in the R console, type: install.packages(&#39;ggplot2&#39;) install.packages(&#39;pheatmap&#39;) install.packages(&#39;aod&#39;) A4. Cloud computing In case you have limited computing power, you can still use cloud computing to finish this course. There can be multiple options and here we mainly recommend RStudio cloud (https://posit.cloud; previously known as https://rstudio.cloud/). You can explore directly from their website. "],["references-2.html", "References", " References "]]
