[["index.html", "Biomedical Data Science - introduction with case studies Welcome", " Biomedical Data Science - introduction with case studies BIOF1001 teaching team 2022-09-05 Welcome Welcome to the book Biomedical Data Science - an introduction with case studies. Most contents are demonstrated with R programming language. This book is designed as a collection of R Markdown notebooks, as supplementary to the lecture notes for the course BIOF1001: Introduction to Biomedical Data Science, an undergraduate course (Year 1) at the University of Hong Kong. Note: Most contents may be only updated before or right after the lectures, so please refer to the updated version. GitHub Repository: you can find the source files on StatBiomed/BMDS-book and the way to re-build this book. "],["preface.html", "Preface Introduction for readers Acknowledgements Last notes", " Preface This book is designed as the lecture notes and textbook for BIOF1001: Introduction to Biomedical Data Science, an undergraduate course (Year 1) at the University of Hong Kong. This book is not aimed to be a comprehensive textbook, but rather more Rmarkdown notebooks as supplementary to lecture notes so that students can reproduce the teaching contents more easily. Introduction for readers What we hope you will learn from this course / book In part I, you will find a general introduction to data science (by Dr YH Huang): Quantitative methods: t-test, correlation analysis, clustering, linear regression, linear classification. Basic programming and visualisation skills: R scripts for the above methods and data visualisation. Gain familiarity with common databases in the biomedical domain. Introduce ethical, legal, social and technological issues related to biomedical data sciences. Introduce good practice in managing a data science project and communicate results to key stakeholders. In part II, you will experience data types in five different biomedical topics, which will be illustrated with both introduction and cases that are suitable for problem-based learning format: Personalised genomic medicine, by Dr David Shih and Dr Yuanhua Huang Medical imaging and digital health, by Dr Joshua Ho and Dr Lequan Yu Infectious disease informatics, by Dr Tommy Lam and Dr Kathy Leung Population genetics and diseases, by Dr Clara Tang and Dr Asif Javed Epidemiology of cancer and other diseases, by Dr Jason Wong and Dr Carlos Wong What we recommend you do while reading this book To enhance the knowledge and skills learned from this book, we recommend the readers Read materials/slides provided in each module Practice quantitative skills by solving problems using R Acknowledgements Last notes "],["introR.html", "Chapter 1 Introduction to R programming 1.1 Data types 1.2 Data structures 1.3 Read and write files (tables) 1.4 Functions and Packages 1.5 Plotting 1.6 Scientific and statistical computating 1.7 Resource links", " Chapter 1 Introduction to R programming This notebook collects the scripts used for teaching in BIOF1001 for Introduction to R (1 hour teaching). You can get this Rmd file here (right click and “save link as” to download). 1.1 Data types During computing, all data are loaded / stored in memory (RAM) through a group of binary bits, usually 8 bits as a byte. In R language setting (similar to some other programming languages), there are a few commonly used data types that have specific number of bytes to use. In R, you can use the class() and typeof() functions to check the class and data type of any variable. In total, R has five data types: Numeric Integers Complex Logical Characters 1.1.1 nemeric (or double) The numberic is for numeric values, as the most common and the default data type. The numberic datatype saves values in double precision (double number of bytes in memory), so the type is also double. x &lt;- c(1.0, 2.0, 5.0, 7.0) x ## [1] 1 2 5 7 class(x) ## [1] &quot;numeric&quot; typeof(x) ## [1] &quot;double&quot; 1.1.2 integer The integer is another data type used for the set of all integers. You can use the capital ‘L’ notation as a suffix to specify a particular value as the integer data type. Also, you can convert a value into an integer type using the as.integer() function. y &lt;- c(1L, 2L, 5L, 7L) y ## [1] 1 2 5 7 class(y) ## [1] &quot;integer&quot; typeof(y) ## [1] &quot;integer&quot; # Assign a integer value to y y = 5 # is y an integer? print(is.integer(y)) ## [1] FALSE 1.1.3 logical In R, the logical data type takes either a value of true or false. A logical value is often generated when comparing between variables. z &lt;- c(TRUE, TRUE, TRUE, FALSE) z ## [1] TRUE TRUE TRUE FALSE typeof(z) ## [1] &quot;logical&quot; 1.1.4 character In R, the character is a data type where you have all the alphabets and special characters. It stores character values or strings. Strings in R can contain alphabets, numbers, and symbols. The character type is usually denoted by wrapping the value inside single or double inverted commas. w &lt;- c(&quot;aa&quot;, &quot;bb&quot;, &quot;5&quot;, &quot;7&quot;) w ## [1] &quot;aa&quot; &quot;bb&quot; &quot;5&quot; &quot;7&quot; typeof(w) ## [1] &quot;character&quot; 1.1.5 Memeory usage As we will see below, integer and logical uses only half of the memory of double (numeric) and character. x &lt;- c(1.0, 2.0, 5.0, 7.0) y &lt;- c(1L, 2L, 5L, 7L) z &lt;- c(TRUE, TRUE, TRUE, FALSE) w &lt;- c(&quot;aa&quot;, &quot;bb&quot;, &quot;5&quot;, &quot;7&quot;) object.size(x) ## 80 bytes object.size(rep(x, 1000)) ## 32048 bytes object.size(rep(y, 1000)) ## 16048 bytes object.size(rep(z, 1000)) ## 16048 bytes object.size(rep(w, 1000)) ## 32272 bytes 1.2 Data structures Data structure is one of most important features in programming. It involves how the data is organised and can be accessed and modified. Using an appropriate data structure may largely improve the computing efficiency. 1.2.1 Vector Vector is a basic data structure in R. It contains elements in the same data type (no matter double, integer, character or others). You can check the data type by using typeof() function and the length of the vector by length() function. Since, a vector must have elements of the same type, this function will try and coerce elements to the same type, if they are different. Coercion is from lower to higher types from logical to integer to double to character. More introduction see here. x &lt;- c(1, 2, 5, 7) x ## [1] 1 2 5 7 typeof(x) ## [1] &quot;double&quot; x &lt;- rep(3, 5) x ## [1] 3 3 3 3 3 typeof(x) ## [1] &quot;double&quot; x &lt;- 1:12 # integer x ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 typeof(x) ## [1] &quot;integer&quot; 1.2.2 Matrix Matrix is a two-dimensional data structure. It is in principle build based on vector but have more convenient built-in functions for computation. It has row and column, both of which can also have name. To check the dimensions, you can use the dim() function. More introduction see here. A &lt;- matrix(1:12, nrow=3) A ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 B &lt;- matrix(1:12, nrow=3, byrow=TRUE) B ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 5 6 7 8 ## [3,] 9 10 11 12 colnames(A) &lt;- c(&quot;C1&quot;,&quot;C2&quot;,&quot;C3&quot;,&quot;C4&quot;) rownames(A) &lt;- c(&quot;R1&quot;,&quot;R2&quot;,&quot;R3&quot;) A ## C1 C2 C3 C4 ## R1 1 4 7 10 ## R2 2 5 8 11 ## R3 3 6 9 12 1.2.2.1 Index Vector and Matrix Two index vector, you can use logical or integer, or the element name if it has. In R, the index starts from 1, unlike most programming languages where index start from 0. We can also use negative integers to return all elements except that those specified. But we cannot mix positive and negative integers while indexing and real numbers, if used, are truncated to integers. N.B., when using logical as index, be careful if the index length is different from the vector. x &lt;- 1:12 x[3] ## [1] 3 x[2:5] ## [1] 2 3 4 5 x[c(2, 5, 6)] # index with integer ## [1] 2 5 6 x[c(TRUE, FALSE, FALSE, TRUE)] # index with logical value ## [1] 1 4 5 8 9 12 Now, index matrix. Very similar as vector, but it has both row and column. A &lt;- matrix(1:12, nrow=3) colnames(A) &lt;- c(&quot;C1&quot;,&quot;C2&quot;,&quot;C3&quot;,&quot;C4&quot;) rownames(A) &lt;- c(&quot;R1&quot;,&quot;R2&quot;,&quot;R3&quot;) A[1, 2] ## [1] 4 A[1, &quot;C2&quot;] ## [1] 4 A[1, c(2, 3)] ## C2 C3 ## 4 7 A[1:2, c(2, 3)] ## C2 C3 ## R1 4 7 ## R2 5 8 Single row or column matrix will become a vector, unless using drop=FALSE A[1, 2:4] ## C2 C3 C4 ## 4 7 10 dim(A[1, 2:4]) ## NULL A[1, 2:4, drop=FALSE] ## C2 C3 C4 ## R1 4 7 10 dim(A[1, 2:4, drop=FALSE]) ## [1] 1 3 1.2.2.2 Modify values A[1, 2:4] &lt;- c(-3, -5, 20) A ## C1 C2 C3 C4 ## R1 1 -3 -5 20 ## R2 2 5 8 11 ## R3 3 6 9 12 1.2.3 List Different from vector that have all elements in the same data type, the list data structure can have components of mixed data types. More broadly, a list can contain a list of any data structure: value, vector, matrix, etc x &lt;- list(2.5, TRUE, 1:3) x ## [[1]] ## [1] 2.5 ## ## [[2]] ## [1] TRUE ## ## [[3]] ## [1] 1 2 3 str(x) ## List of 3 ## $ : num 2.5 ## $ : logi TRUE ## $ : int [1:3] 1 2 3 x &lt;- list(&quot;a&quot; = 2.5, &quot;b&quot; = TRUE, &quot;c&quot; = 1:3) x ## $a ## [1] 2.5 ## ## $b ## [1] TRUE ## ## $c ## [1] 1 2 3 str(x) ## List of 3 ## $ a: num 2.5 ## $ b: logi TRUE ## $ c: int [1:3] 1 2 3 1.2.3.1 Indexing list Different from vector and matrix, for list, you need to double-layer square brackets, either by numeric index or name. Alternatively, you can also use $ symbol. x[[3]] ## [1] 1 2 3 x[[&quot;c&quot;]] ## [1] 1 2 3 x$c ## [1] 1 2 3 1.2.4 Data Frame A special type of List: A list of vectors with the same length. Widely used as a rectangular data with flexible data type (like Excel) df &lt;- data.frame(&quot;SN&quot; = 1:2, &quot;Age&quot; = c(21,15), &quot;Name&quot; = c(&quot;John&quot;,&quot;Dora&quot;)) df ## SN Age Name ## 1 1 21 John ## 2 2 15 Dora df$Age[2] ## [1] 15 # View(df) 1.2.5 Factor vs vector Factor is a data structure used for fields that takes only predefined, finite number of values (categorical data) x = c(&quot;single&quot;, &quot;married&quot;, &quot;married&quot;, &quot;single&quot;) # vector x ## [1] &quot;single&quot; &quot;married&quot; &quot;married&quot; &quot;single&quot; class(x) ## [1] &quot;character&quot; typeof(x) ## [1] &quot;character&quot; y = factor(c(&quot;single&quot;, &quot;married&quot;, &quot;married&quot;, &quot;single&quot;)) y ## [1] single married married single ## Levels: married single class(y) ## [1] &quot;factor&quot; typeof(y) ## [1] &quot;integer&quot; 1.2.5.1 Change the order of factor levels z &lt;- factor(c(&quot;single&quot;, &quot;married&quot;, &quot;married&quot;, &quot;single&quot;) , levels=c(&quot;single&quot;, &quot;married&quot;)) z ## [1] single married married single ## Levels: single married 1.2.5.2 Smaller memory as categorical data type x = c(&quot;single&quot;, &quot;married&quot;, &quot;married&quot;, &quot;single&quot;) # vector y = factor(c(&quot;single&quot;, &quot;married&quot;, &quot;married&quot;, &quot;single&quot;)) object.size(rep(x, 1000)) ## 32160 bytes object.size(rep(y, 1000)) ## 16560 bytes 1.3 Read and write files (tables) Data is available in the data folder on the github repository 1.3.1 Read file See full manuals: help(read.table) or ?read.table help(&quot;read.table&quot;) ?read.table df = read.table(&quot;./SRP029880.colData.tsv&quot;, sep=&quot;\\t&quot;) df ## source_name group ## CASE_1 metastasized cancer CASE ## CASE_2 metastasized cancer CASE ## CASE_3 metastasized cancer CASE ## CASE_4 metastasized cancer CASE ## CASE_5 metastasized cancer CASE ## CTRL_1 normal colon CTRL ## CTRL_2 normal colon CTRL ## CTRL_3 normal colon CTRL ## CTRL_4 normal colon CTRL ## CTRL_5 normal colon CTRL 1.3.2 Write file df$frozen &lt;- c(1, 1, 0, 0, 0, 1, 1, 0, 0, 0) write.table(df, &quot;./SRP029880.colData.add_frozen.tsv&quot;, sep=&quot;\\t&quot;, quote=FALSE) 1.4 Functions and Packages For example mean() is a function here and it is from the base package x &lt;- 4:10 mean(x) ## [1] 7 base::mean(x) ## [1] 7 1.4.1 Install packages It depends on where the package is stored. Please refers to the documentation of the specific package you want to install and use. CRAN (the Comprehensive R Archive Network): main platform For example: install.packages(\"ggplot2\") Bioconductor: primarily for biology related packages For example: BiocManager::install(\"DESeq2\") #install.packages(&quot;ggplot2&quot;) 1.5 Plotting 1.5.1 datasets Using a built-in dataset for illustration: iris (4 flower features in 3plants) head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa summary(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## 1.5.2 Basic plotting 1.5.2.1 Histogram hist(iris$Sepal.Length) 1.5.2.2 Scatter plot plot(x=iris$Sepal.Length, y=iris$Sepal.Width) 1.5.2.3 boxplot x1 &lt;- iris$Sepal.Length[iris$Species == &quot;setosa&quot;] x2 &lt;- iris$Sepal.Length[iris$Species == &quot;versicolor&quot;] x3 &lt;- iris$Sepal.Length[iris$Species == &quot;virginica&quot;] boxplot(x1, x2, x3) 1.5.3 ggplot2 See more instructions: http://www.sthda.com/english/wiki/ggplot2-essentials 1.5.3.1 Install package if (!requireNamespace(&quot;ggplot2&quot;, quietly = TRUE)) install.packages(&quot;ggplot2&quot;) 1.5.3.2 Load package library(ggplot2) ## Need help getting started? Try the R Graphics Cookbook: ## https://r-graphics.org 1.5.3.3 Histogram ggplot(iris, aes(x=Sepal.Length)) + geom_histogram(bins = 8) 1.5.3.4 Scatter plot ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width)) + geom_point() 1.5.3.5 Box plot ggplot(iris, aes(x=Species, y=Sepal.Length)) + geom_boxplot() 1.6 Scientific and statistical computating 1.6.1 Orders of operators See lecture slides. If you are not sure about a certain ordering, use brackets! 5 * 2 &gt; 4 ## [1] TRUE 5 * (2 &gt; 4) ## [1] 0 1.6.2 Functions for statistics More theroy and practice to come in next session 1.6.3 Correlation cor(iris$Sepal.Length, iris$Petal.Length) ## [1] 0.8717538 cor.test(iris$Sepal.Length, iris$Petal.Length) ## ## Pearson&#39;s product-moment correlation ## ## data: iris$Sepal.Length and iris$Petal.Length ## t = 21.646, df = 148, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.8270363 0.9055080 ## sample estimates: ## cor ## 0.8717538 1.6.4 Hypothesis testing (t test) x1 &lt;- iris$Sepal.Length[iris$Species == &quot;setosa&quot;] x2 &lt;- iris$Sepal.Length[iris$Species == &quot;versicolor&quot;] x3 &lt;- iris$Sepal.Length[iris$Species == &quot;virginica&quot;] t.test(x2, x3) ## ## Welch Two Sample t-test ## ## data: x2 and x3 ## t = -5.6292, df = 94.025, p-value = 1.866e-07 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.8819731 -0.4220269 ## sample estimates: ## mean of x mean of y ## 5.936 6.588 1.6.5 Regression fit &lt;- lm(Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width, data=iris) summary(fit) # show results ## ## Call: ## lm(formula = Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width, ## data = iris) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.82816 -0.21989 0.01875 0.19709 0.84570 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.85600 0.25078 7.401 9.85e-12 *** ## Sepal.Width 0.65084 0.06665 9.765 &lt; 2e-16 *** ## Petal.Length 0.70913 0.05672 12.502 &lt; 2e-16 *** ## Petal.Width -0.55648 0.12755 -4.363 2.41e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3145 on 146 degrees of freedom ## Multiple R-squared: 0.8586, Adjusted R-squared: 0.8557 ## F-statistic: 295.5 on 3 and 146 DF, p-value: &lt; 2.2e-16 This means the fitted regression is: Sepal.Length ~ 1.856 + 0.65*Sepal.Width + 0.709*Petal.Length - 0.556*Petal.Width We can check how good the regression is by plotting it out y_pred &lt;- fit$coefficients[1] + fit$coefficients[2] * iris$Sepal.Width + fit$coefficients[3] * iris$Petal.Length + fit$coefficients[4] * iris$Petal.Width cor(iris$Sepal.Length, y_pred) ## [1] 0.926613 plot(iris$Sepal.Length, y_pred) 1.7 Resource links https://www.datamentor.io/r-programming/ https://www.geeksforgeeks.org/r-data-types/ https://data-flair.training/blogs/r-data-types/ http://www.sthda.com/english/wiki/ggplot2-essentials "],["introLinearReg.html", "Chapter 2 Introduction to Linear Regression 2.1 Linear Regression Using Simulated Data 2.2 Least Squares Using Simulated Data 2.3 Diagnostic check of a fitted regression model 2.4 Simple Linear Regression with lm function 2.5 Multiple Regression with lm function", " Chapter 2 Introduction to Linear Regression Acknowledgements: this chapter is adapted and updated from the materials originally produced by STAT1005 teaching team, especially Prof. Jeff Yao. 2.1 Linear Regression Using Simulated Data Let’s first simulate some data and look at how the predicted values (Ye) differ from the actual value (Y). 2.1.1 Simulating data: For X, we generate 100 normally distributed random numbers with mean 1.5 and standard deviation 2.5. For predicted value Ye, we assume an intercept (α) of 2 and a slope (β) of 0.3 and we write \\(Y_e = 2 + 0.3 x\\) Later, we will estimate the values of α and β using the least squares method and see how that changes the efficacy of the model. Though we estimate \\(Y_e = \\alpha + \\beta X\\), in reality Y is rarely perfectly linear. It usually has an error component or residual: \\(Y = \\alpha + \\beta X + R\\), where R is a random variable and is assumed to be normally distributed. Therefore for the actual value Y, we add a residual term (res), a random variable distributed normally with mean 0 and a standard deviation of 0.5. The following cell shows the code snippet to generate these numbers and convert these three columns in a data frame. Read through the code carefully and run the cell to output a sample of our simulated data. # Fix seed: each run gives the same random numbers so the same outputs. # Commenting out this line would read similar but different outputs at each run. # Try it out! set.seed(0) # Generate data X = 2.5 * rnorm(100) + 1.5 # Array of 100 values with mean = 1.5, stddev = 2.5 ypred = 2 + 0.3 * X # Prediction of Y, assuming a = 2, b = 0.3 res = 0.5 * rnorm(100) # Generate 100 residual terms yact = 2 + 0.3 * X + res # Actual values of Y # Create dataframe to store our X, ypred, and yact values df = data.frame(&#39;X&#39; = X, &#39;ypred&#39; = ypred, &#39;yact&#39; = yact) # Show the first six rows of our dataframe head(df) ## X ypred yact ## 1 4.6573857 3.397216 3.788145 ## 2 0.6844166 2.205325 1.816937 ## 3 4.8244982 3.447349 3.139354 ## 4 4.6810733 3.404322 3.427612 ## 5 2.5366036 2.760981 2.195788 ## 6 -2.3498751 1.295037 1.583397 Now let’s plot both the actual output (yact) and predicted output (ypred) against the input variable (X) to see what the difference between yact and ypred is, and therefore, to see how accurately the proposed equation (ypred = 2 + 0.3 * X) has been able to predict the value of the output: # You can use basic plotting functions # plot(x=df$X, y=df$yact, col=&quot;red&quot;) # lines(x=df$X, y=df$ypred, col=&quot;darkgreen&quot;) # But let&#39;s use ggplot2 for higher flexibility library(ggplot2) ggplot(df, aes(X)) + # basic graphical object geom_point(aes(y=yact), colour=&quot;black&quot;) + # first layer geom_line(aes(y=ypred), colour=&quot;darkgreen&quot;) + # second layer ggtitle(&#39;Actual vs Predicted values from the dummy dataset&#39;) 2.1.2 Model efficacy How do we know the values we calculate for α and β are giving us a good model? We can explain the total variability in our model with the Total Sum of Squares or SST: \\[SST = \\sum_{i=1}^n\\Bigl(\\text{yact}_i - \\text{yavg}\\Bigr)^2, \\qquad\\qquad \\text{yavg}=\\frac1n \\sum_{i=1}^n \\text{yact}_i\\] Mathematically, we have \\[ \\sum_{i=1}^n\\Bigl(\\text{yact}_i - \\text{yavg}\\Bigr)^2 = \\sum_{i=1}^n\\Bigl(\\text{ypred}_i -\\text{yavg} \\Bigr)^2 + \\sum_{i=1}^n\\Bigl(\\text{yact}_i - \\text{ypred}_i\\Bigr)^2\\] The identity reads as Sum of Squares Total = Sum of Squares Regression + Sum of Squares Error, or simply , SST = SSR + SSE. The Regression Sum of Squares or SSR measures the variation of the regression/predicted values, and the Sum of Squares Error SSE the variation between the actual and the predicted values. An alternative saying is that SSR is the difference explained by the model, SSE is the difference not explained by the model and is random, and SST is the total error. Note, we often use SSE (Sum of Squares Error) and SSD (Sum of Squares Difference) interchangeably. 2.1.3 R-Squared The higher the ratio of SSR to SST, the better the model is. This ratio is quantified by the coefficient of determination (also known as R2 or R-squared): \\[ R^2= \\frac{SSR}{SST}\\] Since \\(SST= SSR+SSE\\), \\(\\qquad 0\\le R^2\\le 1\\). The closer it is to 1, the better the model. Note that there are many other factors that we need to analyse before we can conclude a linear regression model is effective, but a high \\(R^2\\) is a pretty good indicator. Let’s see what the value of \\(R^2\\) is for our simulated dataset. # Calculate the mean of Y ymean = mean(df$yact) print(paste(&#39;Mean of Y =&#39;, ymean)) # paste brings a white space by default ## [1] &quot;Mean of Y = 2.44422555811815&quot; # Calculate SSR and SST df[&#39;SSR&#39;] = (df[&#39;ypred&#39;] - ymean)**2 df[&#39;SST&#39;] = (df[&#39;yact&#39;] - ymean)**2 SSR = sum(df[&#39;SSR&#39;]) SST = sum(df[&#39;SST&#39;]) # Calculate R-squared R2 = SSR / SST print(paste(&#39;R2 =&#39;, R2)) ## [1] &quot;R2 = 0.583160943681119&quot; The value of \\(R^2=0.583\\) suggests that ypred provides a decent prediction of the yact. We have randomly assumed some values for \\(\\alpha\\) and \\(\\beta\\), but these may or may not be the best values. In the next step, we will use the least sum of square method to calculate the optimum value for \\(\\alpha\\) and \\(\\beta\\) to see if there is an improvement in \\(R^2\\). To get started on the next step, open the notebook called 02-linearReg-02.Rmd. 2.2 Least Squares Using Simulated Data Now, using our simulated data from the previous step, let’s estimate the optimum values of our variable coefficients, \\(\\alpha\\) and \\(\\beta\\). Using the predictor variable, X, and the output variable, yact, we will calculate the values of \\(\\alpha\\) and \\(\\beta\\) using the Least Squares method described in the lecture. The cell below creates the same dataframe as previously. Run the cell to get started! set.seed(0) # Generate data X = 2.5 * rnorm(100) + 1.5 # Array of 100 values with mean = 1.5, stddev = 2.5 ypred = 2 + 0.3 * X # Prediction of Y, assuming a = 2, b = 0.3 res = 0.5 * rnorm(100) # Generate 100 residual terms yact = 2 + 0.3 * X + res # Actual values of Y # Create dataframe to store our X, ypred, and yact values df = data.frame(&#39;X&#39; = X, &#39;ypred&#39; = ypred, &#39;yact&#39; = yact) Just to reiterate, here are the formulas for \\(\\alpha\\) and \\(\\beta\\) again: \\[\\hat\\beta=\\frac{\\sum_{i=1}^n(X_i-\\bar X)(Y_i-\\bar Y)}{\\sum_{i=1}^n(X_i-\\bar X)^2}=\\frac{\\text{cov}(X,Y)}{\\text{var}(X)}\\] \\[\\hat\\alpha=\\bar Y-\\hat\\beta * \\bar X\\] To calculate these coefficients, we will create a few more columns in our df data frame. We need to calculate xmean and ymean to calculate the covariance of X and Y (xycov) and the variance of X (xvar) before we can work out the values for alpha and beta. # Calculate the mean of X and Y xmean = mean(X) ymean = mean(yact) # Calculate the terms needed for the numator and denominator of beta df[&#39;xycov&#39;] = (df[&#39;X&#39;] - xmean) * (df[&#39;yact&#39;] - ymean) df[&#39;xvar&#39;] = (df[&#39;X&#39;] - xmean)**2 # Calculate beta and alpha beta = sum(df[&#39;xycov&#39;]) / sum(df[&#39;xvar&#39;]) alpha = ymean - (beta * xmean) print(paste(&#39;alpha =&#39;, alpha, &#39;;&#39;, &#39;beta =&#39;, beta)) ## [1] &quot;alpha = 1.93401265576322 ; beta = 0.327758955833308&quot; As we can see, the values are only a little different from what we had assumed earlier. Let’s see how the value of \\(R^2\\) changes if we use the new values of \\(\\alpha\\) and \\(\\beta\\). The equation for the new model can be written as: \\[ y=1.934 + 0.328 * x \\] Let’s create a new column in df to accommodate the values generated by this equation and call this ypred2, and calculate the new \\(R^2\\). # Create new column to store new predictions df[&#39;ypred2&#39;] = alpha + beta * df[&#39;X&#39;] # Calculate new SSR with new predictions of Y. # Note that SST remains the same since yact and ymean do not change. df[&#39;SSR2&#39;] = (df[&#39;ypred2&#39;] - ymean)**2 df[&#39;SST&#39;] = (df[&#39;yact&#39;] - ymean)**2 SSR2 = sum(df[&#39;SSR2&#39;]) SST = sum(df[&#39;SST&#39;]) # Calculate new R2 R2_2 = SSR2 / SST print(paste(&#39;New R2 =&#39;, R2_2)) ## [1] &quot;New R2 = 0.69524214766491&quot; The new value of \\(R^2= 0.695\\) shows a slight improvement from the previous value of \\(R^2=0.583\\) (obtained with \\(\\alpha=2,~\\beta=0.3\\)). Let’s also plot our new prediction model against the actual values and our earlier assumed model, just to get a better visual understanding. library(ggplot2) # Put color into aes ggplot(df, aes(X)) + # basic graphical object geom_point(aes(y=yact), colour=&quot;black&quot;) + # first layer geom_line(aes(y=ypred, colour=&quot;Guess&quot;)) + # second layer geom_line(aes(y=ypred2, colour=&quot;OLS&quot;)) + # third layer scale_colour_manual(name=&quot;Models&quot;, values = c(&quot;Guess&quot;=&quot;darkgreen&quot;, &quot;OLS&quot;=&quot;red&quot;)) + ggtitle(&#39;Actual vs Predicted with guessed parameters vs Predicted with calculated parameters&#39;) As we can see, the ypred2 and ypred are more or less overlapping since the respective values of ɑ and β are not very different. Next, we will explore other methods of determining model efficacy by using the notebook called 02-linearReg-03.Rmd. 2.3 Diagnostic check of a fitted regression model Apart from the \\(R^2\\) statistic, there are other statistics and parameters that you need to look at in order to determine if the model is efficient. We will discuss some commonly used statistics – Residual Standard Errors, \\(p\\)-values, and \\(F\\)-statistics. 2.3.1 Residual Standard Errors (RSE) RSE is a common statistic used to calculate the accuracy of values predicted by a model. It is an estimate of the variance of the error term, res. For a simple linear regression model, RSE is defined as: \\[ RSE^2 = \\frac{SSE}{n-2} = \\frac1{n-2} \\sum_{i=1}^n \\Bigl(\\text{yact}_i - \\text{ypred}_i \\Bigr)^2. \\] In general, \\[ RSE^2 = \\frac{SSE}{n-p-1} = \\frac1{n-p-1} \\sum_{i=1}^n \\Bigl(\\text{yact}_i - \\text{ypred}_i \\Bigr)^2. \\] where \\(p\\) is the number of predictor variables in a model where we have more than one predictor variables. A multiple linear regression model is a linear regression model with multiple predictors, written as \\[ Y_e = \\alpha +\\beta_1 * X_1 +\\cdots +\\beta_p X_p. \\] As you see, the parameters and predictors are subscripted from 1 up to the number of predictors \\(p\\). In multiple regression, the value of RSE generally decreases as we add variables that are more significant predictors of the output variable. Using our simulated data from the previous steps, the following code snippet shows how the RSE for a model can be calculated: set.seed(0) # Generate data X = 2.5 * rnorm(100) + 1.5 # Array of 100 values with mean = 1.5, stddev = 2.5 res = 0.5 * rnorm(100) # Generate 100 residual terms yact = 2 + 0.3 * X + res # Actual values of Y # Create dataframe to store our X, ypred, and yact values df = data.frame(&#39;X&#39; = X, &#39;yact&#39; = yact) # Calculate the mean of X and Y xmean = mean(X) ymean = mean(yact) # Calculate the terms needed for the numator and denominator of beta df[&#39;xycov&#39;] = (df[&#39;X&#39;] - xmean) * (df[&#39;yact&#39;] - ymean) df[&#39;xvar&#39;] = (df[&#39;X&#39;] - xmean)**2 # Calculate beta and alpha beta = sum(df[&#39;xycov&#39;]) / sum(df[&#39;xvar&#39;]) alpha = ymean - (beta * xmean) print(paste(&#39;alpha =&#39;, alpha, &#39;;&#39;, &#39;beta =&#39;, beta)) ## [1] &quot;alpha = 1.93401265576322 ; beta = 0.327758955833308&quot; # Store predictions as in previous step df[&#39;ypred&#39;] = alpha + beta * df[&#39;X&#39;] # Show first five rows of dataframe head(df) ## X yact xycov xvar ypred ## 1 4.6573857 3.788145 4.1671116 9.6144310 3.460513 ## 2 0.6844166 1.816937 0.5471556 0.7608280 2.158336 ## 3 4.8244982 3.139354 2.2715611 10.6786935 3.515285 ## 4 4.6810733 3.427612 3.0724952 9.7618890 3.468276 ## 5 2.5366036 2.195788 -0.2434518 0.9602676 2.765407 ## 6 -2.3498751 1.583397 3.3628671 15.2611034 1.163820 # Calculate SSE df[&#39;SSE&#39;] = (df[&#39;yact&#39;] - df[&#39;ypred&#39;])**2 SSE = sum(df[&#39;SSE&#39;]) # Calculate RSE RSE = sqrt(SSE / 98) # n = 100 print(paste(&#39;RSE =&#39;, RSE)) ## [1] &quot;RSE = 0.481279277134956&quot; The value of RSE comes out to be 0.48. As you might have guessed, the smaller the residual standard errors, the better the model is. The benchmark to compare this to is the mean of the actual values, yact. As shown previously, this value is ymean = 2.54. In plain English, this means we observe an error of 0.48 over 2.44 - approximately 19.69%. error = RSE / ymean print(paste(&#39;Mean Y =&#39;, ymean)) ## [1] &quot;Mean Y = 2.44422555811815&quot; print(paste(&#39;Error =&#39;, error)) ## [1] &quot;Error = 0.196904608716023&quot; 2.3.2 p-values The calculation of \\(\\alpha\\) and \\(\\beta\\) are estimates, not exact calculations. Whether their values are significant or not needs to be tested using a hypothesis test. In the equation, \\(Y = \\alpha + \\beta X\\), if we set \\(\\beta=0\\), there will be no relation between \\(Y\\) and \\(X\\). Therefore, the hypothesis tests whether the value of \\(\\beta\\) is non-zero or not. \\[\\begin{align*} \\text{Null hypothesis}~ H_0~:~ \\beta=0, &amp; \\quad \\text{versus} \\\\ \\text{Alternative hypothesis}~ H_1~:~ \\beta\\ne 0.&amp; \\end{align*} \\] Whenever a regression task is performed and \\(\\beta\\) is calculated, there will be an accompanying p-value corresponding to this hypothesis test. We will not go through how this is calculated in this course (you can learn more here), since it is calculated automatically by ready-made methods in R. If the p-value is less than a chosen significance level (e.g. 0.05) then the null hypothesis that \\(\\beta = 0\\) is rejected and \\(\\beta\\) is said to be significant and non-zero. In the case of multiple linear regression, the p-value associated with each \\(\\beta_k\\) can be used to weed out insignificant predictors from the model. The higher the p-value for \\(\\beta_k\\), the less significant \\(X_k\\) is to the model. 2.3.3 F-statistics In a multiple regression model, apart from testing the significance of individual variables by checking the p-values, it is also necessary to check whether, as a group all the predictors are significant. This can be done using the following hypothesis: \\[\\begin{align*} \\text{Null hypothesis}~ H_0~:~ &amp; \\beta_1=\\beta_2=\\cdots=\\beta_p=0, \\quad \\text{versus} \\\\ \\text{Alternative hypothesis}~ H_1~:~&amp; \\text{at least one of the} ~\\beta_k&#39;s ~ \\text{is non zero}. \\end{align*} \\] The statistic that is used to test this hypothesis is called the F-statistic and is defined as follows: \\[ F\\text{-statistic} = \\text{Fisher statistic}= \\frac{ (SST-SSE)/p}{ SSE/(n-p-1)} \\] where \\(n\\) = number of rows (sample points) in the dataset and \\(p\\) = number of predictor variables in the model. There is a \\(p\\)-value that is associated with this \\(F\\)-statistic. If the \\(p\\)-value is smaller than the chosen significance level, the null hypothesis can be rejected. It is important to look at the F-statistic because: p-values are about individual relationships between predictors and the outcome variable. However, one predictor’s relationship with the output might be impacted by the presence of other variables. When the number of predictors in the model is very large and all the \\(\\beta_i\\) are very close to zero, the individual p-values associated with the predictors might give very small values so we might incorrectly conclude that there is a relationship between the predictors and the outcome. 2.4 Simple Linear Regression with lm function There are a few R packages, e.g., the built-in stat package have a lm (linear model) function to fit linear regression very easy - much easier than implementing from scratch like we did in the last lesson. See more details in the lm manual. We will start with the datarium library which contain the advertising data. # Install datarium library if you haven&#39;t if (!requireNamespace(&quot;datarium&quot;, quietly = TRUE)) { install.packages(&quot;datarium&quot;) } library(datarium) # Load data: then we will have a data.frame with name marketing data(marketing) head(marketing) ## youtube facebook newspaper sales ## 1 276.12 45.36 83.04 26.52 ## 2 53.40 47.16 54.12 12.48 ## 3 20.64 55.08 83.16 11.16 ## 4 181.80 49.56 70.20 22.20 ## 5 216.96 12.96 70.08 15.48 ## 6 10.44 58.68 90.00 8.64 We can also check summary statistics of each column summary(marketing) ## youtube facebook newspaper sales ## Min. : 0.84 Min. : 0.00 Min. : 0.36 Min. : 1.92 ## 1st Qu.: 89.25 1st Qu.:11.97 1st Qu.: 15.30 1st Qu.:12.45 ## Median :179.70 Median :27.48 Median : 30.90 Median :15.48 ## Mean :176.45 Mean :27.92 Mean : 36.66 Mean :16.83 ## 3rd Qu.:262.59 3rd Qu.:43.83 3rd Qu.: 54.12 3rd Qu.:20.88 ## Max. :355.68 Max. :59.52 Max. :136.80 Max. :32.40 This dataset contains data about the advertising budget spent on YouTub, Radio, and Newspapers for a particular product and the resulting sales. We expect a positive correlation between such advertising costs and sales. Let’s start with YouTub advertising costs to create a simple linear regression model. First let’s plot the variables to get a better sense of their relationship: # Create scatter plot library(ggplot2) ggplot(marketing, aes(x=youtube, y=sales)) + geom_point(colour=&quot;black&quot;) + ggtitle(&#39;YouTube vs Sales&#39;) As YouTube advertisement cost increases, sales also increase – they are positively correlated! Now with the linear model lm function, let’s create a line of best fit using the least sum of square method. # Fit linear regression # By default it include an incepter, so it is equvialent to add &quot;+ 1&quot; # res.lm &lt;- lm(sales ~ youtube + 1, data = marketing) res.lm &lt;- lm(sales ~ youtube, data = marketing) In the above code, we used lm to fit our simple linear regression model. This takes the formula y ~ X, where X is the predictor variable (YouTube advertising costs) and y is the output variable (Sales). Then, this function will return fitted model via a ordinary least squares (OLS) method. The res.lm is a list, you can get the it attributes by e.g., res.lm$coefficients res.lm$coefficients ## (Intercept) youtube ## 8.43911226 0.04753664 In the notation that we have been using, \\(\\alpha\\) is the intercept and \\(\\beta\\) is the slope i.e.: \\(\\alpha = 8.439, \\quad \\beta = 0.048\\) Thus, the equation for the model will be: \\(\\text{Sales} = 8.439 + 0.048*\\text{YouTube}\\) Let’s also check an indicator of the model efficacy, R2. Luckily, summary function can calculate it from the lm output and gives us a ready-made method for doing this so we don’t need to code all the math ourselves: res_summary = summary(res.lm) # Again, res_summary is also a list res_summary$r.squared ## [1] 0.6118751 We can also take a look at the model summary by writing this snippet: # Print out the summary summary(res.lm) ## ## Call: ## lm(formula = sales ~ youtube, data = marketing) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.0632 -2.3454 -0.2295 2.4805 8.6548 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.439112 0.549412 15.36 &lt;2e-16 *** ## youtube 0.047537 0.002691 17.67 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.91 on 198 degrees of freedom ## Multiple R-squared: 0.6119, Adjusted R-squared: 0.6099 ## F-statistic: 312.1 on 1 and 198 DF, p-value: &lt; 2.2e-16 There is a lot here. Of these results, we have discussed: R-squared F-statistic Prob (F-statistic) - this is the p-value of the F-statistic Intercept coef - this is alpha YouTub coef - this is beta for predictor YouTub P&gt;|t| - this is the p-value for our coefficients Now that we’ve fit a simple regression model, we can try to predict the values of sales based on the equation we just derived! sales_pred = predict(res.lm, newdata = marketing[c(&#39;youtube&#39;)]) marketing[&#39;sales_pred&#39;] = sales_pred The predict fucntion predicts sales value for each row based on the model equation using YouTub costs. This is the equivalent of manually typing out our equation: sales_pred = 8.439 + 0.048*(advert['youtube']). We can visualise our regression model by plotting sales_pred against the YouTube advertising costs to find the line of best fit: library(ggplot2) ggplot(marketing, aes(x=youtube)) + geom_point(aes(y=sales), colour=&quot;black&quot;) + geom_line(aes(y=sales_pred), colour=&quot;red&quot;) + ggtitle(&#39;YouTube vs Sales&#39;) In the next step, we will add more features as predictors and see whether it improves our model. Go to the the notebook called 02-linearReg-05.Rmd. 2.5 Multiple Regression with lm function A multiple linear regression is simply a linear regression that involves more than one predictor variable. It is represented as: \\[\\qquad Y_e = \\alpha + \\beta_1*X_1 + \\beta_2*X_2 + \\dots + \\beta_p*X_p\\] Each βi will be estimated using the least sum of squares method. The data set is \\[ \\begin{array} {~~} Y_1, &amp; X_1^{(1)}, &amp; \\ldots, &amp; X_p^{(1)} \\\\ Y_2, &amp; X_1^{(2)}, &amp; \\ldots, &amp; X_p^{(2)} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ Y_n, &amp; X_1^{(n)}, &amp; \\ldots, &amp; X_p^{(n)} \\end{array} \\] For each sample \\(i\\), the predicted value by the model is: \\(\\qquad Y_{i,e} = \\alpha + \\beta_1*X_1^{(i)} + \\beta_2*X_2^{(i)} + \\dots + \\beta_p*X_p^{(i)}\\) Define the sum of squares \\[ S(\\alpha,\\beta_1,\\ldots,\\beta_p) = \\sum_{i=1}^n \\left\\{ Y_i -Y_{i,e}\\right\\}^2 =\\sum_{i=1}^n \\left\\{ Y_i -\\left( \\alpha + \\beta_1*X_1^{(i)} + \\beta_2*X_2^{(i)} + \\dots + \\beta_p*X_p^{(i)}\\right)\\right\\}^2 \\] Least squares estimators: solve \\[ \\frac{\\partial S(\\alpha,\\beta_1,\\ldots,\\beta_p)}{\\partial \\alpha}=0,\\quad \\frac{\\partial S (\\alpha,\\beta_1,\\ldots,\\beta_p)}{\\partial \\beta_1}=0,\\quad \\ldots,\\quad \\frac{\\partial S (\\alpha,\\beta_1,\\ldots,\\beta_p)}{\\partial \\beta_p}=0. \\] to obtain the least squares estimators of the parameters \\[ \\hat\\alpha, \\hat\\beta_1,\\ldots,\\hat\\beta_p. \\] Note that be definition, \\[ SSE = S(\\hat\\alpha, \\hat\\beta_1,\\ldots,\\hat\\beta_p). \\] In other words, the fitted SSE (sum of squares error) is the minimized value of the sum squares with the estimated values of the parameters. The more varibles, the smaller the \\(R^2\\) Consider two regression models \\(\\quad ~ Y_e = \\alpha + \\beta_1*X_1\\) \\(\\quad \\tilde Y_e = \\alpha + \\beta_1*X_1 + \\beta_2*X_2\\) The model (II) has one more input variable \\(X_2\\). The \\(SSE_I\\) of Model (I) is the minimum of \\[ S_I(\\alpha,\\beta_1) = \\sum_{i=1}^n \\left\\{ Y_i -\\left( \\alpha + \\beta_1*X_1^{(i)} \\right)\\right\\}^2 \\] over all possible values of \\((\\alpha,\\beta_1)\\). The \\(SSE_{II}\\) of Model (II) is the minimum of \\[ S_{II}(\\alpha,\\beta_1,\\beta_2) = \\sum_{i=1}^n \\left\\{ Y_i -\\left( \\alpha + \\beta_1*X_1^{(i)} +\\beta_2*X_2^{(i)} \\right)\\right\\}^2. \\] over all possible values of \\((\\alpha,\\beta_1,\\beta_2)\\). Because \\(\\quad S_I(\\alpha,\\beta_1) = S_{II}(\\alpha,\\beta_1,\\beta_2=0 )\\), we find that \\(SSE_{II}\\le SSE_I\\), so \\[ R^2_{II} = SST - SSE_{II} \\ge SST - SSE_{I} = R^2_{I}. \\] With this simple dataset of three predictor variables, there can be seven possible models: Sales ~ YouTube Sales ~ Newspaper Sales ~ Facebook Sales ~ YouTube + Facebook Sales ~ YouTube + Newspaper Sales ~ Newspaper + Facebook Sales ~ YouTube + Facebook + Newspaper Generally, if there are p possible predictor variables, there can be (2p - 1) possible models – this can get large very quickly! Thankfully, there are a few guidelines to filter some of these and then navigate towards the most efficient one. Keep variables with low p-values and eliminate ones with high p-values Keep variables that increase the value of adjusted-R2 – this penalizes the model for adding insignificant variables and increases when we add significant variables. It is calculated by: \\[ R^2_{adj} = 1- (1-R^2) \\frac{n-1}{n-p-1}\\] Based on these guidelines, there are two approaches to select the predictor variables in the final model: Forward selection: start with a null model (no predictors), then add predictors one by one. If the p-value for the variable is small enough and the value of the adjusted-R2 goes up, the predictor is included in the model. Otherwise, it is not included. Backward selection: starts with a model that has all the possible predictors and discard some of them. If the p-value of a predictor variable is large and adjusted-R2 is lower when removed, it is discarded from the model. Otherwise, it remains a part of the model. Many statistical programs give us an option to select from these approaches while implementing multiple linear regression. For now, let’s manually add a few variables and see how it changes the model parameters and efficacy. First, add the newspaper variable to the model: library(datarium) data(marketing) head(marketing) ## youtube facebook newspaper sales ## 1 276.12 45.36 83.04 26.52 ## 2 53.40 47.16 54.12 12.48 ## 3 20.64 55.08 83.16 11.16 ## 4 181.80 49.56 70.20 22.20 ## 5 216.96 12.96 70.08 15.48 ## 6 10.44 58.68 90.00 8.64 res_lm2 = lm(sales ~ youtube + newspaper, data=marketing) summary(res_lm2) ## ## Call: ## lm(formula = sales ~ youtube + newspaper, data = marketing) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.3477 -2.0815 -0.1138 2.2711 10.1415 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.929938 0.630405 10.993 &lt; 2e-16 *** ## youtube 0.046901 0.002581 18.173 &lt; 2e-16 *** ## newspaper 0.044219 0.010174 4.346 2.22e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.745 on 197 degrees of freedom ## Multiple R-squared: 0.6458, Adjusted R-squared: 0.6422 ## F-statistic: 179.6 on 2 and 197 DF, p-value: &lt; 2.2e-16 As you see, the p-values for the coefficients are very small, suggesting that all the estimates are significant. The equation for this model will be: \\[ \\text{Sales} = 6.93+0.046* \\text{YouTube} + 0.044 * \\text{Newspaper}\\] The values of R2 and adjusted R2 are 0.646 and 0.642, which is just a minor improvement from before (0.612 and 0.610, respectively). Similarly for RSE (3.745). Only a small decrease in RSE and error… Let’s take a closer look at the summary above. The Adj-R2 increases slightly, but the F-statistic decreases (from 312.1 to 179.6), as does the associated p-value. This suggests that adding newspaper didn’t improve the model significantly. Let’s try adding facebook instead: # Initialise and fit new model with TV and Radio as predictors # model3 = smf.ols(&#39;Sales ~ TV + Radio&#39;, data=advert).fit() # print(model3.summary()) res_lm3 = lm(sales ~ youtube + facebook, data=marketing) summary(res_lm3) ## ## Call: ## lm(formula = sales ~ youtube + facebook, data = marketing) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.5572 -1.0502 0.2906 1.4049 3.3994 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.50532 0.35339 9.919 &lt;2e-16 *** ## youtube 0.04575 0.00139 32.909 &lt;2e-16 *** ## facebook 0.18799 0.00804 23.382 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.018 on 197 degrees of freedom ## Multiple R-squared: 0.8972, Adjusted R-squared: 0.8962 ## F-statistic: 859.6 on 2 and 197 DF, p-value: &lt; 2.2e-16 This gives us the model: \\[ \\text{Sales} = 3.51+0.046* \\text{YouTube} + 0.188 * \\text{Facebook}\\] The adjusted R2 value has improved considerably, as did the RSE and F-statistic, indicating an efficient model. Thus, we can conclude that facebook is a great addition to the model. YouTube and facebook advertising costs together are able to predict sales well. But, can we improve it a bit further by combining all three predictor variables? Try it out: see if you can figure out how to do this on your own! # Initialise and fit new model with TV, Newspaper, and Radio as predictors # Print summary of regression results # Calculate RSE - don&#39;t forget that the number of predictors p is now 3 You should get the equation: \\[ \\text{Sales} = 3.53+0.046*\\text{YouTube} -0.001*\\text{Newspaper} +0.188*\\text{Facebook}\\] You should also find that: RSE increases slightly, the coefficient for newspaper is negative, and the F-statistic decreases considerably from 859.6 to 570.3. All these suggest that the model actually became less efficient on addition of newspaper. Why? This step shows clearly that adding one more input variable Newspaper in Model 3 does not lead to any improvement. "],["introClassifier.html", "Chapter 3 Introduction to Classification 3.1 Visualise logistic and logit functions 3.2 Logistic regression on Diabetes 3.3 Cross-validation 3.4 More assessment metrics", " Chapter 3 Introduction to Classification 3.1 Visualise logistic and logit functions In this chapter, we will focus on logistic regression for classification. Let’s first look at what logistic and logit function look like. 3.1.1 Logistic function Let’s write our first function logestic() as follows. # Write your first function logistic &lt;- function(y) { exp(y) / (1 + exp(y)) } # Try it with different values: logistic(0.1) ## [1] 0.5249792 logistic(c(-3, -2, 0.5, 3, 5)) ## [1] 0.04742587 0.11920292 0.62245933 0.95257413 0.99330715 This is the equivalent to the built-in plogis() function in the stat package for the logistic distribution: plogis(0.1) ## [1] 0.5249792 plogis(c(-3, -2, 0.5, 3, 5)) ## [1] 0.04742587 0.11920292 0.62245933 0.95257413 0.99330715 3.1.2 Logit function Now, let look at the logistic’s inverse function logit(), and let’s define it manually. Note, this function only support input between 0 and 1. # Write your first function logit &lt;- function(x) { log(x / (1 - x)) } # Try it with different values: logit(0.4) ## [1] -0.4054651 logit(c(0.2, 0.3, 0.5, 0.7, 0.9)) ## [1] -1.3862944 -0.8472979 0.0000000 0.8472979 2.1972246 logit(c(-1, 2, 0.4)) ## Warning in log(x/(1 - x)): NaNs produced ## [1] NaN NaN -0.4054651 Again, the built-in stat package’s logistic distribution has an equivalent function qlogis(), though with a different name. qlogis(0.4) ## [1] -0.4054651 qlogis(c(0.2, 0.3, 0.5, 0.7, 0.9)) ## [1] -1.3862944 -0.8472979 0.0000000 0.8472979 2.1972246 qlogis(c(-1, 2, 0.4)) ## Warning in qlogis(c(-1, 2, 0.4)): NaNs produced ## [1] NaN NaN -0.4054651 3.1.3 Visualise the distribution Logisitc function # You can use seq() function to generate a vector # Check how to use it by help(seq) or ?seq x = seq(-7, 7, 0.3) df = data.frame(&#39;x&#39;=x, &#39;logistic&#39;=plogis(x)) # You can plot by plot function # plot(x=df$x, y=df$logistic, type=&#39;o&#39;) # Or ggplot2 library(ggplot2) ggplot(df, aes(x=x, y=logistic)) + geom_point() + geom_line() Logit function x = seq(0.001, 0.999, 0.01) df = data.frame(&#39;x&#39;=x, &#39;logit&#39;=qlogis(x)) ggplot(df, aes(x=x, y=logit)) + geom_point() + geom_line() 3.2 Logistic regression on Diabetes 3.2.1 Load Pima Indians Diabetes Database This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage. The datasets consist of several medical predictor (independent) variables and one target (dependent) variable, Outcome. Independent variables include the number of pregnancies the patient has had, their BMI, insulin level, age, and so on. Acknowledgement: This notebook is adapted and updated from STAT1005. # Install the mlbench library for loading the datasets if (!requireNamespace(&quot;mlbench&quot;, quietly = TRUE)) { install.packages(&quot;mlbench&quot;) } # Load data library(mlbench) data(PimaIndiansDiabetes) # Check the first few lines dim(PimaIndiansDiabetes) ## [1] 768 9 head(PimaIndiansDiabetes) ## pregnant glucose pressure triceps insulin mass pedigree age diabetes ## 1 6 148 72 35 0 33.6 0.627 50 pos ## 2 1 85 66 29 0 26.6 0.351 31 neg ## 3 8 183 64 0 0 23.3 0.672 32 pos ## 4 1 89 66 23 94 28.1 0.167 21 neg ## 5 0 137 40 35 168 43.1 2.288 33 pos ## 6 5 116 74 0 0 25.6 0.201 30 neg Now, let’s check two potential features: glucose and age, colored by the diabetes labels. library(ggplot2) ggplot(data=PimaIndiansDiabetes, aes(x=glucose, y=age)) + geom_point(aes(color=diabetes)) Before we start fit models, let’s split the data into training and test sets in a 4:1 ratio. Let define it manually, though there are functions to do it automatically. set.seed(0) idx_train = sample(nrow(PimaIndiansDiabetes), size=0.75*nrow(PimaIndiansDiabetes), replace = FALSE) df_train = PimaIndiansDiabetes[idx_train, ] df_test = PimaIndiansDiabetes[-idx_train, ] # recall the meaning of negative symbol 3.2.2 Fit logistic regression In logistic regression, the predicted probability to be class 1 is: \\[P(y=1|X, W) = \\sigma(w_0, x_1 * w_1 + ... + x_p * w_p)\\] where the \\(\\sigma()\\) denotes the logistic function. In R, the built-in package stats already have functions to fit generalised linear model (GLM), including logistic regression, a type of GML. Here, let’s start with the whole dataset to fit a logistic regression. Note, we will specify the model family as binomial, as the likelihood we are using in logistic regression is a Bernoulli likelihood, a special case of binomial likelihood when the total trial n=1. # Define formula in different ways # my_formula = as.formula(diabetes ~ glucose + age) # my_formula = as.formula(paste(colnames(PimaIndiansDiabetes)[1:8], collapse= &quot; + &quot;)) # my_formula = as.formula(diabetes ~ .) # Fit logistic regression glm_res &lt;- glm(diabetes ~ ., data=df_train, family = binomial) # We can use the logLik() function to obtain the log likelihood logLik(glm_res) ## &#39;log Lik.&#39; -281.9041 (df=9) We can use summary() function to see more details about the model fitting. summary(glm_res) ## ## Call: ## glm(formula = diabetes ~ ., family = binomial, data = df_train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.5366 -0.7691 -0.4238 0.7910 2.7698 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -8.044602 0.826981 -9.728 &lt; 2e-16 *** ## pregnant 0.130418 0.036080 3.615 0.000301 *** ## glucose 0.032196 0.004021 8.007 1.18e-15 *** ## pressure -0.017158 0.006103 -2.811 0.004934 ** ## triceps -0.003425 0.007659 -0.447 0.654752 ## insulin -0.001238 0.001060 -1.169 0.242599 ## mass 0.104029 0.018119 5.741 9.39e-09 *** ## pedigree 0.911030 0.344362 2.646 0.008156 ** ## age 0.012980 0.010497 1.237 0.216267 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 756.83 on 575 degrees of freedom ## Residual deviance: 563.81 on 567 degrees of freedom ## AIC: 581.81 ## ## Number of Fisher Scoring iterations: 5 3.2.3 Assess on test data Now, we can evaluate the accuracy of the model on the 25% test data. # Train the full model on the training data glm_train &lt;- glm(diabetes ~ ., data=df_train, family = binomial) # Predict the probability of being diabeties on test data # We can also set a threshold, e.g., 0.5 for the predicted label pred_prob = predict(glm_train, df_test, type = &quot;response&quot;) pred_label = pred_prob &gt;= 0.5 # Observed label obse_label = df_test$diabetes == &#39;pos&#39; # Calculate the accuracy on test data # think how accuracy is defined # we can use (TN + TP) / (TN + TP + FN + FP) # we can also directly compare the proportion of correctness accuracy = mean(pred_label == obse_label) print(paste(&quot;Accuracy on test set:&quot;, accuracy)) ## [1] &quot;Accuracy on test set: 0.796875&quot; 3.2.4 Model selection and diagnosis 3.2.4.1 Model2: New feature set by removing triceps # Train the full model on the training data glm_mod2 &lt;- glm(diabetes ~ pregnant + glucose + pressure + insulin + mass + pedigree + age, data=df_train, family = binomial) logLik(glm_mod2) ## &#39;log Lik.&#39; -282.0038 (df=8) summary(glm_mod2) ## ## Call: ## glm(formula = diabetes ~ pregnant + glucose + pressure + insulin + ## mass + pedigree + age, family = binomial, data = df_train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.5083 -0.7693 -0.4240 0.8034 2.7689 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -8.0317567 0.8251403 -9.734 &lt; 2e-16 *** ## pregnant 0.1308094 0.0361230 3.621 0.000293 *** ## glucose 0.0324606 0.0039854 8.145 3.80e-16 *** ## pressure -0.0175651 0.0060269 -2.914 0.003563 ** ## insulin -0.0014402 0.0009593 -1.501 0.133291 ## mass 0.1018155 0.0173811 5.858 4.69e-09 *** ## pedigree 0.9000134 0.3428652 2.625 0.008665 ** ## age 0.0131238 0.0105147 1.248 0.211982 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 756.83 on 575 degrees of freedom ## Residual deviance: 564.01 on 568 degrees of freedom ## AIC: 580.01 ## ## Number of Fisher Scoring iterations: 5 # Predict the probability of being diabeties on test data # We can also set a threshold, e.g., 0.5 for the predicted label pred_prob2 = predict(glm_mod2, df_test, type = &quot;response&quot;) pred_label2 = pred_prob2 &gt;= 0.5 accuracy2 = mean(pred_label2 == obse_label) print(paste(&quot;Accuracy on test set with model2:&quot;, accuracy2)) ## [1] &quot;Accuracy on test set with model2: 0.807291666666667&quot; 3.2.4.2 Model3: New feature set by removing triceps and insulin # Train the full model on the training data glm_mod3 &lt;- glm(diabetes ~ pregnant + glucose + pressure + mass + pedigree + age, data=df_train, family = binomial) logLik(glm_mod3) ## &#39;log Lik.&#39; -283.1342 (df=7) summary(glm_mod3) ## ## Call: ## glm(formula = diabetes ~ pregnant + glucose + pressure + mass + ## pedigree + age, family = binomial, data = df_train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.6492 -0.7697 -0.4213 0.8011 2.7414 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -7.797803 0.802287 -9.719 &lt; 2e-16 *** ## pregnant 0.130990 0.035957 3.643 0.00027 *** ## glucose 0.030661 0.003755 8.164 3.23e-16 *** ## pressure -0.017847 0.005953 -2.998 0.00272 ** ## mass 0.097356 0.016969 5.737 9.61e-09 *** ## pedigree 0.824150 0.338299 2.436 0.01484 * ## age 0.015134 0.010426 1.452 0.14663 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 756.83 on 575 degrees of freedom ## Residual deviance: 566.27 on 569 degrees of freedom ## AIC: 580.27 ## ## Number of Fisher Scoring iterations: 5 # Predict the probability of being diabeties on test data # We can also set a threshold, e.g., 0.5 for the predicted label pred_prob3 = predict(glm_mod3, df_test, type = &quot;response&quot;) pred_label3 = pred_prob3 &gt;= 0.5 accuracy3 = mean(pred_label3 == obse_label) print(paste(&quot;Accuracy on test set with model3:&quot;, accuracy3)) ## [1] &quot;Accuracy on test set with model3: 0.786458333333333&quot; 3.3 Cross-validation In last section, we split the whole dataset into 75% for training and 25% for testing. However, when the dataset is small, the test set may not be big enough and introduce high variance on the assessment. One way to reduce this variance in assessment is performing cross-validation, where we split the data into K folds and use K-1 folds for training and the remaining fold for testing. This procedure will be repeated for fold 1 to fold K as testing fold and all folds will be aggregated for joint assessment. K is usually taken 3, 5 or 10. In extreme case that K=n_sample, we call it leave-one-out cross-validation (LOOCV). Let’s load the dataset (again) first. # Load data library(mlbench) data(PimaIndiansDiabetes) Besides implement the cross-validation from scratch, there are packages supporting it well, including caret package. We will install it and use it for cross-validation here. # Install the caret library for cross-validation if (!requireNamespace(&quot;caret&quot;, quietly = TRUE)) { install.packages(&quot;caret&quot;) } library(caret) ## Loading required package: lattice # Define training control # We also want to have savePredictions=TRUE &amp; classProbs=TRUE set.seed(0) my_trControl &lt;- trainControl(method = &quot;cv&quot;, number = 5, classProbs = TRUE, savePredictions = TRUE) # Train the model cv_model &lt;- train(diabetes ~ ., data = PimaIndiansDiabetes, method = &quot;glm&quot;, family=binomial(), trControl = my_trControl) # Summarize the results print(cv_model) ## Generalized Linear Model ## ## 768 samples ## 8 predictor ## 2 classes: &#39;neg&#39;, &#39;pos&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 615, 614, 615, 614, 614 ## Resampling results: ## ## Accuracy Kappa ## 0.7708344 0.4695353 We can also access to detailed prediction results after concatenating the K folds: head(cv_model$pred) ## pred obs neg pos rowIndex parameter Resample ## 1 neg neg 0.9656694 0.03433058 4 none Fold1 ## 2 neg neg 0.8581071 0.14189290 6 none Fold1 ## 3 neg pos 0.9508306 0.04916940 7 none Fold1 ## 4 neg pos 0.6541361 0.34586388 17 none Fold1 ## 5 neg pos 0.7675666 0.23243342 20 none Fold1 ## 6 neg pos 0.6132685 0.38673152 26 none Fold1 We can double check the accuracy: CV_acc = mean(cv_model$pred$pred == cv_model$pred$obs) print(paste(&quot;Accuracy via 5-fold cross-validation&quot;, CV_acc)) ## [1] &quot;Accuracy via 5-fold cross-validation 0.770833333333333&quot; 3.4 More assessment metrics 3.4.1 Two types of error In the above sections, we used the accuracy to perform model diagnosis, either only on one testing dataset or aggregating cross multiple folds in cross- validation. Accuracy is a widely used metric for model evaluation, on the averaged error rate. However, this metric still have limitations when assessing the model performance, especially the following two: When the samples are highly imbalance, high accuracy may not mean a good model. For example, for a sample with 990 negative samples and 10 positive samples, a simple model by predicting for all sample as negative will give an accuracy of 0.99. Thus, for highly imbalanced samples, we should be careful when interpreting the accuracy. In many scenarios, our tolerance on false positive errors and false negative errors may be different and we want to know both for a certain model. They are often called as type I and II errors: Type I error: false positive (rate) Type II error: false negative (rate) - a joke way to remember what type II mean Negative has two stripes. Here, we use the diabetes dataset and their cross-validation results above to illustrate the two types of errors and the corresponding model performance evaluation. # Let&#39;s start to define the values for the confusion matrix first # Recall what the difference between &amp; vs &amp;&amp; # Read more: https://stat.ethz.ch/R-manual/R-devel/library/base/html/Logic.html TP = sum((cv_model$pred$obs == &#39;pos&#39;) &amp; (cv_model$pred$pred == &#39;pos&#39;)) FN = sum((cv_model$pred$obs == &#39;pos&#39;) &amp; (cv_model$pred$pred == &#39;neg&#39;)) FP = sum((cv_model$pred$obs == &#39;neg&#39;) &amp; (cv_model$pred$pred == &#39;pos&#39;)) TN = sum((cv_model$pred$obs == &#39;neg&#39;) &amp; (cv_model$pred$pred == &#39;neg&#39;)) print(paste(&#39;TP, FN, FP, TN:&#39;, TP, FN, FP, TN)) ## [1] &quot;TP, FN, FP, TN: 151 117 59 441&quot; We can also use the table() function to get the whole confusion matrix. Read more about the table function for counting the frequency of each element. A similar way is the confusionMatrix() in caret package. # Calculate confusion matrix confusion_mtx = table(cv_model$pred[, c(&quot;obs&quot;, &quot;pred&quot;)]) confusion_mtx ## pred ## obs neg pos ## neg 441 59 ## pos 117 151 # similar function confusionMatrix # conf_mat = confusionMatrix(cv_model$pred$pred, cv_model$pred$obs) # conf_mat$table We can also plot out the confusion matrix # Change to data.frame before using ggplot confusion_df = as.data.frame(confusion_mtx) ggplot(confusion_df, aes(pred, obs, fill= Freq)) + geom_tile() + geom_text(aes(label=Freq)) + scale_fill_gradient(low=&quot;white&quot;, high=&quot;darkgreen&quot;) Also the false positive rate, false negative rate and true negative rate. Note, the denominator is always the number of observed samples with the same label, namely they are a constant for a specific dataset. FPR = FP / sum(cv_model$pred$obs == &#39;neg&#39;) FNR = FN / sum(cv_model$pred$obs == &#39;pos&#39;) TPR = TP / sum(cv_model$pred$obs == &#39;pos&#39;) print(paste(&quot;False positive rate:&quot;, FPR)) ## [1] &quot;False positive rate: 0.118&quot; print(paste(&quot;False negative rate:&quot;, FNR)) ## [1] &quot;False negative rate: 0.436567164179104&quot; print(paste(&quot;True positive rate:&quot;, TPR)) ## [1] &quot;True positive rate: 0.563432835820896&quot; 3.4.2 ROC curve In the above assessment, we only used \\(P&gt;0.5\\) to denote predicted label as positive. We can imagine if we a lower cutoff lower, we will have more false positives and fewer false negatives. Indeed, in different scenarios, people may choose different level of cutoff for their tolerance of different types of errors. Let’s try cutoff \\(P&gt;0.4\\). Think what will you expect. # Original confusion matrix table(cv_model$pred[, c(&quot;obs&quot;, &quot;pred&quot;)]) ## pred ## obs neg pos ## neg 441 59 ## pos 117 151 # New confusion matrix with cutoff 0.4 cv_model$pred$pred_new = as.integer(cv_model$pred$pos &gt;= 0.4) table(cv_model$pred[, c(&quot;obs&quot;, &quot;pred_new&quot;)]) ## pred_new ## obs 0 1 ## neg 408 92 ## pos 89 179 Therefore, we may want to assess the model performance by varying the cutoffs and obtain a more systematic assessment. Actually, the Receiver operating characteristic (ROC) curve is what you need. It presents the TPR (sensitivity) vs the FPR (i.e., 1 - TNR or 1 - specificity) when varying the cutoffs. In order to achieve this, we can calculate FPR and TPR manually by varying the cutoff through a for loop. Read more about for loop and you may try write your own and here is an example from the cardelino package. For simplicity, let use an existing tool implemented in the plotROC package: plotROC::geom_roc() that is compatible with ggplot2. # Install the plotROC library for plotting ROC curve if (!requireNamespace(&quot;plotROC&quot;, quietly = TRUE)) { install.packages(&quot;plotROC&quot;) } library(ggplot2) library(plotROC) # You can set the n.cuts to show the cutoffs on the curve g = ggplot(cv_model$pred, aes(m = pos, d = as.integer(obs==&#39;pos&#39;))) + geom_roc(n.cuts=7, hjust = -0.4, vjust = 1.5) + coord_equal() + ggtitle(&quot;ROC curve&quot;) # Calculate AUC from the graph AUC_val = calc_auc(g)$AUC # Display the plot g + annotate(&quot;text&quot;, x=0.8, y=0.1, label=paste(&quot;AUC =&quot;, round(AUC_val, 4))) 3.4.3 Homework Now, try another model with removing triceps and plot the ROC curve and calculate the AUC score. Is it higher or lower than using the full features? "],["introHypoTest.html", "Chapter 4 Introduction to Hypothesis testing", " Chapter 4 Introduction to Hypothesis testing "],["genomics.html", "Chapter 5 Personalised Genomic Medicine", " Chapter 5 Personalised Genomic Medicine Contents to be added. "],["image-digital.html", "Chapter 6 Medical Image and Digital Health", " Chapter 6 Medical Image and Digital Health Contents to be added. "],["infectious-dis.html", "Chapter 7 Infectious Disease Informatics", " Chapter 7 Infectious Disease Informatics Contents to be added. "],["pop-genetics.html", "Chapter 8 Population Genetics and Diseases", " Chapter 8 Population Genetics and Diseases Contents to be added. "],["epidemiology.html", "Chapter 9 Epidemiology of Cancer and Other Diseases", " Chapter 9 Epidemiology of Cancer and Other Diseases Contents to be added. "],["install.html", "Appendix A: Install R &amp; RStudio A.1 Install R A.2 Install RStudio A.3 Use R inside RStudio A4. Cloud computing", " Appendix A: Install R &amp; RStudio This manual covers the installation of both R and RStudio for three different operating systems: Windows, macOS and Ubuntu. You only need to follow the one that you are using on your computer. Difference between R and RStudio R is the backbone of R programming. Once R is installed, you can use it via its build-in R Console (self-contained), terminal or any third-party integrated development environment (IDE), e.g., RStudio. RStudio is a multi-facet and user-friendly IDE that can make R programming and data analysis in one place and easy to manage. We recommend using RStudio and only demonstrate with it, while you are free to use any other alternative. Acknowledgements This manual is adapted and updated from the materials produced by Xiunan Fang and other team members in Dr Joshua Ho’s lab. A.1 Install R R on Windows Open an internet browser and go to https://cran.r-project.org/. Click on the Download R for Windows link at the top of the page. Choose the base and then Click on the Download R 4.2.1 for Windows link at the top of the page (or a new version if this manual is outdated). Once the download is finished, you will obtain a file named R-4.2.1-win.exe or similar depending on the version that you download. Most of the time, you will likely want to go with the defaults, so click the button Next until the process is complete. R on macOS Open an internet browser and go to https://cran.r-project.org/. Click on the Download R for macOS link at the top of the page. Click on the file containing the latest version of R under the Latest release. Save the .pkg file, double-click it to open, and follow the installation instructions. Note, there are two versions of the .pkg installation file according to the CPU model: Intel Macs (Intel-based) or M1 Macs (ARM-based). Please choose accordingly. R on Ubuntu As it is common, prior to installing R, let us update the system package index and upgrade all our installed packages using the following two commands: sudo apt update sudo apt -y upgrade After that, all that you have to do is run the following in the command line to install base R. sudo apt -y install r-base A.2 Install RStudio Now that R is installed, you need to download and install RStudio. It is more straightforward to install RStudio and very similar across the three OS. Go to https://www.rstudio.com/products/rstudio/download/#download. We are using `RStudio Desktop Free version. Click on the right file for your OS (e.g., .exe file for Windows) The installation process is very straightforward as the figure below. A.3 Use R inside RStudio R studio RStudio is very powerful for providing a four-pane workspace. Top-left panel: Your scripts of the R codes, script is good to keep a record of your work and also convenient for command execution. You can create a new script by File –&gt; New –&gt; R Script Bottom-left panel: R console for R commands, where you actually run the R codes. Top-right panel: Workspace tab: All the data(more specifically, R objects) you have created in the Workspace and all previous commands you previously ran in the History. Bottom-right panel: Files in your working directory(you probably should also set your working directory) in Files, and the plots you have created in Plots. Set working directory Create a folder named “biof_Rdir” in your preferred directory Create a “data” folder in the “biof_Rdir” From RStudio, use the menu to change your working directory under Session &gt; Set Working Directory &gt; Choose Directory Choose the directory to “biof_Rdir” Or you can type in the console: setwd(&quot;/yourdirectory/biof_Rdir&quot;) For Windows, the command might look like : setwd(&quot;c:/yourdirectory/biof_Rdir&quot;) Some general knowledge R is case-sensitive Type enter to run R code in the console pane Ctrl-enter or Cmd-return if the code is in the scripts pane. Comments come after # will not be treated as codes R has some pre-loaded data sets, to see the list of pre-loaded data, type data() In R, a function is an object, a basic syntax of an R function looks like something below: function_name &lt;- function(arg_1, arg_2, ...) { actual function codes } For example: my_average &lt;- function(x){ sum(x)/length(x) } my_average(c(1, 4, 5, 7)) ## [1] 4.25 R contains a lot of built-in functions, you can use ? to see the documentation of a function, there are also a lot of external libraries with specific functions. To use a library, we do: install.packages(&#39;package_name&#39;) library(package_name) Install packages There are several packages used in this workshop, in the R console, type: install.packages(&#39;ggplot2&#39;) install.packages(&#39;pheatmap&#39;) install.packages(&#39;aod&#39;) A4. Cloud computing In case you have limited computing power, you can still use cloud computing to finish this course. There can be multiple options and here we mainly recommend RStudio cloud (https://rstudio.cloud). You can explore directly from their website. "],["references.html", "References", " References "]]
